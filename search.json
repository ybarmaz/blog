[
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html",
    "href": "posts/2021-02-09-contextual_bandits.html",
    "title": "Thompson sampling for contextual bandits",
    "section": "",
    "text": "The multi-armed bandit problem is inspired by the situation of gamblers facing \\(N\\) slot machines with a limited amount of resources to “invest” in them, without knowing the probability distribution of rewards from each machine. By playing with a machine, they can of course sample its distribution. Once they find a machine that performs well enough, the question is wheter they should try the other ones that might perform even better, at the risk of wasting money because they might be worse. This is an example of the exploration-exploitation tradeoff dilemma. Applications include clinical trial design, portfolio selection, and A/B testing.\nThompson sampling is an approximate solution applicable to bandits for which we have a Bayesian model of the reward \\(r\\), namely a likelihood \\(P(r\\vert \\theta, a)\\) that depends on the action \\(a\\) (the choice of an arm to pull) and a vector of parameters \\(\\theta\\), and a prior distribution \\(P(\\theta)\\). In certain cases, called contextual bandits, the likelihood also depends on a set of features \\(x\\) observed by the players before they choose an action, \\(P(r\\vert \\theta, a, x)\\). After each round, the posterior distribution \\(P(\\theta \\vert \\left\\lbrace r_i, a_i, x_i\\right\\rbrace_{i})\\) is updated with the newly observed data. Then a \\(\\theta^\\ast\\) is sampled from it, the new context \\(x\\) is observed, and the new action is chosen to maximize the expected reward, \\(a^\\ast = \\mathrm{argmax}_a \\ \\mathbb{E}(r\\vert \\theta, a, x)\\).\nThis approach solves the exploration-exploitation dilemma with the random sampling of \\(\\theta^\\ast\\), which gives to every action a chance to be selected, yet favors the most promising ones. The more data is collected, the more informative the posterior distribution will become and the more it will favor its top performer.\nThis mechanism is illustrated in the chapter 6 of Probabilistic Programming & Bayesian Methods for Hackers and the section 4.4 of Bayesian Adaptive Methods for Clinical Trials.\nBoth discuss the case of a binary reward (success and failure) for every action \\(a\\) that follows a Bernoulli distribution with unknown probability of success \\(p_a\\). They assume a beta prior for each of the \\(p_a\\), which is the conjugate prior for the Bernoulli likelihood and makes inference of the posterior straightforward. This is particularly appealing when you have to update your posterior after each play.\nIf there are covariates that can explain the probability of success, one of the simplest models for a binary response of the potential actions is the combination of generalized linear models for each action,\n\\[\nP(r=1 \\vert \\theta, a, x) = \\frac{1}{1 + e^{-\\left(\\alpha_a + \\beta_a^T\\,x\\right)}}\n\\]\nUnfortunately, there is no immediate congugate prior for this type of likelihood, so we have to rely on numerical methods to estimate the posterior distribution. A previous blog post discussed variational inference as a speedier alternative to MCMC algorithms, and we will see here how we can apply it to the problem of contextual bandits with binary response.\nThis problem is relevant in the development of personalized therapies, where the actions represent the different treatments under investigation and the contexts \\(x\\) are predictive biomarkers of their response. The goal of a trial would be to estimate the response to each treatment option given biomarkers \\(x\\), and, based on that, to define the best treatment policy. Adaptive randomization through Thompson sampling ensures that more subjects enrolled in the trial get the optimal treatment based on their biomarkers and the knowledge accrued until their randomization, which is certainly more ethical than a randomization independent on the biomarkers.\nAnother example is online ad serving, where the binary response corresponds to a successful conversion, the action is the selection of an ad for a specific user, and the context is a set of features related to that user. When a new ad enters the portfolio and a new click-through rate model needs to be deployed for it, Thompson sampling can accelerate the training phase and reduce the related costs."
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#bandit-model",
    "href": "posts/2021-02-09-contextual_bandits.html#bandit-model",
    "title": "Thompson sampling for contextual bandits",
    "section": "Bandit model",
    "text": "Bandit model\nFor simplicity, we simulate bandits whose true probabilities of success follow logistic models, so we can see how the posterior distributions concentrate around the true values during training. You can run this notebook in Colab to experiment with more realistic models, and vary the number of arms or the context dimension.\n\n\nCode\nclass ContextualBandit(object):\n    \"\"\"\n    This class represents contextual bandit machines with n_arms arms and\n    linear logits of p-dimensional contexts.\n\n    parameters:\n        arm_true_weights: (n_arms, p) Numpy array of real weights.\n        arm_true_biases:  (n_arms,) Numpy array of real biases\n\n    methods:\n        pull( arms, X ): returns the results, 0 or 1, of pulling \n                   the arms[i]-th bandit given an input context X[i].\n                   arms is an (n,) array of arms indices selected by the player\n                   and X an (n, p) array of contexts observed by the player\n                   before making a choice.\n        \n        get_logits(X): returns the logits of all bandit arms for every context in\n                   the (n, p) array X\n                   \n        get_probs(X): returns sigmoid(get_logits(X))\n        \n        get_selected_logits(arms, X): returns from get_logits(X) only the logits\n                   corresponding to the selected arms\n        \n        get_selected_probs(arms, X): returns sigmoid(get_selected_logits(arms, X))\n        \n        get_optimal_arm(X): returns the arm with the highest probability of success\n                   for every context in X\n\n    \"\"\"\n    def __init__(self, arm_true_weights, arm_true_biases):\n        self._arm_true_weights = tf.convert_to_tensor(\n              arm_true_weights,\n              dtype=tf.float32,\n              name='arm_true_weights')\n        self._arm_true_biases = tf.convert_to_tensor(\n              arm_true_biases,\n              dtype=tf.float32,\n              name='arm_true_biases')\n        self._shape = np.array(\n              self._arm_true_weights.shape.as_list(),\n              dtype=np.int32)\n        self._dtype = tf.convert_to_tensor(\n              arm_true_weights,\n              dtype=tf.float32).dtype.base_dtype\n\n    @property\n    def dtype(self):\n        return self._dtype\n    \n    @property\n    def shape(self):\n        return self._shape\n    \n    def get_logits(self, X):\n        return tf.matmul(X, self._arm_true_weights, transpose_b=True) + \\\n               self._arm_true_biases\n    \n    def get_probs(self, X):\n        return tf.math.sigmoid(self.get_logits(X))\n\n    def get_selected_logits(self, arms, X):\n        all_logits = self.get_logits(X)\n        column_indices = tf.convert_to_tensor(arms, dtype=tf.int64)\n        row_indices = tf.range(X.shape[0], dtype=tf.int64)\n        full_indices = tf.stack([row_indices, column_indices], axis=1)\n        selected_logits = tf.gather_nd(all_logits, full_indices)\n        return selected_logits\n    \n    def get_selected_probs(self, arms, X):\n        return tf.math.sigmoid(self.get_selected_logits(arms, X))\n    \n    def pull(self, arms, X):\n        selected_logits = self.get_selected_logits(arms, X)\n        return tfd.Bernoulli(logits=selected_logits).sample()\n    \n    def pull_all_arms(self, X):\n        logits = self.get_logits(X)\n        return tfd.Bernoulli(logits=logits).sample()\n    \n    def get_optimal_arm(self, X):\n        return tf.argmax(\n            self.get_logits(X),\n            axis=-1)\n\n\nHere we work with a two-dimensional context drawn from two independent standard normal distributions, and we select true weights and biases that correspond to an overall probability of success of about 30% for each arm, a situation that might be encountered in a personalized medicine question.\n\ntrue_weights = np.array([[2., 0.],[0., 3.]])\ntrue_biases = np.array([-1., -2.])\n\nN_ARMS = true_weights.shape[0]\nCONTEXT_DIM = true_weights.shape[1]\n\nbandit = ContextualBandit(true_weights, true_biases)\npopulation = tfd.Normal(loc=tf.zeros(CONTEXT_DIM, dtype=tf.float32),\n                        scale=tf.ones(CONTEXT_DIM, dtype=tf.float32))"
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#thompson-sampling",
    "href": "posts/2021-02-09-contextual_bandits.html#thompson-sampling",
    "title": "Thompson sampling for contextual bandits",
    "section": "Thompson sampling",
    "text": "Thompson sampling\nA Thompson sampler based on a logistic regression can be implemented as a generalization of the probabilistic machine learning model discussed in the previous post. It is essentially a single dense variational layer with one unit per arm of the contextual bandit we want to solve. These units are fed into a Bernoulli distribution layer that simulates the pull of each arm.\nThe parameters \\(\\theta\\) of the model are encoded in the posterior_mean_field used as a variational family for the dense variational layer, and when we fit the full model to data, it converges to an approximation of the true posterior \\(P(\\theta \\vert \\left\\lbrace r_i, a_i, x_i\\right\\rbrace_{i})\\).\nA subsequent call of that dense variational layer on a new input \\(x\\) will return random logits drawn from the approximate posterior predictive distribution and can thus be used to implement Thompson sampling (see the randomize method in the code). The \\(a^\\ast = \\mathrm{argmax}_a \\ \\mathbb{E}(r\\vert \\theta, a, x)\\) step is the selection of the unit with the highest logit.\nFor training, the loss function is the negative log-likelihood of the observed outcome \\(r_i\\), but only for the unit corresponding to the selected action \\(a_i\\), so it is convenient to combine them into a unique output \\(y_i=(a_i,r_i)\\) and write a custom loss function.\n\n\nCode\nclass ThompsonLogistic(tf.keras.Model):\n    \"\"\"\n    This class represents a Thompson sampler for a Bayesian logistic regression\n    model.\n    \n    It is essentially a keras Model of a single layer Bayesian neural network\n    with Bernoulli output enriched with a Thompson randomization method that\n    calls only the dense variational layer.\n    \n    Parameters:\n        - context_dim: dimension of the context\n        - n_arms: number of arms of the multi-arm bandit under investigation\n        - sample_size: size of the current training set of outcome observations,\n                       used to scale the kl_weight of the dense variational layer\n    \n    Methods:\n        - randomize(inputs): returns a logit for each arm drawn from the (approximate)\n            posterior predictive distribution\n        - get_weights_stats(): returns means and sttdevs of the surrogate posterior\n            of the model parameters\n        - predict_probs(X, sample_size): returns the posterior probability of success\n            for each context in the array X and each arm of the bandit, sample_size specifies\n            the sample size of the Monte Carlo estimate\n        - assign_best_mc(X, sample_size): returns the arms with the highest\n                                          predict_probs(X, sample_size)\n        - assign_best(X): returns the arms with the highest expected logit, should\n            be very similar to assign_best_mc, a little bit less accurate\n    \"\"\"\n    def __init__(self, context_dim, n_arms, sample_size):\n        super().__init__()\n        self.context_dim = context_dim\n        self.n_arms = n_arms\n        self.densevar = tfp.layers.DenseVariational(n_arms, posterior_mean_field, prior_ridge, kl_weight=1/sample_size)\n        self.bernoullihead = tfp.layers.DistributionLambda(lambda t: tfd.Bernoulli(logits=t))\n    \n    def call(self, inputs):\n        x = self.densevar(inputs)\n        return self.bernoullihead(x)\n    \n    def randomize(self, inputs):\n        return self.densevar(inputs)\n    \n    def get_weights_stats(self):\n        n_params = self.n_arms * (self.context_dim + 1)\n        c = np.log(np.expm1(1.))\n        \n        weights = self.densevar.weights[0]\n\n        means = weights[:n_params].numpy().reshape(self.context_dim + 1, self.n_arms)\n        stddevs = (1e-5 + tf.nn.softplus(c + weights[n_params:])).numpy().reshape(self.context_dim + 1, self.n_arms)\n\n        mean_weights = means[:-1]\n        mean_biases = means[-1]\n\n        std_weights = stddevs[:-1]\n        std_biases = stddevs[-1]\n        return mean_weights, mean_biases, std_weights, std_biases\n    \n    def assign_best(self, X):\n        mean_weights, mean_biases, std_weights, std_biases = self.get_weights_stats()\n        logits = tf.matmul(X, mean_weights) + mean_biases\n        return tf.argmax(logits, axis=1)\n    \n    def predict_probs(self, X, sample_size=100):\n        mean_weights, mean_biases, std_weights, std_biases = self.get_weights_stats()\n        \n        weights = tfd.Normal(loc=mean_weights, scale=std_weights).sample(sample_size)\n        biases = tfd.Normal(loc=mean_biases, scale=std_biases).sample(sample_size)\n        \n        probs = tf.math.sigmoid(tf.matmul(X, weights)+biases[:,tf.newaxis,:])\n        return tf.reduce_mean(probs, axis=0)\n              \n    def assign_best_mc(self, X, sample_size=100):\n        probs = self.predict_probs(X, sample_size)\n        return tf.argmax(probs, axis=1)\n    \n\n# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\ndef posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n    n = kernel_size + bias_size\n    c = np.log(np.expm1(1.))\n    return tf.keras.Sequential([\n        tfp.layers.VariableLayer(2 * n,\n                                 initializer=tfp.layers.BlockwiseInitializer([\n                                     'zeros',\n                                     tf.keras.initializers.Constant(np.log(np.expm1(.7))),\n                                 ], sizes=[n, n]),\n                                 dtype=dtype),\n        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n            tfd.Normal(loc=t[..., :n],\n                       scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n            reinterpreted_batch_ndims=1)),\n    ])\n\n\n# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\ndef prior_ridge(kernel_size, bias_size, dtype=None):\n    return lambda _: tfd.Independent(\n        tfd.Normal(loc=tf.zeros(kernel_size + bias_size),\n                   scale=tf.concat([2*tf.ones(kernel_size),\n                                    4*tf.ones(bias_size)],\n                                   axis=0)),\n        reinterpreted_batch_ndims=1\n    )\n\n    \ndef build_model(context_dim, n_arms, sample_size, learning_rate=0.01):\n    model = ThompsonLogistic(context_dim, n_arms, sample_size)\n    \n    # the loss function is the negloglik of the outcome y[:,1] and the head corresponding\n    # to the arm assignment y[:,0] is selected with a one-hot mask\n    loss_fn = lambda y, rv_y: tf.reduce_sum(-rv_y.log_prob(y[:,1, tf.newaxis]) * tf.one_hot(y[:,0], n_arms), axis=-1)\n    \n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss=loss_fn)\n    model.build(input_shape=(None, context_dim))\n    return model"
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#learning-strategy",
    "href": "posts/2021-02-09-contextual_bandits.html#learning-strategy",
    "title": "Thompson sampling for contextual bandits",
    "section": "Learning strategy",
    "text": "Learning strategy\nIn the learning phase of the model, at each step a new context \\(x_i\\) is observed (or drawn from the population), an action \\(a_i\\) is chosen, a reward \\(r_i\\) is observed (or simulated with bandit.pull), and the model is updated.\n\n\nCode\nclass BayesianStrategy(object):\n    \"\"\"\n    Implements an online, learning strategy to solve\n    the contextual multi-armed bandit problem.\n    \n    parameters:\n      bandit: an instance of the ContextualBandit class\n    \n    methods:\n      thompson_randomize(X): draws logits from the posterior distribution and\n                             returns the arms with the highest values\n      \n      _update_step(X, y): updates the model with the new observations\n      \n      one_trial(n, population): samples n elements from population, selects\n                             an arm for each of them through Thompson sampling,\n                             pulls it, updates the model\n      \n      train_on_data(X_train, all_outcomes_train): implements Thompson sampling\n                             on pre-sampled data where an omnicient being has\n                             pulled all the arms. The reason is to compare with\n                             standard Bayesian inference on the same data\n                            \n      evaluate_training_decisions: returns statistics about action selection\n                             during training\n    \"\"\"\n    \n    def __init__(self, bandit):\n        self.bandit = bandit\n        self.context_dim = bandit.shape[1]\n        self.n_arms = bandit.shape[0]\n        dtype = tf.float32\n        self.X = tf.cast(tf.reshape((), (0, self.context_dim)), tf.float32)\n        self.y = tf.cast(tf.reshape((), (0, 2)), tf.int32)\n        self.model = build_model(self.context_dim, self.n_arms, 1, learning_rate=0.008)\n        self.loss = []\n        self.weights = []\n      \n\n    def thompson_randomize(self, X):\n        return tf.argmax(self.model.randomize(X), axis=1)\n    \n    \n    def _update_step(self, X, y, epochs=10):\n        self.X = tf.concat([self.X, X], axis=0)\n        self.y = tf.concat([self.y, y], axis=0)\n        weights = self.model.get_weights()\n        self.model = build_model(self.context_dim, self.n_arms, self.X.shape[0], learning_rate=0.008)\n        self.model.set_weights(weights)\n        hist = self.model.fit(self.X, self.y, verbose=False, epochs=epochs)\n        self.loss.append(hist.history['loss'])\n        self.weights.append(self.model.get_weights_stats())\n\n        \n    def one_trial(self, n, population, epochs=10):\n        X = population.sample(n)\n        selected_arms = self.thompson_randomize(X)\n        outcomes = self.bandit.pull(selected_arms, X)\n        y = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes[:,tf.newaxis]], axis=1)\n        self._update_step(X, y, epochs)\n    \n    \n    def train_on_data_step(self, X, all_outcomes, epochs):\n        selected_arms = self.thompson_randomize(X)\n        column_indices = tf.convert_to_tensor(selected_arms, dtype=tf.int64)\n        row_indices = tf.range(X.shape[0], dtype=tf.int64)\n        full_indices = tf.stack([row_indices, column_indices], axis=1)\n        outcomes = tf.gather_nd(all_outcomes, full_indices)\n        y = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes[:,tf.newaxis]], axis=1)\n        self._update_step(X, y, epochs)\n    \n    \n    def train_on_data(self, X_train, all_outcomes_train, batch_size=1, epochs=10):\n        n_train = X_train.shape[0]\n        ds = tf.data.Dataset.from_tensor_slices((X_train, all_outcomes_train)).batch(batch_size)\n        for (X, all_outcomes) in ds:\n            self.train_on_data_step(X, all_outcomes, epochs)\n    \n    \n    def train_on_data_standard(self, X_train, all_outcomes_train, epochs=1000):\n        n_train = X_train.shape[0]\n        n_zeros = n_train//2\n        n_ones = n_train - n_zeros\n        selected_arms = tf.cast(tf.math.floormod(tf.range(n_train), 2), tf.int64)\n        column_indices = tf.convert_to_tensor(selected_arms, dtype=tf.int64)\n        row_indices = tf.range(n_train, dtype=tf.int64)\n        full_indices = tf.stack([row_indices, column_indices], axis=1)\n        outcomes_train = tf.gather_nd(all_outcomes_train, full_indices)\n        y_train = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes_train[:,tf.newaxis]], axis=1)\n        self._update_step(X_train, y_train, epochs)\n    \n    \n    def evaluate_training_decisions(self):\n        best_arm_proportion = tf.reduce_mean(tf.cast(\n            tf.cast(self.y[:,0], tf.int64)==self.bandit.get_optimal_arm(self.X), tf.float32)).numpy()\n        success_rate = self.y[:,1].numpy().sum()/self.y.shape[0]\n        prob_of_success = tf.reduce_mean(self.bandit.get_selected_probs(tf.cast(self.y[:,0], tf.int64), self.X), axis=0).numpy()\n        return {'training_best_arm_proportion': best_arm_proportion,\n                'training_success_rate': success_rate,\n                'training_prob_of_success': prob_of_success\n               }\n\n\nAfter 60 to 80 iterations, the surrogate posteriors seem to have converged to distributions that are compatible with the true values of the parameters.\n\n\n\n\n\nFor comparison, we can train models on the same sample that has been assigned purely randomly to each arm.\n\n\n\n\n\nThe surrogate posterior distributions look similar to the ones obtained from Thompson sampling, and the predictive performance on a test set are comparable. In the following table, “best_arm_selection_rate” describes how frequently the best action is selected for contexts in the test set according to the predictions of the two models, and “model_prob_of_success” is the average of the true probabilities of success for the actions selected by the model. For reference, “arms_probs_of_success” shows the average of the true probabilities of success for each action in the case it is always picked. The benefit of Thompson sampling is revealed in the predictive performance during training. In the same table, “training_best_arm_proportion” indicates how often the best action is selected during training (as expected, roughly half the time for standard randomization), “training_success_rate” the observed success rate during training and “training_prob_of_success” the average probability of success following the assignment decisions made during training.\n\n\n\n\n\n\n  \n    \n      \n      Thompson randomization\n      Standard randomization\n    \n  \n  \n    \n      training_best_arm_proportion\n      0.775\n      0.5875\n    \n    \n      training_success_rate\n      0.4375\n      0.375\n    \n    \n      training_prob_of_success\n      0.448006\n      0.357196\n    \n    \n      best_arm_selection_rate\n      0.9228\n      0.9126\n    \n    \n      model_prob_of_success\n      0.48482\n      0.484251\n    \n    \n      arms_probs_of_success\n      [0.35353488, 0.27571228]\n      [0.35353488, 0.27571228]\n    \n  \n\n\n\n\nIn terms of reward, it is clear that training a model with Thompson randomization costs less than with standard randomization, and the inferred arm selection policy after training is very similar. In a clinical trial, that would translate into more enrolled subjects getting the best therapy according to their biomarkers."
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#tougher-bandits",
    "href": "posts/2021-02-09-contextual_bandits.html#tougher-bandits",
    "title": "Thompson sampling for contextual bandits",
    "section": "Tougher bandits",
    "text": "Tougher bandits\nThe simple model presented in this note can be expanded in several directions. We can obviously consider more arms and contexts of higher dimensions. In that case, incorporating expert knowledge in the form of more informative priors or more complex surrogate posteriors can be useful. We can also include past observations to achieve faster convergence, and tamper them with lower weights if they are less relevant than the data sampled from the bandits during training. This type of jump start is particularly relevant in fields like online advertising where lower overall probabilities require more observations.\nMore complex mechanisms could be modeled with deeper Bayesian neural networks. The important requirement is a layer that can implement Thompson sampling. Moreover, the DistributionLambda top layer is not limited to Bernoulli distributions, and a wide variety of reward distributions can be easily simulated. It is probably reasonable to start with a single DenseVariational layer with adequate priors and variational surrogate posteriors with a top DistributionLambda layer compatible with the rewards, and then try to add layers to improve performance. As in most machine learning problems, the key is experimentation."
  },
  {
    "objectID": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html",
    "href": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html",
    "title": "Hamiltonian mechanics with JAX",
    "section": "",
    "text": "I finally spent some time playing around with JAX. I was especially curious about its automatic differentiation system, which is obviously very useful for computing gradients in machine learning, but also opens up new possibilities for physical modeling. In particular, the Autodiff Cookbook advertises the correspondence between the grad API and differential geometry, which is the fundamental language of classical mechanics.\nConcretely, you can quickly define functions on the phase space with jax.numpy and then automatically compute their gradients. The phase space is the space of all possible states of a physical system, for instance the positions \\(\\vec x_i\\) of its constituent particles and their momenta \\(\\vec p_i\\), or the orientation of a rigid body and its angular momentum. In general, the coordinates of a phase space can always be locally chosen so that they are split into positions \\(\\mathbf{q} = (q_1, \\dots, q_n)\\) and conjugate momenta \\(\\mathbf{p} = (p_1, \\dots, p_n)\\), and their time evolution is given by the equations of motion\n\\[\n\\begin{aligned}\n\\dot{\\mathbf{q}} &=  \\frac{\\partial H}{\\partial \\mathbf{p}} = \\lbrace \\mathbf{q}, H \\rbrace \\\\\n\\dot{\\mathbf{p}} &=  -\\frac{\\partial H}{\\partial \\mathbf{q}} = \\lbrace \\mathbf{p}, H \\rbrace\n\\end{aligned} \\tag{1}\n\\]\nwhere \\(H(\\mathbf{q}, \\mathbf{p})\\) is the Hamiltonian function of the system, or simply the Hamiltonian, which is essentially its energy. With automatic differentiation, all you need to build this set of differential equations is to specify the Hamiltonian function, which can spare you a lot of pain.\nThe curly braces are the Poisson bracket, a fundamental structure of the phase space. In the canonical coordinates \\((\\mathbf{q}, \\mathbf{p})\\), also called the Darboux coordinates, it is defined as\n\\[\n\\lbrace f, g \\rbrace = \\sum_{i=1}^n\\left(\n  \\frac{\\partial f}{\\partial q_i}\\frac{\\partial g}{\\partial p_i}\n  - \\frac{\\partial f}{\\partial p_i}\\frac{\\partial g}{\\partial q_i}\n\\right) \\tag{2}\n\\]\nfor any two functions \\(f\\) and \\(g\\) on the phase space. Together with the Hamiltonian function, the Poisson bracket encodes the time evolution of any observable of the system (a quantity that depends on the state of the system and can be measured),\n\\[\n\\frac d{dt}f(\\mathbf{q}, \\mathbf{p}, t) = \\lbrace f, H \\rbrace + \\frac{\\partial f}{\\partial t}. \\tag{3}\n\\]\nThe mathematical interpretation is that the Poisson bracket transforms the Hamiltonian function \\(H\\) into a vector field \\(\\lbrace \\cdot, H \\rbrace\\), the so-called Hamiltonian vector field.\nThe Poisson bracket is also an essential piece in the connection between classical and quantum mechanics, but this is another story and we will get back to it when quantum computers are more broadly available.\nIn JAX, the Poisson bracket in Darboux coordinates can be defined as a function that takes as its arguments two functions of q and p and returns a third function of q and p, where q and p are arrays."
  },
  {
    "objectID": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#harmonic-oscillator",
    "href": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#harmonic-oscillator",
    "title": "Hamiltonian mechanics with JAX",
    "section": "Harmonic Oscillator",
    "text": "Harmonic Oscillator\nAs an example, we can consider the harmonic oscillator, the Drosophila of theoretical physics, with Hamiltonian\n\\[H(\\mathbf{q}, \\mathbf{p}) = \\frac{\\mathbf{p}^2}{2m} + \\frac{k\\,\\mathbf{q}^2}{2}, \\tag{4}\\]\nand we set \\(m=k=1\\) for convenience. Two observables are simply the position and the momentum.\n\ndef harm_osc_hamiltonian(q, p):\n    return 0.5*jnp.dot(q, q) + 0.5*jnp.dot(p, p)\n\ndef position(q, p):\n    return q\n\ndef momentum(q, p):\n    return p\n\nprint(poisson_bracket_darboux(harm_osc_hamiltonian, position)(5., 10.))\n\n-10.0\n\n\nNote that this code works only if \\(q\\) and \\(p\\) are one-dimensional since the Poisson bracket accepts only scalar-valued functions as its arguments. If we work with higher-dimensional spaces, we can also select a single coordinate, for instance with a lambda function.\n\nq_3d = jnp.array([1., 2., 3.])\np_3d = jnp.array([4., 5., 6.])\n\nprint(poisson_bracket_darboux(harm_osc_hamiltonian, lambda q, p: q[1])(q_3d, p_3d))\n\n-5.0\n\n\nFor easy integration, the right hand side of the Hamiltonian equations of motion can be fed to the odeint integrator from the jax.experimental.ode module. The initial conditions can be specified as an array or a pytree of arrays. In this case, a tuple y0 = (q0, p0) of the initial values of the position and momentum is convenient as it can be passed as a *args to the functions we defined on the phase space.\nThe first argument of odeint is a function to evaluate the time derivative of the solution y at time t. In the geometric interpretation, we talk about a vector field. Here we can use the poisson_bracket_darboux function applied on the functions we defined on the phase space.\n\nfrom jax.experimental.ode import odeint\n\nt = jnp.linspace(0., 10., 101)\ny0 = (1., 0.)\n\ndef harm_osc_hamiltonian_vector_field(y, t):\n    return (poisson_bracket_darboux(position, harm_osc_hamiltonian)(*y),\n            poisson_bracket_darboux(momentum, harm_osc_hamiltonian)(*y))\n\ny = odeint(harm_osc_hamiltonian_vector_field, y0, t)\n\nWe can plot the solution to check that we obtained the expected sinusoidal oscillations."
  },
  {
    "objectID": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#gravity",
    "href": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#gravity",
    "title": "Hamiltonian mechanics with JAX",
    "section": "Gravity",
    "text": "Gravity\nTo explore more features of Hamiltonian mechanics, we need a problem with more dimensions. In this age of space tourism, the physics of a small test mass moving around in the gravitational field of a much more massive object is particularly relevant. “Much more massive” in this context means that we can assume that the very heavy body sits at rest and only the very light body orbits around it. The phase space of this system corresponds to the position \\(\\vec{q}\\) and momentum \\(\\vec{p}\\) of the test mass, where we have set the origin at the position of the heavy object. The Hamiltonian is given by\n\\[H(\\vec{q}, \\vec{p}) = \\frac{\\Vert\\vec{p}\\Vert^2}{2m} - \\frac{GmM}{\\Vert\\vec{q}\\Vert} \\tag{5}\\]\nwhere \\(m\\) and \\(M\\) are the masses of the light and heavy objects respectively, and \\(G\\) is the gravitational constant. Since I am a mathematical physicist, I like to use a natural unit system where \\(G\\cdot M=e=c=\\hbar=k_B=1\\) and cows are spherical with radius 1. For convenience, we also set \\(m=1\\).\nIf we wanted to use the Poisson bracket to compute the Hamiltonian vector field like we did for the one-dimensional harmonic oscillator, we would have to do it component by component, so it is more practical to use the gradients of \\(H\\) directly in the equations of motion (1).\n\ndef gravitation_hamiltonian(q, p):\n    return 0.5*jnp.dot(p, p) - 1/jnp.sqrt(jnp.dot(q, q))\n\nr0 = 1.\np0 = 1.2\nt = jnp.linspace(0., 15., 101)\ny0 = (jnp.array([r0, 0., 0.]), jnp.array([0., p0, 0.]))\n\ndef grav_hamiltonian_vector_field(y, t):\n    return (grad(gravitation_hamiltonian, argnums=1)(*y),\n          -grad(gravitation_hamiltonian, argnums=0)(*y))\n  \ny = odeint(grav_hamiltonian_vector_field, y0, t)\n\nThe initial conditions were chosen so that the orbital plane is orthogonal to the \\(z\\)-axis, and we can easily plot the orbit.\n\n\n\n\n\nIt is encouraging to see that this numerical solution follows Kepler’s first law and draws an elliptic orbit where the center of the gravitational field is one of the focal points.\nSince the Hamiltonian is invariant under rotations, Noether’s theorem guarantees that the angular momentum is conserved. Using equation (3), we can quickly check that the time derivative of its \\(z\\)-component (the only non-zero component here) \\(\\dot L_z = \\lbrace L_z, H \\rbrace\\) is zero numerically. This is a nice exercise to showcase the practicality of the vectorizing map vmap.\n\ndef angular_momentum(q, p):\n    return jnp.cross(q, p)\n\ndef L_z(q, p):\n    return angular_momentum(q, p)[3]\n\nfrom jax import vmap\nprint(vmap(poisson_bracket_darboux(L_z, gravitation_hamiltonian))(*y))\n\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0.]\n\n\nThese two examples illustrate the power of JAX to express concepts from differential geometry in code and run simulations efficiently, which facilitates the development of physics-based models. To explore that subject further, I can recommend to look into JAX, MD, a library for molecular dynamics simulations, or JAX-CFD, one for computational fluid dynamics."
  },
  {
    "objectID": "posts/2021-01-13-anomaly-detection-in-protocol-deviations.html",
    "href": "posts/2021-01-13-anomaly-detection-in-protocol-deviations.html",
    "title": "Anomaly detection in protocol deviations",
    "section": "",
    "text": "The number of protocol deviations \\(n_{pdevs}\\) reported by an investigator site should depend linearly on the number of patients \\(n_{pats}\\) enrolled at that site, as more patients mean more chances of deviations, so these are the minimal attributes that should get collected prior to the analysis. The following table is a sample of the data we will use as an example.\n\n\n\n\n\n\n  \n    \n      \n      site_id\n      n_pats\n      n_pdevs\n    \n  \n  \n    \n      0\n      site_0\n      9\n      15\n    \n    \n      1\n      site_1\n      10\n      14\n    \n    \n      2\n      site_2\n      15\n      18\n    \n    \n      3\n      site_3\n      9\n      10\n    \n    \n      4\n      site_4\n      5\n      4\n    \n  \n\n\n\n\nA quick glance at the full dataset reveals there is indeed a relationship that looks linear, so a potential approach would be to build a regression model \\(n_{pdevs} \\sim \\theta \\cdot n_{pats}\\) and quantify how every observation of \\(n_{pdevs}\\) deviates from its estimation.\n\n\n\n\n\nFrom that scatterplot, it also appears that the residuals of a regression would not be iid. Rather, their variance would grow as the number of patients increases. This rules out least square regression, as it assumes iid normal residuals. Since we are dealing with count data, working with the Poisson distribution is a natural approach,\n\\[n_{pdevs} \\vert n_{pats} \\sim Poi(\\lambda(n_{pats})),\\]\nand we can set \\(\\lambda(n_{pats}) = \\theta \\cdot n_{pats}\\) to reflect our assumption of a linear relationship between \\(n_{pdevs}\\) and \\(n_{pats}\\). Note that a regular Poisson regression would fail to capture that linear relationship due to its exponential link function.\nIn this model, we immediately have \\(E\\left[n_{pdevs} \\vert n_{pats}\\right] = Var\\left[n_{pdevs} \\vert n_{pats}\\right] = \\theta \\cdot n_{pats}\\), which seems to reproduce the increasing spread of \\(n_{pdevs}\\).\nWe can infer the value of \\(\\theta\\) through maximum likelihood estimation and use the resulting conditional Poisson model at each site to compute the cumulative distribution function (CDF) of the observed numbers of protocol deviations.\n\n\nCode\ndef loss(par, n_pat, n_dev):\n    theta = tf.math.exp(par[0])\n    dist = tfd.Poisson(n_pat * theta)\n    return -tf.reduce_sum(dist.log_prob(n_dev))\n\ndef compute_cdf(par, n_pat, n_dev):\n    theta = tf.math.exp(par[0])\n    dist = tfd.Poisson(n_pat * theta)\n    return dist.cdf(n_dev)\n\n@tf.function\ndef loss_and_gradient(par, n_pat, n_dev):\n    return tfp.math.value_and_gradient(lambda par: loss(par, n_pat, n_dev), par)\n\n\ndef fit(n_pat, n_dev):\n    init = 2*tf.ones(1)\n    opt = tfp.optimizer.lbfgs_minimize(\n        lambda par: loss_and_gradient(par, n_pat, n_dev), init, max_iterations=1000\n    )\n    return opt\n\nn_pats = tf.constant(data['n_pats'], dtype=tf.float32)\nn_pdev = tf.constant(data['n_pdevs'], dtype=tf.float32)\n\nmle = fit(n_pats, n_pdev)\n\n#print(f\"converged: {mle.converged}\")\n#print(f\"iterations: {mle.num_iterations}\")\n\nx = np.linspace(0, 40)\npar = mle.position\ny = np.exp(par[0]) * x\n\ncdfs = compute_cdf(par, n_pats, n_pdev)\n\n\nThese CDF values are concentrated around 0 and 1, which makes this approach quite impractical and suggests that the variance of the model is lower than the variance of the data.\n\n\n\n\n\nThe low variance can be increased by treating \\(\\lambda(n_{pats})\\) as a random function, rather than a deterministic one. So we assume that \\(\\lambda(n_{pats})\\) is drawn from a gamma distribution, \\(\\lambda(n_{pats}) \\sim \\Gamma(\\alpha, \\beta)\\), where the rate parameter \\(\\beta\\) is inversely proportional to the expected number of protocol deviations from a given site, \\(\\beta = \\beta_{pat} / n_{pats}\\), in order to ensure linearity in \\(n_{pats}\\). In this context, maximum likelihood estimation would be a nightmare to implement (because of the rate parameters of the Poisson distribution) and probably not very stable, so it is best to turn to Bayesian inference via MCMC algorithms. We thus pick gamma priors for \\(\\alpha\\) and \\(\\beta_{pat}\\) with a shape parameters of 2 to prevent the corresponding Markov chains from drifting too close to zero, where pathological behaviors seem to occur with more permissive priors in this model.\n\n\nCode\nsites = tf.constant(data['site_id'])\nn_pats = tf.constant(data['n_pats'], dtype=tf.float32)\nn_pdev = tf.constant(data['n_pdevs'], dtype=tf.float32)\n\nmdl_pd = tfd.JointDistributionSequential([\n    #alpha\n    tfd.Gamma(2, 2, name='alpha'),\n    #beta_pt\n    tfd.Gamma(2, 2, name='beta_pt'),\n    #pdev rates for each patient\n    lambda beta_pt, alpha: tfd.Independent(\n        tfd.Gamma(alpha[...,tf.newaxis], beta_pt[...,tf.newaxis] / n_pats[tf.newaxis,...]),\n        reinterpreted_batch_ndims=1\n    ),\n    #observed pdevs\n    lambda rates: tfd.Independent(tfd.Poisson(rates), reinterpreted_batch_ndims=1)\n])\n\n\nWe can sample the posterior distribution of this model with a Hamiltonian Monte Carlo algorithm and assess the convergence of the Markov chains before computing the posterior probabilities of interest.\n\n\nCode\ndtype = tf.dtypes.float32\nnchain = 5\nburnin=3000\nnum_steps=10000\nalpha0, beta_pt0, rates0, _ = mdl_pd.sample(nchain)\ninit_state = [alpha0, beta_pt0, rates0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [0.01, 0.01, 0.01]]\ntarget_log_prob_fn = lambda *init_state: mdl_pd.log_prob(\n    list(init_state) + [tf.cast(n_pdev, dtype=dtype)])\n\n\nunconstraining_bijectors = 3*[tfb.Exp()]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=num_steps, burnin=burnin):\n    \n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.is_accepted\n               )\n\n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn,\n        num_leapfrog_steps=3,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin\n    )\n\n    # Sampling from the chain.\n    [alpha, beta_pt, rates], is_accepted = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return alpha, beta_pt, rates, is_accepted\n\nalpha, beta_pt, rates, is_accepted = run_chain(\n    init_state, step_size, target_log_prob_fn, unconstraining_bijectors)\n\nalpha_ = alpha[burnin:,:]\nalpha_ = tf.reshape(alpha_, [alpha_.shape[0]*alpha_.shape[1]])\nbeta_pt_ = beta_pt[burnin:,:]\nbeta_pt_ = tf.reshape(beta_pt_, [beta_pt_.shape[0]*beta_pt_.shape[1]])\nrates_ = rates[burnin:,:]\nrates_ = tf.reshape(rates_, [rates_.shape[0]*rates_.shape[1], rates_.shape[2]])\n\nrates_dist_ = tfd.Gamma(alpha_[:,tf.newaxis], beta_pt_[:, tf.newaxis] / n_pats[tf.newaxis,...])\nrates_cdf_ = rates_dist_.cdf(rates_)\n\nposterior = {}\nposterior['alpha'] = tf.transpose(alpha[burnin:, :]).numpy()\nposterior['beta_pt'] = tf.transpose(beta_pt[burnin:, :]).numpy()\nposterior['rate0'] = tf.transpose(rates[burnin:, :, 0])\nposterior['rate1'] = tf.transpose(rates[burnin:, :, 1])\nposterior['rate2'] = tf.transpose(rates[burnin:, :, 2])\n\naz_trace = az.from_dict(posterior=posterior)\n\nprint(f'MCMC acceptance rate: {is_accepted.numpy().mean()}')\n\naz.plot_trace(az_trace)\nplt.show()\n\n\nMCMC acceptance rate: 0.73832\n\n\n\n\n\nGiven a Markov chain sample \\((\\hat\\alpha, \\hat\\beta_{pat}, (\\hat\\lambda_i)_{i=1,\\dots,N})\\), where \\(i\\) indexes the investigator sites, we can evaluate the CDF of \\(\\Gamma(\\hat\\alpha, \\hat\\beta_{pat} / n_{pats, i})\\) at \\(\\hat\\lambda_i\\) and average these quantities along the whole Markov chain to obtain an indicator of over- and underreporting. This indicator corresponds to the rate tail area of the inferred Poisson rates under their posterior predictive distribution. Low values mean a risk of underreporting, and high values a risk of overreporting (see the last column of the following sample table).\n\n\n\n\n\n\n  \n    \n      \n      site\n      n_pats\n      n_pdev\n      mean_pdev_rate\n      std_pdev_rate\n      rate_tail_area\n    \n  \n  \n    \n      0\n      site_0\n      9\n      15\n      15.51\n      3.82\n      0.42\n    \n    \n      1\n      site_1\n      10\n      14\n      14.56\n      3.72\n      0.36\n    \n    \n      2\n      site_2\n      15\n      18\n      18.73\n      4.28\n      0.31\n    \n    \n      3\n      site_3\n      9\n      10\n      10.77\n      3.19\n      0.30\n    \n    \n      4\n      site_4\n      5\n      4\n      4.86\n      2.08\n      0.24\n    \n    \n      5\n      site_5\n      7\n      19\n      18.98\n      4.26\n      0.61\n    \n    \n      6\n      site_6\n      25\n      31\n      31.74\n      5.59\n      0.32\n    \n    \n      7\n      site_7\n      2\n      11\n      9.82\n      2.83\n      0.82\n    \n    \n      8\n      site_8\n      6\n      7\n      7.75\n      2.71\n      0.32\n    \n    \n      9\n      site_9\n      17\n      27\n      27.65\n      5.17\n      0.40\n    \n  \n\n\n\n\nThe distribution of the rate tail areas looks more convenient than in the first simple model. Not only did we add variance with a mixture model, but we also assess the inferred Poisson parameters rather than the observations, and the former are shrunk by their prior.\n\n\n\n\n\nThis metric also seems to agree with the intuition of what underreporting and overreporting should look like.\n\n\n\n\n\nWe can set thresholds for over- and underreporting alerts at .8 and .2 respectively to illustrate how an auditor could use this model to select investigator sites to focus on.\n\n\n\n\n\n\n\n\n\n\nThis method illustrates the potential of Bayesian modeling to supercharge a regression analysis toolbox with a variety of likelihood functions that can capture the intricacies of the data generating process and inference methods that are more flexible in quantifying uncertainties than the standard GLM methods. These properties are particularly helpful in practical tasks such as anomaly detection or risk management that combine expert insights with quantitative modeling."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "",
    "text": "If you plan to automate the execution of multible Bayesian inference jobs, typically to regularly update your posterior distribution as new data comes in, you might find that MCMC algorithms take too long to sample their chains. Variational inference can speed things up considerably (you can find a good introduction here), at the expense of converging only to an approximation of the true posterior, which is often good enough for practical applications.\nLately, I have been experimenting with TensorFlow-Probability that implement automatic differentiation variational inference, namely tfp.vi.fit_surrogate_posterior and tfp.layers, to see how to integrate them into some of my projects.\nI collected insights from various guides, tutorials, and code documentation, that I am summarizing here, mainly for future reference, but also for the benefit of people I will manage to convert to Bayesianism. The code is applied to a toy example of Bayesian logistic regression on simulated data, because it is helpful in this context to compare results to the true parameters of the data generating process."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#mcmc",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#mcmc",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "MCMC",
    "text": "MCMC\nIt is always good to start with a benchmark, so I collected an MCMC sample of the posterior distribution and computed the means and standard deviations of the model parameters.\n\n# Specify the model for Bayesian logistic regression.\nmdl_logreg = tfd.JointDistributionSequentialAutoBatched([\n    #betas\n    tfd.Sample(tfd.Normal(loc=0., scale=5.), X.shape[1]),\n    #alpha\n    tfd.Normal(loc=0., scale=20.),\n    #observations\n    lambda alpha, betas: tfd.Independent(\n        tfd.Bernoulli(logits=alpha + tf.tensordot(X, betas, axes=1)),\n        reinterpreted_batch_ndims=1)\n])\n\n# Specify the MCMC algorithm.\ndtype = tf.dtypes.float32\nnchain = 5\nb0, a0, _ = mdl_logreg.sample(nchain)\ninit_state = [b0, a0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [.1, .1]]\ntarget_log_prob_fn = lambda *init_state: mdl_logreg.log_prob(\n    list(init_state) + [y])\n\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Identity(),\n    tfb.Identity(),\n]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=8000, burnin=1000):\n\n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.target_log_prob,\n            pkr.inner_results.inner_results.leapfrogs_taken,\n            pkr.inner_results.inner_results.has_divergence,\n            pkr.inner_results.inner_results.energy,\n            pkr.inner_results.inner_results.log_accept_ratio\n               )\n  \n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin,\n      step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n          inner_results=pkr.inner_results._replace(step_size=new_step_size)),\n      step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n      log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio\n    )\n\n    chain_state, sampler_stat = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return chain_state, sampler_stat\n\n# Run the chain\nsamples, sampler_stat = run_chain(\n    init_state, step_size, target_log_prob_fn, unconstraining_bijectors)\n\n# using the pymc3 naming convention\nsample_stats_name = ['lp', 'tree_size', 'diverging', 'energy', 'mean_tree_accept']\nsample_stats = {k:v.numpy().T for k, v in zip(sample_stats_name, sampler_stat)}\nsample_stats['tree_size'] = np.diff(sample_stats['tree_size'], axis=1)\n\nvar_name = ['beta', 'alpha']\nposterior = {k:np.swapaxes(v.numpy(), 1, 0) \n             for k, v in zip(var_name, samples)}\n\naz_trace = az.from_dict(posterior=posterior, sample_stats=sample_stats)\n\naz.plot_trace(az_trace)\nplt.show()"
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.vi",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.vi",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "Variational inference with tfp.vi",
    "text": "Variational inference with tfp.vi\nThe dedicated tool for variational inference in TensorFlow-Probability, tfp.vi.fit_surrogate_posterior, requires a similar amount of preparatory work as tfp.mcmc algorithms. The specification of the target posterior is actually the same.\n\nmdl_logreg = tfd.JointDistributionSequentialAutoBatched([\n    #betas\n    tfd.Sample(tfd.Normal(loc=0., scale=5.), X.shape[1]),\n    #offset\n    tfd.Normal(loc=0., scale=20.),\n    #observations\n    lambda offset, betas: tfd.Independent(\n        tfd.Bernoulli(logits=offset + tf.tensordot(X, betas, axes=1)),\n        reinterpreted_batch_ndims=1)\n])\n\nunnormalized_log_prob = lambda *x: mdl_logreg.log_prob(x + (y,))\n\nThen, instead of specifying a Markov chain, we have to define a variational family of surrogate posterior candidates. This can require quite a bit of work, but if our model has been built with a joint distribution list and we are happy with a mean field approximation (this is usually the case if we care only about the marginal posterior distributions of the individual model parameters and not their correlation), the TensorFlow tutorial on modeling with joint distributions provides a helper function to do that. Note that if the support of the distributions is not a full \\(\\mathbb{R}^n\\), we have to implement unconstraining bijectors. The same tutorial shows how to do it.\n\n# Build meanfield ADVI for a jointdistribution\n# Inspect the input jointdistribution and replace the list of distribution with\n# a list of Normal distribution, each with the same shape.\ndef build_meanfield_advi(jd_list, observed_node=-1):\n    \"\"\"\n    The inputted jointdistribution needs to be a batch version\n    \"\"\"\n    # Sample to get a list of Tensors\n    list_of_values = jd_list.sample(1)  # <== sample([]) might not work\n\n    # Remove the observed node\n    list_of_values.pop(observed_node)\n\n    # Iterate the list of Tensor to a build a list of Normal distribution (i.e.,\n    # the Variational posterior)\n    distlist = []\n    for i, value in enumerate(list_of_values):\n        dtype = value.dtype\n        rv_shape = value[0].shape\n        loc = tf.Variable(\n            tf.random.normal(rv_shape, dtype=dtype),\n            name='meanfield_%s_mu' % i,\n            dtype=dtype)\n        scale = tfp.util.TransformedVariable(\n            tf.fill(rv_shape, value=tf.constant(0.02, dtype)),\n            tfb.Softplus(),\n            name='meanfield_%s_scale' % i,\n        )\n\n        approx_node = tfd.Normal(loc=loc, scale=scale)\n        if loc.shape == ():\n            distlist.append(approx_node)\n        else:\n            distlist.append(\n              # TODO: make the reinterpreted_batch_ndims more flexible (for \n              # minibatch etc)\n              tfd.Independent(approx_node, reinterpreted_batch_ndims=1)\n            )\n\n    # pass list to JointDistribution to initiate the meanfield advi\n    meanfield_advi = tfd.JointDistributionSequential(distlist)\n    return meanfield_advi\n\nIt remains to choose an optimizer and set a few hyperparameters such as the number of optimization steps, the sample size used to estimate the loss function, and the learning rate of the optimizer. To better tune those and then to assess convergence, it can be helpful to enrich the trace function with statistics of the variational distribution.\n\nmeanfield_advi = build_meanfield_advi(mdl_logreg, observed_node=-1)\n\n# Check the logp and logq\nadvi_samples = meanfield_advi.sample(4)\nprint([\n  meanfield_advi.log_prob(advi_samples),\n  unnormalized_log_prob(*advi_samples)\n  ])\n\n# Specify a trace function that collects statistics during inference and an optimizer\ntrace_fn = lambda x: (x.loss, meanfield_advi.mean(), meanfield_advi.stddev())\nopt = tf.optimizers.Adam(learning_rate=.08)\n\n#@tf.function(experimental_compile=True)\ndef run_approximation():\n    loss_ = tfp.vi.fit_surrogate_posterior(\n                unnormalized_log_prob,\n                surrogate_posterior=meanfield_advi,\n                optimizer=opt,\n                sample_size=50,\n                num_steps=200,\n                trace_fn=trace_fn\n    )\n    return loss_\n\nloss_, q_mean_, q_std_ = run_approximation()\n\n\n\n\n\n\nThe loss itself is obviously a good indicator of convergence, but the model parameters seem to need a few more iterations to reach a steady state of the optimizer.\n\n\n\n\n\nThe standard deviation estimates exhibit some more noise.\n\n\n\n\n\nTo reduce the noise, one can try to increase the sample size used to estimate the loss function, or decrease the learning rate. The price for both actions is a slower convergence, so the number of iterations would need to be adjusted accordingly. Implementing a learning rate schedule could offer a trade-off."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.layers",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.layers",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "Variational inference with tfp.layers",
    "text": "Variational inference with tfp.layers\nIf the model can be cast as a neural network with a prior distribution on the neuron parameters, chances are it can be expressed as a Keras model with dedicated tfp.layers, a TensorFlow tool for probabilistic machine learning.\nThe probabilistic layers tutorial covers a least squares regression example in details and was a good inspiration, especially the case 4: aleatoric & epistemic uncertainty. The biggest difference is in the specification of the prior, to which they assign learnable parameters.\nTo understand how to build a probabilistic machine learning model, we can start with the neural network expression of the logistic regression, namely a single neuron with a sigmoid activation, and “probabilize” it.\n\ntfk = tf.keras\n\nclassical_model = tf.keras.Sequential([\n    tfk.layers.Dense(1),\n    tfk.layers.Activation('sigmoid')\n])\n\nclassical_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n                        loss=tfk.losses.BinaryCrossentropy()\n                       )\n\nclassical_model.fit(X, y, epochs=50)\n\nThe output of this network is the probability parameter of a Bernoulli distribution that is fit to the observed data through minimization of the binary cross-entropy. The activation layer can be replaced with a tfp.layers.DistributionLambda layer that outputs a tfd.Bernoulli distribution directly, which can be fit through minimization of the negative log-likelihood (note that the activation can be skipped if we use the logit parameter).\nThe parameters of the logistic regression are encoded in the tfk.layers.Dense layer. Its probabilistic version, tfp.layers.DenseVariational, also specifies a prior distribution for these parameters as well as a variational family to estimate their posterior.\n\n# Define the negative log likelihood loss function for the `DistributionLambda`\n# head of the model.\nnegloglik = lambda y, rv_y: -rv_y.log_prob(y)\n\n# Define a function to constrain the scale parameters to the real positive\ndef constrain_scale(x):\n    c = np.log(np.expm1(1.))\n    return 1e-5 + tf.nn.softplus(c + x)\n\n# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\ndef posterior_mean_field(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    return tf.keras.Sequential([\n        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n            tfd.Normal(loc=t[..., :n],\n                       scale=constrain_scale(t[..., n:])\n                       #scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n                      ),\n            reinterpreted_batch_ndims=1)),\n    ])\n\n# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\ndef prior_ridge(kernel_size, bias_size, dtype=None):\n    return lambda _: tfd.Independent(\n        tfd.Normal(loc=tf.zeros(kernel_size + bias_size),\n                   scale=tf.concat([5*tf.ones(kernel_size),\n                                    20*tf.ones(bias_size)],\n                                   axis=0)),\n        reinterpreted_batch_ndims=1\n    )\n\n# Specify the model\nprobabilistic_model = tf.keras.Sequential([\n  tfp.layers.DenseVariational(units=1,\n                              make_posterior_fn=posterior_mean_field,\n                              make_prior_fn=prior_ridge,\n                              kl_weight=1/X.shape[0]\n                             ),\n  tfp.layers.DistributionLambda(lambda t: tfd.Bernoulli(logits=t)),\n])\n\nTo understand the kl_weight argument of tfp.layers.DenseVariational, we need to take a look at the mathematics behind variational inference that has been so far left out of this discussion. To estimate the posterior distribution \\(P(Z\\vert X)\\) of the parameters \\(Z\\) given the data \\(X\\), the variational inference algorithms implemented here look for the distribution \\(Q(Z)\\) in the variational family that minimizes the Kullback-Leibler divergence\n\\[\nD_{\\mathrm{KL}}(Q \\Vert P) = \\mathbb{E}_Q\\left[ \\log \\frac{Q(Z)}{P(Z\\vert X)} \\right]\n\\]\nof \\(P(Z\\vert X)\\) from \\(Q(Z)\\). This quantity still depends on the unknown posterior \\(P(Z\\vert X)\\), but minimizing it is equivalent to maximizing the evidence lower bound\n\\[\n\\mathrm{ELBO} = \\mathbb{E}_Q\\left[ \\log P(X \\vert Z) + \\log P(Z) - \\log Q(Z) \\right]\n\\]\nwhich depends only on the likelihood, the prior, and the variational distributions.\nWe prefer minimization problems, so we consider the negative ELBO loss function, and we make the dependency on individual data points explicit,\n\\[\n- \\mathrm{ELBO} =  \\mathbb{E}_Q\\left[ \\sum_i \\left( - \\log P(X_i \\vert Z) + \\frac{1}{N}( \\log Q(Z) - \\log P(Z))\\right) \\right]\n\\]\nwhere \\(N\\) is the number of data points. The first term in the sum corresponds to the negative log-likelihood passed as the loss argument to probabilistic_model.compile. The rest of the sum is a regularization implemented in tfp.layers.DenseVariational, where \\(Q(Z)\\) corresponds to the make_posterior_fn argument, \\(P(Z)\\) to make_prior_fn, and \\(N\\) to kl_weight=1/X.shape[0].\nAs a side note, the sample size argument in tfp.vi.fit_surrogate_posterior gives the number of points sampled from \\(Q\\) to compute a Monte Carlo estimate of the gradient of the ELBO.\nWith the Keras API, we can use callbacks to collect training statistics or implement early stopping policies, and we have access to the tf.data.Dataset API for batch training (kl_weight would need to be adapted accordingly). Here we use callbacks to record the parameters of the variational distribution during training.\n\ndef get_model_stats(probabilistic_model):\n    weights = probabilistic_model.layers[0].weights[0]\n    k = X.shape[1]\n    locs = weights[:k+1].numpy()\n    scales = constrain_scale(weights[k+1:]).numpy()\n    return locs, scales\n\nparams_history = []\n\nparams_callback = tfk.callbacks.LambdaCallback(\n    on_epoch_end=lambda epoch, logs: params_history.append(\n        np.array(get_model_stats(probabilistic_model))))\n\n\nprobabilistic_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n                            loss=negloglik\n                           )\n\nhistory = probabilistic_model.fit(X, y,\n                                  epochs=200,\n                                  callbacks=[params_callback])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence of the standard deviation parameters is a bit slow but could be improved with a more informed choice of starting values."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#conclusion",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#conclusion",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "Conclusion",
    "text": "Conclusion\nThe means and standard deviations of the posterior distributions inferred with tfp.mcmc, tfp.vi and tfp.layers (bnn) are summarized in the following table.\n\n\n\n\n\n\n  \n    \n      \n      true_value\n      mcmc_mean\n      mcmc_std\n      vi_mean\n      vi_std\n      bnn_mean\n      bnn_std\n    \n    \n      parameter\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      beta_0\n      1.000\n      1.095\n      0.145\n      1.094\n      0.125\n      1.108\n      0.115\n    \n    \n      beta_1\n      0.000\n      0.079\n      0.122\n      0.084\n      0.115\n      0.055\n      0.120\n    \n    \n      beta_2\n      -2.000\n      -1.840\n      0.184\n      -1.838\n      0.163\n      -1.835\n      0.146\n    \n    \n      alpha\n      1.000\n      1.223\n      0.142\n      1.203\n      0.131\n      1.218\n      0.124\n    \n  \n\n\n\n\nThe results are very similar across the three methods. Convergence is quite faster with variational inference, but it requires a bit more work to specify sensible variational families.\nWhile tfp.vi is applicable to a wider class of problems, tfp.layers gives access to Keras functionalities such as callbacks, and, more interestingly, batch training with the tf.data.Dataset API.\nThere is no definitive rule for which method to apply to which problem, but it is important to be aware of the limitations and benefits of variational inference algorithms before using them. In this example we knew the true parameters in advance, but in real applications, one should have validation procedures in place to ensure the variational family is large enough to capture the phenomenon of interest, for instance by comparison with MCMC results, or for prediction tasks with a test set where the labels/outcomes are known."
  },
  {
    "objectID": "posts/2021-06-05-gaussian-random-walks-with-tfp.html",
    "href": "posts/2021-06-05-gaussian-random-walks-with-tfp.html",
    "title": "Gaussian random walks",
    "section": "",
    "text": "def make_gaussian_random_walk(length, step_size=1.):\n    random_walk = tfd.TransformedDistribution(\n        tfd.TransformedDistribution(\n            tfd.Sample(tfd.Normal(loc=0, scale=step_size), sample_shape=(length-1,)),\n            tfb.Pad(paddings=((1,0),))\n        ),\n        tfb.Cumsum()\n        )\n    return random_walk\n\nThe sample method of the resulting tfp distribution allows us to quickly simulate realizations of a random walk.\n\n\n\n\n\nTo make sure that this implementation actually works, we can reproduce the analysis of the stochastic volatility model treated in the PyMC3 documentation, and described as an example in the original No-U-Turn sampler paper by Hoffman and Gelman. Incidentally, it is how I found out that starting at zero was important. At first I was not doing it, and it led to an overestimation of the volatility at the onset of the time series.\nThe goal is to model the volatility of the S&P 500 index.\n\n\n\n\n\nThe model is described in the PyMC3 example and the NUTS paper, so we just provide the TensorFlow-Probability version:\n\nstoch_vol_mdl = tfd.JointDistributionSequential([\n    #step_size\n    tfd.Exponential(10, name='step_size'),\n    #volatility\n    lambda step_size: make_gaussian_random_walk(observed_returns.shape[0], step_size),\n    #nu (degrees of freedom)\n    tfd.Exponential(0.1, name='nu'),\n    #returns\n    lambda nu, volatility, step_size: tfd.Independent(\n        tfd.StudentT(df=nu[...,tf.newaxis],\n                     loc=0,\n                     scale=tf.math.exp(volatility),\n                     name='returns'),\n        reinterpreted_batch_ndims=1\n    )\n])\n\nTo infer the posterior distributions, we also use the No-U-Turn sampler, and we can compare the marginal posteriors of the nu and step_size parameters as well as the estimated volatility over time with the results of the PyMC3 case study to gain confidence in the validity of the method.\n\n\nCode\n# Specify the MCMC algorithm.\ndtype = tf.dtypes.float32\nnchain = 5\nssize0, vol0, nu0, _ = stoch_vol_mdl.sample(nchain)\ninit_state = [ssize0, vol0, nu0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [.1, .1, .1]]\ntarget_log_prob_fn = lambda *init_state: stoch_vol_mdl.log_prob(\n    list(init_state) + [observed_returns])\n\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Exp(),\n    tfb.Identity(),\n    tfb.Exp()\n]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=1000, burnin=1000):\n\n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.target_log_prob,\n            pkr.inner_results.inner_results.leapfrogs_taken,\n            pkr.inner_results.inner_results.has_divergence,\n            pkr.inner_results.inner_results.energy,\n            pkr.inner_results.inner_results.log_accept_ratio,\n            pkr.inner_results.inner_results.is_accepted\n               )\n  \n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin,\n      step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n          inner_results=pkr.inner_results._replace(step_size=new_step_size)),\n      step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n      log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio\n    )\n\n    chain_state, sampler_stat = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return chain_state, sampler_stat\n\n# Run the chain\nsamples, sampler_stat = run_chain(\n    init_state, step_size, target_log_prob_fn, unconstraining_bijectors)"
  },
  {
    "objectID": "posts/2021-01-08-probabilistic assessment-of-safety-underreporting.html",
    "href": "posts/2021-01-08-probabilistic assessment-of-safety-underreporting.html",
    "title": "Probabilistic assessment of safety underreporting",
    "section": "",
    "text": "A clinical trial is usually run across several investigator sites, which are required to report to the trial sponsor adverse events experienced by enrolled subjects. This is necessary to assess the safety of the intervention under investigation. Underreporting from certain sites has been a recurrent issue. Trial sponsors and health authorities rely on audits and inspections to ensure completeness of the collected safety data. This effort can and should be informed by statistical analysis of the adverse event reporting process with a focus on identifying sites that report at lower rates.\nThe main hurdle is that you are looking for missing data that you do not know is missing. We developed a solution around an observational model for adverse event reporting and a way to characterizing outlying investigator sites.\nThe relevant data is the count of adverse events reported by every patient enrolled in a study and their site assignment. To illustrate our methodology, we use data from the control arm of NCT00617669, an oncology study from Project Data Sphere.\nBayesian data analysis combines expert knowledge on the process of interest, expressed as a probabilistic model, with observed data to infer posterior distributions of the model parameters. These posterior distributions allow us to quantify uncertainties and risks, and to compute posterior expectation values of quantities of interest.\nHierarchical models provide a natural framework for subgroup analysis, where similar entities such as the sites of a single study can share information while maintaining a certain degree of independence. In our situation, at the bottom of the hierarchy, the count of adverse events reported by the \\(n_i\\) patients of site \\(i\\) can be modelled with a Poisson distribution, \\(Y_i \\sim \\mathrm{Poi}(\\lambda_i)\\). The \\(N_{\\mathrm{sites}}\\) Poisson rates \\(\\lambda_i\\) can in turn be modelled as realizations of a random variable unique to the whole study with Gamma distribution \\(\\Gamma(\\alpha, \\beta)\\). The parameters \\(\\alpha\\) and \\(\\beta\\) are unknown, so we assume a vague prior for both of them, \\(\\alpha \\sim \\mathrm{Exp}(1)\\) and \\(\\beta \\sim \\mathrm{Exp}(10)\\). The full joint distribution \\[\nP(\\alpha, \\beta, \\lambda_i, Y_{i,j}) = P(\\alpha)P(\\beta)\\prod_{i=1}^{N_{\\mathrm{sites}}}P(\\lambda_i\\vert\\alpha,\\beta)\\prod_{j=1}^{n_i} P(Y_{i,j}\\vert \\lambda_i)\n\\] is summarized in the following graphical representation.\n\nNumerical modelling of such joint distributions is made easy by probabilistic programming libraries such as TensorFlow-Probability (utilized here), Stan, PyMC3 or Pyro.\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfb = tfp.bijectors\ntfd = tfp.distributions\n\nsites = tf.constant(data['site_number'])\nobserved_ae = tf.constant(data['ae_count_cumulative'])\n\nunique_sites, sites_idx, sites_counts = tf.unique_with_counts(sites)\n\nae_per_site = tf.RaggedTensor.from_value_rowids(\n    values=observed_ae,\n    value_rowids=sites_idx)\n\nmdl_ae = tfd.JointDistributionSequential([\n    #alpha\n    tfd.Gamma(1, 1, name='alpha'),\n    #beta\n    tfd.Gamma(1, 10, name='beta'),\n    #Poisson rates for each sites\n    lambda beta, alpha: tfd.Sample(tfd.Gamma(alpha, beta), sample_shape=unique_sites.shape, name='rates'),\n    #observed AEs\n    lambda rates: tfd.Independent(\n        tfd.Poisson(tf.gather(rates, sites_idx, axis=-1)), reinterpreted_batch_ndims=1, name='observations')\n])\n\nIn Bayesian inference, the analytical derivation of the posterior distribution is often impossible, but the same probabilistic programming libraries provide efficient implementations of MCMC algorithms that return samples of the posterior distribution.\n\n\nCode\ndtype = tf.dtypes.float32\nnchain = 10\nburnin=1000\nnum_steps=10000\nalpha0, beta0, rates0, _ = mdl_ae.sample(nchain)\ninit_state = [alpha0, beta0, rates0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [.1, .1, .1]]\ntarget_log_prob_fn = lambda *init_state: mdl_ae.log_prob(\n    list(init_state) + [tf.cast(observed_ae, dtype=dtype)])\n\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Exp(),\n    tfb.Exp(),\n    tfb.Exp()\n]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=num_steps, burnin=burnin):\n    \n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.is_accepted\n               )\n\n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn,\n        num_leapfrog_steps=3,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin\n    )\n\n    # Sampling from the chain.\n    [alpha, beta, rates], is_accepted = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return alpha, beta, rates, is_accepted\n\n\nTo assess convergence, we sample several chains that we can inspect visually (here with the ArviZ package) to make sure that they converge to the same distribution, mix well, and do not display pathological autocorrelations.\n\n\n\n\n\nIf we have to monitor several studies, we might want to automate that process. In that case, we can compute statistics of the sampled chains such as effective sample sizes or \\(\\hat{R}\\) and implement automatic checks, for instance that \\(\\hat{R}\\) is sufficiently close to 1.\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      3001\n      3.193\n      1.246\n      1.081\n      5.565\n      0.022\n      0.016\n      3146.0\n      3146.0\n      2849.0\n      3764.0\n      1.0\n    \n    \n      3002\n      3.050\n      0.616\n      1.964\n      4.250\n      0.006\n      0.004\n      11787.0\n      11787.0\n      11502.0\n      17056.0\n      1.0\n    \n    \n      3003\n      6.043\n      1.695\n      3.084\n      9.280\n      0.022\n      0.015\n      6031.0\n      6031.0\n      5772.0\n      9765.0\n      1.0\n    \n    \n      3004\n      12.764\n      2.030\n      9.037\n      16.628\n      0.014\n      0.010\n      19819.0\n      19819.0\n      19489.0\n      28664.0\n      1.0\n    \n    \n      alpha\n      1.774\n      0.223\n      1.364\n      2.192\n      0.003\n      0.002\n      6352.0\n      6352.0\n      6253.0\n      16128.0\n      1.0\n    \n    \n      beta\n      0.119\n      0.017\n      0.087\n      0.151\n      0.000\n      0.000\n      7576.0\n      7576.0\n      7441.0\n      18742.0\n      1.0\n    \n  \n\n\n\n\nFrom the samples \\((\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\lambda}_i)\\) of the Markov chain, we can estimate the posterior risk of underreporting. One way to do it is to compute the left tail area of each \\(\\lambda_i\\) (remember that the index \\(i\\) enumerates the sites) under the distribution \\(\\Gamma(\\hat{\\alpha}, \\hat{\\beta})\\) and average it along the trace of the Markov chain. This corresponds to the probability that a Poisson rate drawn randomly from the study level distribution falls below the inferred Poisson rate of site \\(i\\), or, more explicitly, that a reference site from the same study would report less adverse events.\n\n\n\n\n\n\n  \n    \n      \n      site\n      mean_ae_rate\n      std_ae_rate\n      rate_tail_area\n      observed_ae\n    \n  \n  \n    \n      0\n      3001\n      3.20\n      1.25\n      0.09\n      [4, 1]\n    \n    \n      1\n      3002\n      3.05\n      0.62\n      0.08\n      [2, 2, 1, 2, 5, 5, 5, 1]\n    \n    \n      2\n      3003\n      6.04\n      1.69\n      0.22\n      [7, 4]\n    \n    \n      3\n      3004\n      12.76\n      2.03\n      0.52\n      [3, 27, 8]\n    \n    \n      4\n      3005\n      9.35\n      2.10\n      0.37\n      [12, 6]\n    \n    \n      5\n      3006\n      3.69\n      1.32\n      0.11\n      [2, 4]\n    \n    \n      6\n      3007\n      6.03\n      2.31\n      0.22\n      [5]\n    \n    \n      7\n      3008\n      17.95\n      1.72\n      0.69\n      [11, 4, 16, 31, 23, 23]\n    \n    \n      8\n      3009\n      6.52\n      1.76\n      0.24\n      [6, 6]\n    \n    \n      9\n      3010\n      14.71\n      0.92\n      0.59\n      [21, 10, 6, 17, 10, 7, 26, 19, 18, 1, 18, 23, ...\n    \n  \n\n\n\n\nA lower value of this rate tail area thus indicates a higher risk of underreporting. This metric can be used by auditors and inspectors to prioritize their activities. Moreover, since it is the probability of a specified event, one can immediately compare the rate tail areas of sites from different studies. This is especially interesting for quality programs that oversee several trials.\nThis approach demonstrates the flexibility of Bayesian methods to build models that answer specific questions about a given process. In this example, the reporting rates of the different sites are the quantities of interest, but they are unobserved and have to be inferred from the available data with a mathematical model. The user-friendly API of modern probabilistic programming libraries combined with efficient inference algorithms have been making this type of workflow much easier than in the past and will certainly fuel a broader adoption in sectors that have not been traditionally driven by quantitative insights."
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html",
    "title": "Bayesian Poisson factorization",
    "section": "",
    "text": "A lot of recommender systems are built on matrix factorization models, where the partially observed matrix of user/item interactions is approximated by a product of matrices encoding latent characteristics of users and items. They can be corrected by user and item bias terms, and modified by activation functions that map to the data type of the observed interactions (e.g. binary kudos on Strava activities, counts of visits of a YouTube channel, ratings on Tripadvisor, or time spent watching a TikTok video before swiping up).\nIn machine learning, these matrix factorizations are often implemented as embeddings into latent spaces followed by scalar products of the user latent vectors by the item latent vectors, and the model is fit to historical data with a gradient descent algorithm. A Keras tutorial on collaborative filtering describes the methodology with the Movielens dataset.\nWhile the resulting model only provides point estimates of future interactions between users and items, a Bayesian treatment of the problem would add an approximation of the uncertainty of these estimates. Formally, we could express the same model with priors on the vector embeddings in a probabilistic programming library and sample the posteriors with an MCMC algorithm, but computing the likelihood of past interactions can be impractical for very large datasets. Moreover, due to the invariance of the model under permutations of the embedding dimensions, MCMC sampling of the multimodal posterior would be a nightmare. On the other hand, minimizing the Kullback-Leibler divergence \\(D_{KL}\\left(Q(Z) \\Vert P(Z\\vert X)\\right)\\) between a surrogate posterior and the true posterior distribution through variational inference produces a mode-seeking behavior (see for instance these lecture notes), a bit like gradient descent in “classical” machine learning finds a local minimum.\nIn this blog post, we will be exploring how to implement a model inspired by Gopalan, Hofman and Blei (Scalable Recommendation with Poisson Factorization) with TensorFLow Probability."
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html#bayesian-models-and-variational-inference",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html#bayesian-models-and-variational-inference",
    "title": "Bayesian Poisson factorization",
    "section": "Bayesian models and variational inference",
    "text": "Bayesian models and variational inference\nAs a reminder, minimizing the (intractable) Kullback-Leibler divergence \\(D_{KL}\\left(Q(Z) \\Vert P(Z\\vert X)\\right)\\) between the variational distribution \\(Q(Z)\\) and the true posterior \\(P(Z\\vert X)\\) is equivalent to maximizing the (computable) evidence lower bound\n\\[\n\\mathrm{ELBO} = \\mathbb{E}_Q\\left[ \\log P(X \\vert Z) \\right] - D_{KL}(Q(Z) \\Vert P(Z)),\n\\]\nwhere the first term is the expectation under the surrogate distribution of the likelihood function of the model and the second term the negative Kullback-Leibler divergence between the surrogate distribution and the prior distribution of the model parameters.\nIn the mean field approximation, where we assume that the variational distribution \\(Q\\) factorizes over the latent variables, we can implement variational inference with probabilistic layers from the tfp.layers module, which will automatically keep track of the variational parameters during training. These layers are conveniently combined in a Keras model, where the last layer is typically a distribution layer corresponding to the observed data \\(X\\), and from which which we can use the log_prob method to compute the log-likelihood of the model. In their forward mode, the probabilistic layers draw samples from the surrogate distribution \\(Q\\) they implement that we can use to compute Monte-Carlo estimates of \\(\\mathbb{E}_Q\\left[ \\log P(X \\vert Z) \\right]\\), the first part of the ELBO function.\nThe second part, \\(D_{KL}(Q(Z) \\Vert P(Z))\\), can be implemented either through an activity regularizer, activity_regularizer = tfpl.KLDivergenceRegularizer(prior_distribution), or through a custom loss term added in the call method of the layer. The former is suitable for global parameters of the model that are shared by all training examples, and the latter is required for latent variables associated to specific training examples.\nWe will wrap up these initial theoretical considerations with a couple of observations that are important for a good implementation of these methods. The first is that Keras models are trained through minimization of a loss function, so instead of maximizing the ELBO, we will be minimizing the negative ELBO. Concretely, the loss function passed to the model.compile method will be the negative log-likelihood, and \\(D_{KL}(Q(Z) \\Vert P(Z))\\) will be added to the loss rather than subtracted from it. The second is that Keras model training routines evaluate the loss function as a sum over the training examples. While this is straightforward for the likelihood part of the loss when observations are assumed to be conditionally independent,\n\\[\n- \\log P(X \\vert Z) = - \\sum_i  \\log P(X_i \\vert Z),\n\\]\nextra care needs to be taken for the Kullback-Leibler term\n\\[\nD_{KL}(Q(Z) \\Vert P(Z)) = \\mathbb{E}_Q\\left[ \\log Q(Z) - \\log P(Z))\\right],\n\\]\nwhich has to be expressed as a sum over the training examples like the log-likelihood. In practice, this can often be achieved through a weighted sum. For instance, if we have only parameters that are shared by all training examples, we can apply a \\(1/N\\) weight, where \\(N\\) is the number of data points \\(X_i\\),\n\\[\n- \\mathrm{ELBO} =  \\mathbb{E}_Q\\left[ \\sum_i \\left( - \\log P(X_i \\vert Z) + \\frac{1}{N}( \\log Q(Z) - \\log P(Z))\\right) \\right].\n\\]\nFor parameters linked to only subsets of the data points, or even individual data points (we often speak of latent variables), a bit of algebra might be needed to find the proper weights.\nConstant weights can be specified as optional parameters of methods like tfpl.KLDivergenceRegularizer, but non-uniform weights have to be coded in the add_loss method of the Keras model, which is the reason why this approach is required as mentioned earlier."
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-embeddings",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-embeddings",
    "title": "Bayesian Poisson factorization",
    "section": "Probabilistic embeddings",
    "text": "Probabilistic embeddings\nWhile a standard embedding maps discrete values to vectors in a latent space, a probabilistic embedding specifies a probability distribution over the latent space. It can be constructed as a sequence of a parameter layer, that contains the variational distribution parameters, and a TensorFlow Probability distribution layer that encapsulates the sample method and the prior distribution for the KL term. The parameter layer can be expressed as a standard embedding into a space of dimension \\(n_{params} \\times D\\), where \\(D\\) is the dimension of the original latent space, and \\(n_{params}\\) the number of variational parameters of the corresponding component of the variational distribution \\(Q\\).\nA custom Keras layer wraps the construction of a probabilistic embedding, with its __init__ method receiving the hyperparameters. In the following code example, we picked Gamma distributions for both the priors and the variational distributions. Note how the KL term is added to the loss with self.add_loss and weighted by latent_kl_weights passed as an argument to the call method of the custom Keras layer. It allows us to specify a unique weight for each training example from inputs.\n\nclass GammaEmbedding(tfkl.Layer):\n    def __init__(self, num_classes, embedding_size,\n                 embedding_concentration, embedding_rate,\n                 **kwargs):\n        super(GammaEmbedding, self).__init__(**kwargs)\n        \n        self.embedding_parameters = tfkl.Embedding(\n            num_classes,\n            2 * embedding_size,\n            embeddings_initializer=\"he_normal\"\n        )\n        \n        self.embedding_distribution = tfpl.DistributionLambda(\n            lambda x: tfd.Independent(\n                tfd.Gamma(tf.math.exp(x[:, :embedding_size]),\n                          rate=tf.math.exp(x[:, embedding_size:])),\n                reinterpreted_batch_ndims=1)\n        )\n        \n        self.embedding_prior = tfd.Independent(\n            tfd.Gamma(\n                embedding_concentration *\n                tf.ones(shape=(embedding_size,), dtype=tf.float32),\n                rate=embedding_rate),\n            reinterpreted_batch_ndims=1\n        )\n            \n    def __call__(self, inputs, latent_kl_weights):\n        embedding_param = self.embedding_parameters(inputs)\n        embedding_distribution = self.embedding_distribution(embedding_param)\n        self.add_loss(\n            tf.reduce_sum(latent_kl_weights *\n                embedding_distribution.kl_divergence(self.embedding_prior)\n                         )\n        )\n        return embedding_distribution\n\nThe user and item bias terms mentioned earlier could be implemented as one-dimensional probabilistic embeddings and added to the scalar products of user and item embeddings, but Gopalan et al. suggest to introduce these degrees of freedom as random rate parameters of the embedding Gamma distributions with a hierarchical model construction. Compared to the simple Gamma embedding where the embedding priors were fixed and specified in the __init__ method, we now need them to evolve with the variational parameters of their parent distribution during training, so they need to be dynamically computed in the __call__ method. To model the random rate parameter, we can make use of the Gamma embedding layer we already constructed, with an embedding dimension of 1.\n\nclass RateAdjustedGammaEmbedding(tfkl.Layer):\n    def __init__(self, num_classes, embedding_size,\n                 parent_concentration, parent_rate,\n                 embedding_concentration, **kwargs):\n        super(RateAdjustedGammaEmbedding, self).__init__(**kwargs)\n        \n        self.embedding_size = embedding_size\n        \n        self.rate_distribution = GammaEmbedding(num_classes=num_classes,\n                                                embedding_size=1,\n                                                embedding_concentration=\n                                                parent_concentration,\n                                                embedding_rate=parent_rate\n                                               )\n        \n        \n        self.embedding_concentration = embedding_concentration\n        \n        self.embedding_parameters = tfkl.Embedding(\n            num_classes,\n            2 * embedding_size,\n            embeddings_initializer=\"he_normal\"\n        )\n        \n        self.embedding_distribution = tfpl.DistributionLambda(\n            lambda x: tfd.Independent(\n                tfd.Gamma(tf.math.exp(x[:, :embedding_size]),\n                          rate=tf.math.exp(x[:, embedding_size:])),\n                reinterpreted_batch_ndims=1\n            )\n        )\n            \n    def __call__(self, inputs, latent_kl_weights):\n        embedding_param = self.embedding_parameters(inputs)\n        embedding_distribution = self.embedding_distribution(embedding_param)\n\n        embedding_rate = self.rate_distribution(inputs, latent_kl_weights)\n        embedding_prior = tfd.Independent(\n            tfd.Gamma(self.embedding_concentration,\n                      rate=embedding_rate * tf.ones((1, self.embedding_size))),\n            reinterpreted_batch_ndims=1)\n        self.add_loss(\n            tf.reduce_sum(latent_kl_weights *\n                          embedding_distribution.kl_divergence(embedding_prior))\n        )\n        \n        return embedding_distribution"
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-recommender",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-recommender",
    "title": "Bayesian Poisson factorization",
    "section": "Probabilistic recommender",
    "text": "Probabilistic recommender\nWith the probabilistic embeddings defined as custom layers, the full model only needs a few lines of code. The user and movie embeddings are constructed as rate-adjusted Gamma embeddings, and their scalar product will be the rate of the Poisson distribution that generates the observations, implemented as a distribution lambda layer.\nThe KL weights need to be passed to the corresponding probabilistic layers, so we need to include them in the input of the model, for instance in two additional columns. For the user part, we observe that the KL divergence term is decomposed as a sum over user terms,\n\\[\n\\mathbb{E}_Q\\left[ \\log Q(Z_{users}) - \\log P(Z_{users})) \\right] = \\sum_u \\mathbb{E}_Q\\left[ \\log Q(Z_u) - \\log P(Z_u)) \\right],\n\\]\nbut we need to express it as a sum over all user/movie interactions of the training set. If we simply replace the sum, we are counting the same user once for every movie they have rated, so we can rescale these terms by this number,\n\\[\n\\mathbb{E}_Q\\left[ \\log Q(Z_{users}) - \\log P(Z_{users})) \\right] = \\sum_i \\mathbb{E}_Q\\left[ \\log Q(Z_{u[i]}) - \\log P(Z_{u[i]})) \\right]\\frac1{N_{u[i]}},\n\\]\nwhere \\(u[i]\\) denotes the user of interaction \\(i\\), and \\(N_{u[i]}\\) the number of movies rated by this user. The KL weights of the movie part can be derived in the same way.\n\nclass ProbabilisticRecommender(tfk.Model):\n    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n        super(ProbabilisticRecommender, self).__init__(**kwargs)\n        self.num_users = num_users\n        self.num_movies = num_movies\n        self.embedding_size = embedding_size\n        \n        self.user_embedding = RateAdjustedGammaEmbedding(\n            num_users,\n            embedding_size,\n            parent_concentration=1.,\n            parent_rate=.8,\n            embedding_concentration=1.\n        )\n        \n        self.movie_embedding = RateAdjustedGammaEmbedding(\n            num_movies,\n            embedding_size,\n            parent_concentration=1.,\n            parent_rate=.8,\n            embedding_concentration=1.\n        )\n\n        self.head = tfpl.DistributionLambda(lambda t: tfd.Poisson(t))\n        \n    def call(self, inputs):\n        user_vector = self.user_embedding(inputs[:, 0], inputs[:, 2])\n        movie_vector = self.movie_embedding(inputs[:, 1], inputs[:, 3])\n        dot_user_movie = tf.reduce_sum(user_vector * movie_vector, axis=-1)\n        return self.head(dot_user_movie)\n\nAs mentioned in the preliminary observations, this model requires a negative log-likelihood loss function, otherwise it is straightforward to train it like any Keras model.\n\nEMBEDDING_SIZE = 20\n\nBATCH_SIZE = 1024\n\nnegloglik = lambda y, rv_y: -rv_y.log_prob(y)\n\nprob_model = ProbabilisticRecommender(num_users,\n                                      num_movies,\n                                      embedding_size=EMBEDDING_SIZE\n                                     )\nprob_model.compile(\n    loss=negloglik, optimizer=tfk.optimizers.Adam(learning_rate=0.01)\n)\n\nOnce the model has been trained, we can call it on new user/item pairs to produce a Poisson distribution of posterior predicted observations. We can directly sample user ratings from this distribution with .sample(), or call its .rate_parameter() method to find the posterior predicted rate. The latter offers a higher resolution to rank items for a given user (it is a continuous variable rather than an integer) and is therefore more practical for recommender systems.\nWhen the model is called, each probabilistic layer returns a single sample from its learned variational distribution. To estimate the posterior predicted Poisson rate of a user/item interaction, one can call the model several times to obtain a sample. In real world applications, drawing a single Poisson rate or only a few of them rather than estimating the posterior mean to score an item might prove more useful as it offers a broader variety of suggestions to users whose tastes are less certain, namely with user embedding distributions of higher variance. It addresses the exploration-exploitation trade-off with a mechanism similar to Thompson sampling (see also this previous blog post).\nIn general, users who have provided less ratings or good ratings across a large spectrum of items will get less defined embeddings, and this approach will give them recommendations that explore the item landscape more broadly than for users with tighter embeddings. These users with better-known tastes will still get random suggestions relatively far from their usual preference, albeit less frequently than the users with less defined embeddings, but this is not something that happens with models based on classical embeddings, which always return the same results.\nThis mechanism is also interesting when acquiring new users who have not yet provided ratings. We can itialize their embeddings to match the priors, or learned distributions from similar users but with wider variance, and use this untrained model to draw recommendations that are compatible with our prior knowledge. When they start giving ratings, we can train the corresponding user embedding layers to incorporate the new knowledge, while freezing the item embedding layers for stability and increased speed.\nWith TensorFlow Probability layers, we can thus add a Bayesian flavor to more traditional recommender systems and address issues such as exploration-exploitation trade-offs or cold starts in a more principled way. From another angle, we can express probabilistic models such as matrix factorization models as Keras models and take advantage of the tf.data.Dataset API for batch training with potentially large datasets."
  },
  {
    "objectID": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html",
    "href": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html",
    "title": "Cleaning Strava data with Kalman smoothers",
    "section": "",
    "text": "Note: This is a second version of the article, edited on 2022-11-09, with a better method to reject outliers suggested by Isaac Skog.\nLike many cyclists and mountain bikers, I record and share my rides with Strava. When I use my electric mountain bike, I normally export the relevant data to Strava from the Bosch eBike Flow app that controls and monitors the performance of the motor and gets geolocation data from the mobile phone. I noticed that this data is sometimes affected by measurement errors that look like weird spikes on my routes. This is visible on this screenshot of a ride I did in Morgins last summer.\nTo the data scientist in me who likes to correlate how much he rides week on week with the amount of ice cream he eats, this is frustrating because these errors are reflected in the total distances and elevation gains.\nAccording to my satellite navigation guru friend Yannick, this can happen when the signal from a satellite bounces on cliffs before reaching the receiver, and I assume that the processing software puts strong priors on roads and trails that lead to funky corrections. For instance, the border crossing of Pas de Morgins seems to attract quite a lot of points from the nearby hillside.\nMeasurement errors can be somewhat corrected with signal processing techniques, but first we need to get the data. Strava allows you to export gpx files of your rides, which can be read with the gpxpy library. The result is a time series of latitude, longitude and elevation measurements collected every second (except when the motor automatically turns off when I spend too much time eating snacks or petting cows and alpacas), and as a preprocessing step we can normalize them.\nSome of the errors like the one at Pas de Morgins seem to be characterized by the track alternating between the real underlying trajectory and a single erroneous point. If we count the occurrences of the most frequent coordinates, that point at Pas de Morgins is precisely at the top of the list.\nTherefore the first obvious fix would be to exclude the most frequently repeated points and try to replace them through interpolation of the real trajectory, for instance with a Kalman filter, with the added benefit of reducing measurement noise on the rest of the trajectory.\nIn the version of the Kalman filter with no control input to the dynamical system, we assume that the data is generated by a latent Markov process (the state \\(z_{t+1}\\) at a given time step only depends on the previous step \\(z_t\\) and not the earlier ones) with normally distributed transitions,\n\\[\nz_{t+1} \\vert z_t \\sim \\mathcal{N} \\left( F\\, z_t + b, \\ \\Sigma_{tr} \\right),\n\\]\nwhere \\(F\\) is a (possibly time-dependent) transition matrix, \\(b\\) a bias vector and \\(\\Sigma_{tr}\\) the covariance matrix of the transition noise. We further assume that a similar Gaussian linear process generates the observations from a given latent state,\n\\[\nx_t \\vert z_t \\sim \\mathcal{N} \\left( H\\, z_t + c, \\ \\Sigma_{obs} \\right),\n\\]\nwhere \\(H\\) is an observation matrix, \\(c\\) a bias term and \\(\\Sigma_{obs}\\) the covariance of the observation process.\nWith these strong assumptions of normality and linear transformations, the posterior distribution of the latent states given observations can be derived through linear algebra operations. These operations are readily implemented in the tfd.LinearGaussianStateSpaceModel distribution in the TensorFlow Probability library. The forward_filter method runs a Kalman filter to compute the filtered marginal distribution \\(P(z_t \\vert x_{1..t})\\) conditioned only on past observations, and the posterior_marginal method runs a Kalman smoother to compute the filtered marginal distribution \\(P(z_t \\vert x_{1..T})\\) conditioned on the full history of observations, including the future ones. These algorithms are discussed in details in section 8.3 of Probabilistic Machine Learning: Advanced Topics. Since we have access to the whole GPS track, we will of course use the Kalman smoother. Conveniently, these methods also work if we condition only on a subset of the observations. Which observations should be ignored can be specified with an optional mask argument.\nThe first example in the official TensorFlow Probability documentation is precisely the tracking problem that interests us, where the latent space is the real position and the observation is the noisy measurement. Here we will increase the dimension of the latent space and add a velocity vector \\(\\mathbf{v}\\) and an acceleration vector \\(\\mathbf{a}\\) to the true position \\(\\mathbf{s} = (latitude, longitude, elevation)\\), so that \\(z = (\\mathbf{s}, \\mathbf{v}, \\mathbf{a})\\), and use transitions inspired by classical mechanics,\n\\[\nz_{t+1} \\vert z_t \\sim \\mathcal{N} \\left( (\\mathbf{s}_t + \\mathbf{v}_t, \\mathbf{v}_t + \\mathbf{a}_t, 0), \\Sigma_{tr} \\right),\n\\]\nwith a diagonal transition covariance matrix \\(\\Sigma_{tr}\\) that carries much more uncertainty in its acceleration components. This kind of model is useful to estimate the velocity and acceleration solely from the position measurements (simply taking finite differences would be very inaccurate given the measurement noise), and it also incorporates knowledge from the laws of physics on how past states are going to influence future states. In a nutshell, it assumes that the forces affecting the bike and its rider are subject to random changes (I can brake, accelerate or turn, or collide with external obstacles), and my velocity and position are going to be a solution to Newton’s second law of motion (\\(F = ma\\)). Such a model can extrapolate the future position from the current velocity, and correct the velocity and acceleration estimations from the measured positions.\nNote that to estimate the physical velocity and acceleration, it would be better to transform the spherical coordinates into cartesian coordinates, but for our purpose of error correction, it should be good enough to use the (locally normalized) spherical ones.\nIf we overlap the observed time series of normalized geographic coordinates with the filtered ones, it appears that the solid blocks on the observed values that correspond to the back and forth patterns do get corrected by the Kalman smoothing with masked repeated values.\nThe spikes caused by isolated errors are a bit attenuated by the filter, provided the observation noise scale parameter is large enough. If it is too small, extreme points are interpreted as legitimate measurements because they are unlikely to be observation errors under the model specifications. But too large a scale parameter blurs out the inferred trajectory as it cannot resolve the trail geometry.\nSo the trick is to run a first Kalman smoother with an observation noise scale parameter that is large enough to identify the individual outliers (while masking the repeated measurements that we already identified), followed by a second one with lower noise while masking the individual outliers in addition to the repeated measurements to infer the true trajectory. For the identification of individual outliers after the first smoothing, we can make use of the posterior marginal distributions \\(\\mathcal{N}(\\hat\\mu_t, \\hat\\Sigma_t)\\) returned by the smoother that should “explain” the observation \\(x_t\\) after multiplication by the observation matrix \\(H\\). Concretely, the quantity\n\\[Q = \\left(x_t - H\\, \\hat\\mu_t\\right)^T \\, \\left(H \\, \\hat\\Sigma_t \\, H^T \\right)^{-1} \\, \\left(x_t - H\\, \\hat\\mu_t\\right)\\]\nshould follow a \\(\\chi^2_3\\) distribution under the model assumptions, so we can use a chi-squared test to pick outliers. Note that the threshold of that test has to be set fairly low to catch all outliers, because of the larger noise scale paramter of the first smoother.\nThe posterior mean of the second smoother produces a clean estimation of the real trajectory, free of any weird spike. As a last step, we can rescale it and plot it on OpenStreetMap to admire the result and think of future bike rides!"
  },
  {
    "objectID": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html#acknowledgement",
    "href": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html#acknowledgement",
    "title": "Cleaning Strava data with Kalman smoothers",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nI would like to thank Maxime Baillifard for showing me his hometrails around Morgins, an area that seems to affect GPS signals like the Bermuda Triangle, Yannick Stebler for providing a rational explanation about signals bouncing on cliffs that debunked that myth, and Isaac Skog for suggesting to use a chi-squared test to reject outliers after reading a first version of this article that was using a less robust quadratic form."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "mountain biking\n\n\nsignal processing\n\n\ntensorflow-probability\n\n\nsatellite navigation\n\n\n\n\nA tale of mountain biking, paranormal activity and signal processing.\n\n\n\n\n\n\nNov 5, 2022\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nvariational inference\n\n\ntensorflow-probability\n\n\nmachine learning\n\n\n\n\nA probabilistic recommender system implemented with TensorFlow Probability layers.\n\n\n\n\n\n\nFeb 23, 2022\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJAX\n\n\nautograd\n\n\ndifferential geometry\n\n\nclassical mechanics\n\n\n\n\nAn application of automatic differentiation with JAX in classical mechanics.\n\n\n\n\n\n\nDec 4, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrandom walks\n\n\nbayesian modeling\n\n\ntime series\n\n\ntensorflow-probability\n\n\n\n\nA construction of a TensorFlow-Probability distribution implementing Gaussian random walks, with an application to stochastic volatility modeling.\n\n\n\n\n\n\nJun 5, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontextual bandits\n\n\nreinforcement learning\n\n\nbayesian modeling\n\n\nvariational inference\n\n\nprobabilistic machine learning\n\n\ntensorflow-probability\n\n\n\n\nAn introduction to Thompson sampling and how to implement it with probabilistic machine learning to tackle contextual bandits.\n\n\n\n\n\n\nFeb 9, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nvariational inference\n\n\ntensorflow-probability\n\n\n\n\nAn overview of the variational inference APIs available in TensorFlow-Probability.\n\n\n\n\n\n\nFeb 1, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nclinical quality\n\n\n\n\nWhen math helps you define too many and too few protocol deviations.\n\n\n\n\n\n\nJan 13, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nclinical quality\n\n\n\n\nAdverse event underreporting is a problem that sometimes affects clinical trials. Probabilistic programming is the modern way to address this issue.\n\n\n\n\n\n\nJan 8, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My background is in theoretical physics (quantum fields and strings) and I now work as a data scientist in the pharmaceutical industry. I focus mostly on quantitative methods in drug development, optimization of business processes, and recently probabilistic modeling to support the development of next generation nanopore sequencers.\nIn my spare time, I enjoy ski touring, mountain biking and cycling, especially in the Swiss Alps where I grew up.\nIn this blog, I am sharing learnings from my data science and applied mathematics journey."
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About",
    "section": "Get in touch",
    "text": "Get in touch\nYou can email me at yves.barmaz@gmail.com."
  },
  {
    "objectID": "about.html#selected-publications",
    "href": "about.html#selected-publications",
    "title": "About",
    "section": "Selected publications",
    "text": "Selected publications\n\nBayesian modeling for the detection of adverse events underreporting in clinical trials\nEnabling Data-Driven Clinical Quality Assurance: Predicting Adverse Event Reporting in Clinical Trials Using Machine Learning\nUsing Statistical Modeling for Enhanced and Flexible Pharmacovigilance Audit Risk Assessment and Planning\nChern-Simons Theory with Wilson Lines and Boundary in the BV-BFV Formalism"
  }
]