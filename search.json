[
  {
    "objectID": "posts/2023-07-14-Neural-likelihood-estimation.html",
    "href": "posts/2023-07-14-Neural-likelihood-estimation.html",
    "title": "Neural likelihood estimation and simulation-based inference",
    "section": "",
    "text": "The problem of estimating quantities that are not directly observable is very common in science, engineering and business applications, and evaluating the uncertainty of these estimates is often as important as the accuracy of the “best guess”. Hypothesis testing and risk management would not be possible without quantifying uncertainty, which has motivated the development of statistics as the scientific discipline that addresses these questions.\nThe usual workflow of a statistical analysis involves the definition of a joint probability distribution of measurements and (hidden) quantities of interest, followed by the application of some inference algorithm to retrieve those quantities from the observed data according to the likelihood principle, which states that all the evidence in a sample relevant to model parameters is contained in the likelihood function.\nHowever, scientific models are normally built in the opposite direction: given some model parameters, they prescribe a way to simulate observations, often with intermediate unobserved states. Statistical inference algorithms then effectively aim at solving the inverse problem: given the observations, they prescribe a way to retrieve the model parameters, and hidden states when applicable.\nSometimes the scientific models are expressed as a probability distribution, which immediately defines a likelihood function, and the statistical analysis can focus on inference. In most cases, though, the joint distribution is only implicit and the statistical analysis must start with the derivation of a likelihood function at best, or at least of an approximation.\nA common approach is to simulate observations from a model and build an empirical distribution of measurements (or summary statistics thereof) to design a statistical test of the validity of the model. This approach is at the heart of the scientific method but is more cumbersome when we want to estimate the value of model parameters."
  },
  {
    "objectID": "posts/2023-07-14-Neural-likelihood-estimation.html#simulation-based-inference",
    "href": "posts/2023-07-14-Neural-likelihood-estimation.html#simulation-based-inference",
    "title": "Neural likelihood estimation and simulation-based inference",
    "section": "Simulation-based inference",
    "text": "Simulation-based inference\nIn the last post of this blog, we discussed such a problem where we were interested in inferring the parameters of a population competition model with no known likelihood function. We relied on SMC-ABC, an example of approximate Bayesian computation, where a surrogate likelihood function is used in a Bayesian inference algorithm.\nThis type of algorithm that does not require a likelihood function is often referred to as simulation-based inference, a very active research area in computational statistics. This activity is partly fueled by advances in machine learning, where neural networks have been built to approximate all kinds of mathematical functions. In particular, normalizing flows have proven useful in approximating probability distributions, especially the masked autoregressive flow for density estimation.\nThe benefit of these normalizing flows for simulation-based inference becomes apparent if we sample model parameters \\(\\theta\\) from a certain proposal distribution \\(\\pi(\\theta)\\), and use the simulator model to generate synthetic data \\(X\\). We can then use a flow model to learn an estimate \\(\\tilde p(\\theta, X)\\) of the joint distribution, or even \\(\\tilde p(X \\vert \\theta)\\) of the conditional distribution of the observations given the parameters.\nFor an example, we can go back to the Lotka-Volterra competition model discussed in the last post.\n\n# ode_int = tfp.math.ode.BDF()\node_int = tfp.math.ode.DormandPrince()\n\n# simulator function\n@tf.function(experimental_compile=True)\ndef competition_model(x0, y0, alpha, beta, gamma, delta):\n  # Lotka - Volterra equation\n  def ode_fn(t, X):\n    \"\"\" Return the growth rate of velociraptor and rabbit populations. \"\"\"\n    dX_dt = alpha*X[0] - beta*X[0]*X[1]\n    dY_dt = -gamma*X[1] + delta*X[0]*X[1]\n    return [dX_dt, dY_dt]\n\n  return ode_int.solve(\n      ode_fn,\n      initial_time=0.,\n      initial_state=[x0, y0],\n      solution_times=t)\n\n# Generating noisy data to be used as observed data.\ndef add_noise(x0, y0, a, b, c, d):\n    noise = np.random.normal(size=(size, 2))\n    simulated = tf.stack(competition_model(\n      x0, y0, a, b, c, d).states).numpy().T\n    simulated += noise\n    indexes = np.sort(np.random.randint(low=0, high=size, size=size))    \n    return simulated[indexes]\n\nobserved = add_noise(X0[0], X0[1], a, b, c, d)\n\nWe can use the prior distribution as a proposal distribution to sample model parameters, and generate synthetic observations with the competition_model simulator.\n\node_prior = tfd.JointDistributionSequential([\n  tfd.LogNormal(tf.math.log(X0[0]), .25,),\n  tfd.LogNormal(tf.math.log(X0[1]), .25),\n  tfd.LogNormal(tf.math.log(.9), .5),\n  tfd.LogNormal(tf.math.log(.09), .5),\n  tfd.LogNormal(tf.math.log(1.), .5),\n  tfd.LogNormal(tf.math.log(.06), .5),\n])\n\nN = 200\nproposals = ode_prior.sample(N)\nsimulation = competition_model(*proposals)\n\ntheta = tf.stack(proposals, axis=1)\nX = tf.transpose(tf.stack(simulation.states, axis=0))\nX = tf.reshape(X, [-1, 200])\n\nFor density estimation, TensorFlow Probability readily implements autoregressive neural networks. Roughly, the idea is that a mapping \\(f_{nn}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) defined by a dense neural network is in general not necessarily bijective, but an autoregressive condition can be enforced by applying a mask to the weights of the network, which effectively renders the restricted \\(f_{nn}\\) bijective. This mapping can then be used to define a masked autoregressive flow that deforms a base normal distribution into a target distribution that should approximate the simulated data points. The weights of the best fitting flow can be found by minimizing the average negative log-likelihood over the training examples through gradient descent.\nMoreover, a masked autoregressive network can be defined in such a way that it accepts a conditional input, which can be provided during training, and subsequently when we sample from the resulting transformed distribution or evaluate its conditional density.\n\n# Conditional density estimation with MADE.\nmade = tfb.AutoregressiveNetwork(\n  params=2,\n  hidden_units=[8, 8],\n  event_shape=(X.shape[-1],),\n  conditional=True,\n  kernel_initializer=tfk.initializers.VarianceScaling(0.1, seed=42),\n  conditional_event_shape=(theta.shape[-1],)\n)\n\ndistribution = tfd.TransformedDistribution(\n  distribution=tfd.Sample(tfd.Normal(loc=0., scale=1.),\n                          sample_shape=[X.shape[-1]]),\n  bijector=tfb.MaskedAutoregressiveFlow(made))\n\n# Construct and fit a model.\nX_ = tfkl.Input(shape=(X.shape[-1],), dtype=tf.float32)\ntheta_ = tfkl.Input(shape=(theta.shape[-1],), dtype=tf.float32)\nlog_prob_ = distribution.log_prob(\n  X_, bijector_kwargs={'conditional_input': theta_})\nmodel = tfk.Model([X_, theta_], log_prob_)\n\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001),\n              loss=lambda _, log_prob: -log_prob)\n\nbatch_size = 50\n\nhistory = model.fit(x=[X, theta],\n                    y=np.zeros((X.shape[0], 0), dtype=np.float32),\n                    batch_size=batch_size,\n                    epochs=3000,\n                    steps_per_epoch=X.shape[0] // batch_size,\n                    shuffle=True,\n                    verbose=False)"
  },
  {
    "objectID": "posts/2023-07-14-Neural-likelihood-estimation.html#neural-likelihood-estimation",
    "href": "posts/2023-07-14-Neural-likelihood-estimation.html#neural-likelihood-estimation",
    "title": "Neural likelihood estimation and simulation-based inference",
    "section": "Neural likelihood estimation",
    "text": "Neural likelihood estimation\nThe key idea of the sequential neural likelihood estimation (SNLE) algorithm is that the estimated density function of the conditional distribution of the measurements given the parameters \\(\\tilde p(X=X_0 \\vert \\theta)\\) evaluated at the observed data \\(X_0\\) is a neural estimate of the likelihood function of the model.\n\nX_0 = tf.reshape(observed, (200,))\n\ndef neural_likelihood(*state):\n    return distribution.log_prob(X_0,\n                                 bijector_kwargs={'conditional_input': state})\n\nIt can then be combined with the prior density and used in an MCMC algorithm as the target unnormalized density to generate samples of an approximation of the posterior distribution.\n\ndef target_log_prob_fn(*state):\n    return neural_likelihood(*state) + ode_prior.log_prob(state)\n\nHere we sampled training examples with the prior distribution as the proposal distribution for the model parameters. The problem with this approach is that the neural likelihood estimator is more accurate where the sample parameters are denser, which is not necessarily the case in the region of the actual parameter corresponding to the observation \\(X_0\\). The SNLE algorithm addresses this issue by first sampling parameters from the prior and generating observations with the simulator model, then estimating the likelihood, and then sampling more parameters through MCMC with corresponding simulated observations, and repeating this sequence of neural model calibration and synthetic data generation a predefined number of times. At each cycle, it should generate training parameters closer to the true posterior and improve the estimation of the likelihood function.\n\nR = 10  # number of sampling/fitting rounds\nN = 200 # number of simulations sampled at each round\n\nsynthetic_X = X\nsynthetic_theta = theta\n\ndef sample_params(init_theta, size=100):\n    init_state = list(init_theta.numpy())\n    samples, _ = run_chain(\n        init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n        num_steps=size, burnin=50)\n    return samples\n\ndef sample_synthetic_data(param_samples):\n    simulation = competition_model(*param_samples)\n    new_X = tf.reshape(tf.transpose(tf.stack(simulation.states, axis=0)),\n                       [-1, 200])\n    new_theta = tf.stack(param_samples, axis=1)\n    return new_theta, new_X\n\nfor i in range(R):\n    print(f'Step {i}')\n    print('sampling simulations')\n    new_theta, new_X = sample_synthetic_data(\n        sample_params(synthetic_theta[-1], size=N)\n    )\n    synthetic_theta = tf.concat([synthetic_theta, new_theta], axis=0)\n    synthetic_X = tf.concat([synthetic_X, new_X], axis=0)\n\n    print('calibrating neural likelihood')\n    history = model.fit(x=[synthetic_X, synthetic_theta],\n                    y=np.zeros((synthetic_X.shape[0], 0),\n                               dtype=np.float32),\n                    batch_size=batch_size,\n                    epochs=300,\n                    steps_per_epoch=synthetic_X.shape[0] // batch_size,\n                    shuffle=True,\n                    verbose=False)\n\nFinally we can run the MCMC algorithm one last time to generate samples of the estimated posterior distribution."
  },
  {
    "objectID": "posts/2023-07-14-Neural-likelihood-estimation.html#further-resources",
    "href": "posts/2023-07-14-Neural-likelihood-estimation.html#further-resources",
    "title": "Neural likelihood estimation and simulation-based inference",
    "section": "Further resources",
    "text": "Further resources\nThe SNLE algorithm presented here is only one algorithm among a big family of simulation-based inference algorithms leveraging neural density estimation. Other members of this family try to estimate the posterior distribution directly, or the likelihood ratio that is used in the rejection step of MCMC samplers. They also vary in the way they define a proposal distribution for model parameters, and the way they schedule the simulation of synthetic observations, whether they do it in one go before the inference, or in different steps alternating with inference operations. The review paper “The frontier of simulation-based inference” gives an overview of these various designs and discusses current research areas. The website simulation-based-inference.org is a great resource for applications, articles and software. The sbi package is probably the easiest way to get started and implements the most common algorithms, along with a thorough benchmark."
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html",
    "href": "posts/2021-02-09-contextual_bandits.html",
    "title": "Thompson sampling for contextual bandits",
    "section": "",
    "text": "The multi-armed bandit problem is inspired by the situation of gamblers facing \\(N\\) slot machines with a limited amount of resources to “invest” in them, without knowing the probability distribution of rewards from each machine. By playing with a machine, they can of course sample its distribution. Once they find a machine that performs well enough, the question is wheter they should try the other ones that might perform even better, at the risk of wasting money because they might be worse. This is an example of the exploration-exploitation tradeoff dilemma. Applications include clinical trial design, portfolio selection, and A/B testing.\nThompson sampling is an approximate solution applicable to bandits for which we have a Bayesian model of the reward \\(r\\), namely a likelihood \\(P(r\\vert \\theta, a)\\) that depends on the action \\(a\\) (the choice of an arm to pull) and a vector of parameters \\(\\theta\\), and a prior distribution \\(P(\\theta)\\). In certain cases, called contextual bandits, the likelihood also depends on a set of features \\(x\\) observed by the players before they choose an action, \\(P(r\\vert \\theta, a, x)\\). After each round, the posterior distribution \\(P(\\theta \\vert \\left\\lbrace r_i, a_i, x_i\\right\\rbrace_{i})\\) is updated with the newly observed data. Then a \\(\\theta^\\ast\\) is sampled from it, the new context \\(x\\) is observed, and the new action is chosen to maximize the expected reward, \\(a^\\ast = \\mathrm{argmax}_a \\ \\mathbb{E}(r\\vert \\theta, a, x)\\).\nThis approach solves the exploration-exploitation dilemma with the random sampling of \\(\\theta^\\ast\\), which gives to every action a chance to be selected, yet favors the most promising ones. The more data is collected, the more informative the posterior distribution will become and the more it will favor its top performer.\nThis mechanism is illustrated in the chapter 6 of Probabilistic Programming & Bayesian Methods for Hackers and the section 4.4 of Bayesian Adaptive Methods for Clinical Trials.\nBoth discuss the case of a binary reward (success and failure) for every action \\(a\\) that follows a Bernoulli distribution with unknown probability of success \\(p_a\\). They assume a beta prior for each of the \\(p_a\\), which is the conjugate prior for the Bernoulli likelihood and makes inference of the posterior straightforward. This is particularly appealing when you have to update your posterior after each play.\nIf there are covariates that can explain the probability of success, one of the simplest models for a binary response of the potential actions is the combination of generalized linear models for each action,\n\\[\nP(r=1 \\vert \\theta, a, x) = \\frac{1}{1 + e^{-\\left(\\alpha_a + \\beta_a^T\\,x\\right)}}\n\\]\nUnfortunately, there is no immediate congugate prior for this type of likelihood, so we have to rely on numerical methods to estimate the posterior distribution. A previous blog post discussed variational inference as a speedier alternative to MCMC algorithms, and we will see here how we can apply it to the problem of contextual bandits with binary response.\nThis problem is relevant in the development of personalized therapies, where the actions represent the different treatments under investigation and the contexts \\(x\\) are predictive biomarkers of their response. The goal of a trial would be to estimate the response to each treatment option given biomarkers \\(x\\), and, based on that, to define the best treatment policy. Adaptive randomization through Thompson sampling ensures that more subjects enrolled in the trial get the optimal treatment based on their biomarkers and the knowledge accrued until their randomization, which is certainly more ethical than a randomization independent on the biomarkers.\nAnother example is online ad serving, where the binary response corresponds to a successful conversion, the action is the selection of an ad for a specific user, and the context is a set of features related to that user. When a new ad enters the portfolio and a new click-through rate model needs to be deployed for it, Thompson sampling can accelerate the training phase and reduce the related costs."
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#bandit-model",
    "href": "posts/2021-02-09-contextual_bandits.html#bandit-model",
    "title": "Thompson sampling for contextual bandits",
    "section": "Bandit model",
    "text": "Bandit model\nFor simplicity, we simulate bandits whose true probabilities of success follow logistic models, so we can see how the posterior distributions concentrate around the true values during training. You can run this notebook in Colab to experiment with more realistic models, and vary the number of arms or the context dimension.\n\n\nCode\nclass ContextualBandit(object):\n    \"\"\"\n    This class represents contextual bandit machines with n_arms arms and\n    linear logits of p-dimensional contexts.\n\n    parameters:\n        arm_true_weights: (n_arms, p) Numpy array of real weights.\n        arm_true_biases:  (n_arms,) Numpy array of real biases\n\n    methods:\n        pull( arms, X ): returns the results, 0 or 1, of pulling \n                   the arms[i]-th bandit given an input context X[i].\n                   arms is an (n,) array of arms indices selected by the player\n                   and X an (n, p) array of contexts observed by the player\n                   before making a choice.\n        \n        get_logits(X): returns the logits of all bandit arms for every context in\n                   the (n, p) array X\n                   \n        get_probs(X): returns sigmoid(get_logits(X))\n        \n        get_selected_logits(arms, X): returns from get_logits(X) only the logits\n                   corresponding to the selected arms\n        \n        get_selected_probs(arms, X): returns sigmoid(get_selected_logits(arms, X))\n        \n        get_optimal_arm(X): returns the arm with the highest probability of success\n                   for every context in X\n\n    \"\"\"\n    def __init__(self, arm_true_weights, arm_true_biases):\n        self._arm_true_weights = tf.convert_to_tensor(\n              arm_true_weights,\n              dtype=tf.float32,\n              name='arm_true_weights')\n        self._arm_true_biases = tf.convert_to_tensor(\n              arm_true_biases,\n              dtype=tf.float32,\n              name='arm_true_biases')\n        self._shape = np.array(\n              self._arm_true_weights.shape.as_list(),\n              dtype=np.int32)\n        self._dtype = tf.convert_to_tensor(\n              arm_true_weights,\n              dtype=tf.float32).dtype.base_dtype\n\n    @property\n    def dtype(self):\n        return self._dtype\n    \n    @property\n    def shape(self):\n        return self._shape\n    \n    def get_logits(self, X):\n        return tf.matmul(X, self._arm_true_weights, transpose_b=True) + \\\n               self._arm_true_biases\n    \n    def get_probs(self, X):\n        return tf.math.sigmoid(self.get_logits(X))\n\n    def get_selected_logits(self, arms, X):\n        all_logits = self.get_logits(X)\n        column_indices = tf.convert_to_tensor(arms, dtype=tf.int64)\n        row_indices = tf.range(X.shape[0], dtype=tf.int64)\n        full_indices = tf.stack([row_indices, column_indices], axis=1)\n        selected_logits = tf.gather_nd(all_logits, full_indices)\n        return selected_logits\n    \n    def get_selected_probs(self, arms, X):\n        return tf.math.sigmoid(self.get_selected_logits(arms, X))\n    \n    def pull(self, arms, X):\n        selected_logits = self.get_selected_logits(arms, X)\n        return tfd.Bernoulli(logits=selected_logits).sample()\n    \n    def pull_all_arms(self, X):\n        logits = self.get_logits(X)\n        return tfd.Bernoulli(logits=logits).sample()\n    \n    def get_optimal_arm(self, X):\n        return tf.argmax(\n            self.get_logits(X),\n            axis=-1)\n\n\nHere we work with a two-dimensional context drawn from two independent standard normal distributions, and we select true weights and biases that correspond to an overall probability of success of about 30% for each arm, a situation that might be encountered in a personalized medicine question.\n\ntrue_weights = np.array([[2., 0.],[0., 3.]])\ntrue_biases = np.array([-1., -2.])\n\nN_ARMS = true_weights.shape[0]\nCONTEXT_DIM = true_weights.shape[1]\n\nbandit = ContextualBandit(true_weights, true_biases)\npopulation = tfd.Normal(loc=tf.zeros(CONTEXT_DIM, dtype=tf.float32),\n                        scale=tf.ones(CONTEXT_DIM, dtype=tf.float32))"
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#thompson-sampling",
    "href": "posts/2021-02-09-contextual_bandits.html#thompson-sampling",
    "title": "Thompson sampling for contextual bandits",
    "section": "Thompson sampling",
    "text": "Thompson sampling\nA Thompson sampler based on a logistic regression can be implemented as a generalization of the probabilistic machine learning model discussed in the previous post. It is essentially a single dense variational layer with one unit per arm of the contextual bandit we want to solve. These units are fed into a Bernoulli distribution layer that simulates the pull of each arm.\nThe parameters \\(\\theta\\) of the model are encoded in the posterior_mean_field used as a variational family for the dense variational layer, and when we fit the full model to data, it converges to an approximation of the true posterior \\(P(\\theta \\vert \\left\\lbrace r_i, a_i, x_i\\right\\rbrace_{i})\\).\nA subsequent call of that dense variational layer on a new input \\(x\\) will return random logits drawn from the approximate posterior predictive distribution and can thus be used to implement Thompson sampling (see the randomize method in the code). The \\(a^\\ast = \\mathrm{argmax}_a \\ \\mathbb{E}(r\\vert \\theta, a, x)\\) step is the selection of the unit with the highest logit.\nFor training, the loss function is the negative log-likelihood of the observed outcome \\(r_i\\), but only for the unit corresponding to the selected action \\(a_i\\), so it is convenient to combine them into a unique output \\(y_i=(a_i,r_i)\\) and write a custom loss function.\n\n\nCode\nclass ThompsonLogistic(tf.keras.Model):\n    \"\"\"\n    This class represents a Thompson sampler for a Bayesian logistic regression\n    model.\n    \n    It is essentially a keras Model of a single layer Bayesian neural network\n    with Bernoulli output enriched with a Thompson randomization method that\n    calls only the dense variational layer.\n    \n    Parameters:\n        - context_dim: dimension of the context\n        - n_arms: number of arms of the multi-arm bandit under investigation\n        - sample_size: size of the current training set of outcome observations,\n                       used to scale the kl_weight of the dense variational layer\n    \n    Methods:\n        - randomize(inputs): returns a logit for each arm drawn from the (approximate)\n            posterior predictive distribution\n        - get_weights_stats(): returns means and sttdevs of the surrogate posterior\n            of the model parameters\n        - predict_probs(X, sample_size): returns the posterior probability of success\n            for each context in the array X and each arm of the bandit, sample_size specifies\n            the sample size of the Monte Carlo estimate\n        - assign_best_mc(X, sample_size): returns the arms with the highest\n                                          predict_probs(X, sample_size)\n        - assign_best(X): returns the arms with the highest expected logit, should\n            be very similar to assign_best_mc, a little bit less accurate\n    \"\"\"\n    def __init__(self, context_dim, n_arms, sample_size):\n        super().__init__()\n        self.context_dim = context_dim\n        self.n_arms = n_arms\n        self.densevar = tfp.layers.DenseVariational(n_arms, posterior_mean_field, prior_ridge, kl_weight=1/sample_size)\n        self.bernoullihead = tfp.layers.DistributionLambda(lambda t: tfd.Bernoulli(logits=t))\n    \n    def call(self, inputs):\n        x = self.densevar(inputs)\n        return self.bernoullihead(x)\n    \n    def randomize(self, inputs):\n        return self.densevar(inputs)\n    \n    def get_weights_stats(self):\n        n_params = self.n_arms * (self.context_dim + 1)\n        c = np.log(np.expm1(1.))\n        \n        weights = self.densevar.weights[0]\n\n        means = weights[:n_params].numpy().reshape(self.context_dim + 1, self.n_arms)\n        stddevs = (1e-5 + tf.nn.softplus(c + weights[n_params:])).numpy().reshape(self.context_dim + 1, self.n_arms)\n\n        mean_weights = means[:-1]\n        mean_biases = means[-1]\n\n        std_weights = stddevs[:-1]\n        std_biases = stddevs[-1]\n        return mean_weights, mean_biases, std_weights, std_biases\n    \n    def assign_best(self, X):\n        mean_weights, mean_biases, std_weights, std_biases = self.get_weights_stats()\n        logits = tf.matmul(X, mean_weights) + mean_biases\n        return tf.argmax(logits, axis=1)\n    \n    def predict_probs(self, X, sample_size=100):\n        mean_weights, mean_biases, std_weights, std_biases = self.get_weights_stats()\n        \n        weights = tfd.Normal(loc=mean_weights, scale=std_weights).sample(sample_size)\n        biases = tfd.Normal(loc=mean_biases, scale=std_biases).sample(sample_size)\n        \n        probs = tf.math.sigmoid(tf.matmul(X, weights)+biases[:,tf.newaxis,:])\n        return tf.reduce_mean(probs, axis=0)\n              \n    def assign_best_mc(self, X, sample_size=100):\n        probs = self.predict_probs(X, sample_size)\n        return tf.argmax(probs, axis=1)\n    \n\n# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\ndef posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n    n = kernel_size + bias_size\n    c = np.log(np.expm1(1.))\n    return tf.keras.Sequential([\n        tfp.layers.VariableLayer(2 * n,\n                                 initializer=tfp.layers.BlockwiseInitializer([\n                                     'zeros',\n                                     tf.keras.initializers.Constant(np.log(np.expm1(.7))),\n                                 ], sizes=[n, n]),\n                                 dtype=dtype),\n        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n            tfd.Normal(loc=t[..., :n],\n                       scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n            reinterpreted_batch_ndims=1)),\n    ])\n\n\n# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\ndef prior_ridge(kernel_size, bias_size, dtype=None):\n    return lambda _: tfd.Independent(\n        tfd.Normal(loc=tf.zeros(kernel_size + bias_size),\n                   scale=tf.concat([2*tf.ones(kernel_size),\n                                    4*tf.ones(bias_size)],\n                                   axis=0)),\n        reinterpreted_batch_ndims=1\n    )\n\n    \ndef build_model(context_dim, n_arms, sample_size, learning_rate=0.01):\n    model = ThompsonLogistic(context_dim, n_arms, sample_size)\n    \n    # the loss function is the negloglik of the outcome y[:,1] and the head corresponding\n    # to the arm assignment y[:,0] is selected with a one-hot mask\n    loss_fn = lambda y, rv_y: tf.reduce_sum(-rv_y.log_prob(y[:,1, tf.newaxis]) * tf.one_hot(y[:,0], n_arms), axis=-1)\n    \n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss=loss_fn)\n    model.build(input_shape=(None, context_dim))\n    return model"
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#learning-strategy",
    "href": "posts/2021-02-09-contextual_bandits.html#learning-strategy",
    "title": "Thompson sampling for contextual bandits",
    "section": "Learning strategy",
    "text": "Learning strategy\nIn the learning phase of the model, at each step a new context \\(x_i\\) is observed (or drawn from the population), an action \\(a_i\\) is chosen, a reward \\(r_i\\) is observed (or simulated with bandit.pull), and the model is updated.\n\n\nCode\nclass BayesianStrategy(object):\n    \"\"\"\n    Implements an online, learning strategy to solve\n    the contextual multi-armed bandit problem.\n    \n    parameters:\n      bandit: an instance of the ContextualBandit class\n    \n    methods:\n      thompson_randomize(X): draws logits from the posterior distribution and\n                             returns the arms with the highest values\n      \n      _update_step(X, y): updates the model with the new observations\n      \n      one_trial(n, population): samples n elements from population, selects\n                             an arm for each of them through Thompson sampling,\n                             pulls it, updates the model\n      \n      train_on_data(X_train, all_outcomes_train): implements Thompson sampling\n                             on pre-sampled data where an omnicient being has\n                             pulled all the arms. The reason is to compare with\n                             standard Bayesian inference on the same data\n                            \n      evaluate_training_decisions: returns statistics about action selection\n                             during training\n    \"\"\"\n    \n    def __init__(self, bandit):\n        self.bandit = bandit\n        self.context_dim = bandit.shape[1]\n        self.n_arms = bandit.shape[0]\n        dtype = tf.float32\n        self.X = tf.cast(tf.reshape((), (0, self.context_dim)), tf.float32)\n        self.y = tf.cast(tf.reshape((), (0, 2)), tf.int32)\n        self.model = build_model(self.context_dim, self.n_arms, 1, learning_rate=0.008)\n        self.loss = []\n        self.weights = []\n      \n\n    def thompson_randomize(self, X):\n        return tf.argmax(self.model.randomize(X), axis=1)\n    \n    \n    def _update_step(self, X, y, epochs=10):\n        self.X = tf.concat([self.X, X], axis=0)\n        self.y = tf.concat([self.y, y], axis=0)\n        weights = self.model.get_weights()\n        self.model = build_model(self.context_dim, self.n_arms, self.X.shape[0], learning_rate=0.008)\n        self.model.set_weights(weights)\n        hist = self.model.fit(self.X, self.y, verbose=False, epochs=epochs)\n        self.loss.append(hist.history['loss'])\n        self.weights.append(self.model.get_weights_stats())\n\n        \n    def one_trial(self, n, population, epochs=10):\n        X = population.sample(n)\n        selected_arms = self.thompson_randomize(X)\n        outcomes = self.bandit.pull(selected_arms, X)\n        y = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes[:,tf.newaxis]], axis=1)\n        self._update_step(X, y, epochs)\n    \n    \n    def train_on_data_step(self, X, all_outcomes, epochs):\n        selected_arms = self.thompson_randomize(X)\n        column_indices = tf.convert_to_tensor(selected_arms, dtype=tf.int64)\n        row_indices = tf.range(X.shape[0], dtype=tf.int64)\n        full_indices = tf.stack([row_indices, column_indices], axis=1)\n        outcomes = tf.gather_nd(all_outcomes, full_indices)\n        y = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes[:,tf.newaxis]], axis=1)\n        self._update_step(X, y, epochs)\n    \n    \n    def train_on_data(self, X_train, all_outcomes_train, batch_size=1, epochs=10):\n        n_train = X_train.shape[0]\n        ds = tf.data.Dataset.from_tensor_slices((X_train, all_outcomes_train)).batch(batch_size)\n        for (X, all_outcomes) in ds:\n            self.train_on_data_step(X, all_outcomes, epochs)\n    \n    \n    def train_on_data_standard(self, X_train, all_outcomes_train, epochs=1000):\n        n_train = X_train.shape[0]\n        n_zeros = n_train//2\n        n_ones = n_train - n_zeros\n        selected_arms = tf.cast(tf.math.floormod(tf.range(n_train), 2), tf.int64)\n        column_indices = tf.convert_to_tensor(selected_arms, dtype=tf.int64)\n        row_indices = tf.range(n_train, dtype=tf.int64)\n        full_indices = tf.stack([row_indices, column_indices], axis=1)\n        outcomes_train = tf.gather_nd(all_outcomes_train, full_indices)\n        y_train = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes_train[:,tf.newaxis]], axis=1)\n        self._update_step(X_train, y_train, epochs)\n    \n    \n    def evaluate_training_decisions(self):\n        best_arm_proportion = tf.reduce_mean(tf.cast(\n            tf.cast(self.y[:,0], tf.int64)==self.bandit.get_optimal_arm(self.X), tf.float32)).numpy()\n        success_rate = self.y[:,1].numpy().sum()/self.y.shape[0]\n        prob_of_success = tf.reduce_mean(self.bandit.get_selected_probs(tf.cast(self.y[:,0], tf.int64), self.X), axis=0).numpy()\n        return {'training_best_arm_proportion': best_arm_proportion,\n                'training_success_rate': success_rate,\n                'training_prob_of_success': prob_of_success\n               }\n\n\nAfter 60 to 80 iterations, the surrogate posteriors seem to have converged to distributions that are compatible with the true values of the parameters.\n\n\n\n\n\nFor comparison, we can train models on the same sample that has been assigned purely randomly to each arm.\n\n\n\n\n\nThe surrogate posterior distributions look similar to the ones obtained from Thompson sampling, and the predictive performance on a test set are comparable. In the following table, “best_arm_selection_rate” describes how frequently the best action is selected for contexts in the test set according to the predictions of the two models, and “model_prob_of_success” is the average of the true probabilities of success for the actions selected by the model. For reference, “arms_probs_of_success” shows the average of the true probabilities of success for each action in the case it is always picked. The benefit of Thompson sampling is revealed in the predictive performance during training. In the same table, “training_best_arm_proportion” indicates how often the best action is selected during training (as expected, roughly half the time for standard randomization), “training_success_rate” the observed success rate during training and “training_prob_of_success” the average probability of success following the assignment decisions made during training.\n\n\n\n\n\n\n  \n    \n      \n      Thompson randomization\n      Standard randomization\n    \n  \n  \n    \n      training_best_arm_proportion\n      0.775\n      0.5875\n    \n    \n      training_success_rate\n      0.4375\n      0.375\n    \n    \n      training_prob_of_success\n      0.448006\n      0.357196\n    \n    \n      best_arm_selection_rate\n      0.9228\n      0.9126\n    \n    \n      model_prob_of_success\n      0.48482\n      0.484251\n    \n    \n      arms_probs_of_success\n      [0.35353488, 0.27571228]\n      [0.35353488, 0.27571228]\n    \n  \n\n\n\n\nIn terms of reward, it is clear that training a model with Thompson randomization costs less than with standard randomization, and the inferred arm selection policy after training is very similar. In a clinical trial, that would translate into more enrolled subjects getting the best therapy according to their biomarkers."
  },
  {
    "objectID": "posts/2021-02-09-contextual_bandits.html#tougher-bandits",
    "href": "posts/2021-02-09-contextual_bandits.html#tougher-bandits",
    "title": "Thompson sampling for contextual bandits",
    "section": "Tougher bandits",
    "text": "Tougher bandits\nThe simple model presented in this note can be expanded in several directions. We can obviously consider more arms and contexts of higher dimensions. In that case, incorporating expert knowledge in the form of more informative priors or more complex surrogate posteriors can be useful. We can also include past observations to achieve faster convergence, and tamper them with lower weights if they are less relevant than the data sampled from the bandits during training. This type of jump start is particularly relevant in fields like online advertising where lower overall probabilities require more observations.\nMore complex mechanisms could be modeled with deeper Bayesian neural networks. The important requirement is a layer that can implement Thompson sampling. Moreover, the DistributionLambda top layer is not limited to Bernoulli distributions, and a wide variety of reward distributions can be easily simulated. It is probably reasonable to start with a single DenseVariational layer with adequate priors and variational surrogate posteriors with a top DistributionLambda layer compatible with the rewards, and then try to add layers to improve performance. As in most machine learning problems, the key is experimentation."
  },
  {
    "objectID": "posts/2023-06-24-Rabbits-and-velociraptors.html",
    "href": "posts/2023-06-24-Rabbits-and-velociraptors.html",
    "title": "Rabbits and velociraptors: scientific modeling on steroids",
    "section": "",
    "text": "Machine learning algorithms are often criticized for their poor interpretability and a lack of uncertainty quantification. They are widely used nonetheless thank to modular libraries that allow us to build flexible models and training pipelines adapted to all kinds of problems. Automatic differentiation and specialized compilers are technologies at the core of those capabilities, and interestingly they have also been enabling rapid advances in the field of scientific computing, which naturally strives for interpretability and uncertainty awareness. Hamiltonian Monte Carlo and variational inference algorithms implemented in libraries such as PyMC, Pyro or TensorFlow-Probability are examples that immediately come to mind. When it comes to simulation, differentiable programming is gaining traction as it simplifies the process of calibrating computer models to experimental data. The idea is that the same principle as backpropagation in machine learning can be used to adjust the model parameters with gradient-based optimization algorithms until the simulated data closely matches the experimental data. The only requirement is that the model has to be written in a language that supports automatic differentiation.\nFor a long time, scientific computing relied on libraries written in Fortran, C/C++ or Python, or even MATLAB, by generations upon generations of graduate students and postdocs, and derivatives had to be calculated by hand (I sound like my grand-parents when they were telling me they had to walk several kilometers every day to go to school). Here comes JAX in the picture, a high-performance array computing library built around automatic differentiation and the XLA compiler, with a NumPy-like API that makes it easy to express scientific models in a code that will be differentiable and support hardware acceleration out of the box. Moreover, a whole ecosystem of specialized libraries have been built around JAX that can handle common scientific computing tasks, at a more or less high level, for instance:\n\nOptax for gradient-based optimization\nDiffrax to integrate differential equations\nTensorFlow Probability on JAX and NumPyro for probabilistic programming and statistical analysis\nBlackJAX for efficient sampling\nJAX, M.D. for molecular dynamics\nJAX-CFD for computational fluid dynamics\nJAX-cosmo for cosmology\n\nThe common JAX backend of these libraries allows for a great deal of modularity in the composition of scientific analysis pipelines. As an example, we will see how to quickly specify a predator-prey model in jax.numpy, integrate it with Diffrax, calibrate the resulting simulation to experimental data with Optax, and finally quantify uncertainties with a statistical analysis run in TensorFlow Probability on JAX. People who know me must now be thinking that I am a big fan of the JAX ecosystem because I come from a village called Nax, and they would not be completely wrong…"
  },
  {
    "objectID": "posts/2023-06-24-Rabbits-and-velociraptors.html#lotka-volterra-model",
    "href": "posts/2023-06-24-Rabbits-and-velociraptors.html#lotka-volterra-model",
    "title": "Rabbits and velociraptors: scientific modeling on steroids",
    "section": "Lotka-Volterra model",
    "text": "Lotka-Volterra model\nThe Lotka-Volterra differential equations describe the time-evolution of the sizes of two populations of preys, denoted by \\(x(t)\\), and predators, denoted by \\(y(t)\\).\n\\[\n\\begin{aligned}\n\\frac{dx}{dt} &= \\alpha \\, x - \\beta\\, x \\, y \\\\\n\\frac{dy}{dt} &= \\delta \\, x \\, y - \\gamma \\, y\n\\end{aligned}\n\\]\nAt first glance, these equations look relatively simple, but the quadratic terms make them painful to solve by hand. They are easily numerically integrated with the Diffrax library, which implements state-of-the-art integrators for differential equations.\nThe treatment of the problem presented here is inspired by the PyMC documentation and the corresponding tfp demo.\n\n\nCode\n# definition of parameters\na = 1.\nb = 0.1\nc = 1.5\nd = 0.075\n\n# initial population of rabbits and velociraptors\nX0 = jnp.array([10., 5.])\n\n# size of data\nsize = 100\n\n# time lapse\ntime = 15\nt = jnp.linspace(0, time, size)\nsaveat = SaveAt(ts=t)\n\n\nBut here we use a powerful integrator from Diffrax, further sped up with the magic of Just in Time compilation.\n\n@jit\ndef competition_model(x0, y0, alpha, beta, gamma, delta):\n    # solves a Lotka-Volterra system of ODEs\n\n    # definition of the system of ODEs\n    def f(t, X, args):\n        return jnp.array([alpha * X[0] - beta * X[0] * X[1],\n                          delta * X[0] * X[1] - gamma * X[1]])\n    \n    stepsize_controller = PIDController(rtol=1e-5, atol=1e-5)\n    \n    soln = diffeqsolve(ODETerm(f), Dopri5(), t0=0, t1=time, dt0=0.1,\n                       y0=jnp.array([x0, y0]), saveat=saveat,\n                       stepsize_controller=stepsize_controller)\n    \n    return soln\n\nWe can generate synthetic observations with this solver to which we add a bit of noise to make the inference problems more realistic."
  },
  {
    "objectID": "posts/2023-06-24-Rabbits-and-velociraptors.html#model-calibration",
    "href": "posts/2023-06-24-Rabbits-and-velociraptors.html#model-calibration",
    "title": "Rabbits and velociraptors: scientific modeling on steroids",
    "section": "Model calibration",
    "text": "Model calibration\nSince the Diffrax library is written in JAX, we can chain the numerical solution with a loss function that penalizes discrepancies between the simulations and the observations, and apply automatic differentiation on the output with JAX operators such as jax.grad.\nTo model the discrepancies, we assume that the observations are normally distributed around the simulated values, and construct a corresponding pseudolikelihood function.\n\nepsilon = .5\nkernel = tfd.Normal(0., epsilon)\n\n@jit\ndef pseudolikelihood(x0, y0, alpha, beta, gamma, delta):\n    sim_data = competition_model(x0, y0, alpha, beta, gamma, delta)\n    error = observed - jnp.stack(sim_data.ys)\n    return jnp.mean(kernel.log_prob(error))\n\nTo find the best fit parameters, we can then maximize this pseudolikelihood with an optimizer from the Optax library.\n\n\nCode\nparams = jnp.array([5., 5., 1., 1., 1., 1.])\n\nstart_learning_rate = 1e-1\noptimizer = optax.adam(start_learning_rate)\n\nopt_state = optimizer.init(params)\n\n@jit\ndef compute_loss(params):\n    return -pseudolikelihood(*params)\n\n# A simple update loop.\nfor _ in range(1000):\n    grads = grad(compute_loss)(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n\n# computing the best fit solution\nfitsoln = competition_model(*params)\n\n\n\n\n\n\n\n\n  \n    \n      \n      true value\n      estimated value\n    \n    \n      parameter\n      \n      \n    \n  \n  \n    \n      x_0\n      10.0\n      10.176\n    \n    \n      y_0\n      5.0\n      3.917\n    \n    \n      alpha\n      1.000\n      0.821\n    \n    \n      beta\n      0.100\n      0.091\n    \n    \n      gamma\n      1.500\n      1.854\n    \n    \n      delta\n      0.075\n      0.095"
  },
  {
    "objectID": "posts/2023-06-24-Rabbits-and-velociraptors.html#uncertainty",
    "href": "posts/2023-06-24-Rabbits-and-velociraptors.html#uncertainty",
    "title": "Rabbits and velociraptors: scientific modeling on steroids",
    "section": "Uncertainty",
    "text": "Uncertainty\nThe estimated values do not quite match the true parameters, and it would certainly be interesting to get error bars. A Bayesian analysis is a convenient way to achieve this, so the first step is to specify a prior distribution on the parameters.\n\node_prior = tfd.JointDistributionSequential([\n  tfd.Normal(X0[0], .25),\n  tfd.Normal(X0[1], .25),\n  tfd.Normal(1., .5),\n  tfd.Normal(.5, .1),\n  tfd.Normal(2., .5),\n  tfd.Normal(1., .1),\n])\n\nSamples from the corresponding prior predictive distribution of the observed data provide a good sanity check of the choice of priors.\n\n\n\n\n\nThe problem now is that we cannot build a joint distribution of the parameters and the observations reflecting our model assumptions that there is an underlying ordinary differential equation governing the dynamic of the system. As a result, we cannot derive a likelihood function that is required by the usual MCMC and variational inference algorithms for Bayesian statistics.\nAs a workaround, there exist some likelihood-free inference algorithms for models where we know how to simulate data, but not how to construct a likelihood function. This is the case for a significant portion of scientific models, and the research field of simulation-based inference is growing to address this need. Approximate Bayesian computation is a family of such algorithms (see this review article for more details). It includes a variant of sequential Monte Carlo (SMC) that can be implemented in PyMC and TensorFlow Probability, which we will be using here.\nThis algorithm requires a batched version of the pseudolikelihood function, so that it can be applied to a large sample of model parameters, the so-called particles, in parallel. For every particle, the batched pseudolikelihood function calls the differential equation solver, so a simplified version of the solver with no timestep adaptation can help avoid maximum timesteps errors.\n\n@jit\ndef batched_competition_model(x0, y0, alpha, beta, gamma, delta):\n    # solves a Lotka-Volterra system of ODEs\n\n    def f(t, X, args):\n        return jnp.array([alpha * X[0] - beta * X[0] * X[1],\n                          delta * X[0] * X[1] - gamma * X[1]])\n    soln = diffeqsolve(ODETerm(f), Dopri5(), t0=0, t1=time, dt0=0.1,\n                       y0=jnp.array([x0, y0]), saveat=saveat)\n\n    return soln\n\n@jit\ndef batched_pseudolikelihood(x0, y0, alpha, beta, gamma, delta):\n    sim_data = batched_competition_model(x0, y0, alpha, beta, gamma, delta)\n    error = observed[...,jnp.newaxis] - jnp.stack(sim_data.ys)\n    return jnp.mean(kernel.log_prob(error), axis=[0, 1])\n\nA typical SMC sampler requires a prior density function and a likelihood function to build a sequence of (unnormalized) density functions between the prior and the target posterior we are interested in (for a thorough introduction to SMC algorithms, see chapter 13 of Probabilistic Machine Learning: Advanced Topics). It then applies a particle filter algorithm to get samples of these density functions until the final one provides the desired output. In SMC-ABC, the likelihood function is replaced by a pseudolikelihood function.\n\n\nCode\nsample_smc_chain = tfp.experimental.mcmc.sample_sequential_monte_carlo\n\n@jit\ndef run_abc():\n    n_stage, final_state, final_kernel_results = sample_smc_chain(\n      prior_log_prob_fn=ode_prior.log_prob,\n      likelihood_log_prob_fn=batched_pseudolikelihood,\n      current_state=ode_prior.sample(1000, seed=sample_key),\n      min_num_steps=5,\n      seed=sample_key,\n      )\n    return n_stage, final_state, final_kernel_results\n\nn_stage, final_state, final_kernel_results = run_abc()\n\n\nThe final state of the particle filter is a sample from the (approximate) posterior distribution.\n\n\n\n\n\nThe posterior standard deviations provide the desired uncertainty quantification for the parameter estimates.\n\n\n\n\n\n\n  \n    \n      \n      true value\n      posterior mean\n      posterior std\n    \n    \n      parameter\n      \n      \n      \n    \n  \n  \n    \n      x_0\n      10.000\n      9.975\n      0.269\n    \n    \n      y_0\n      5.000\n      4.949\n      0.277\n    \n    \n      alpha\n      1.000\n      0.864\n      0.055\n    \n    \n      beta\n      0.100\n      0.091\n      0.007\n    \n    \n      gamma\n      1.500\n      1.761\n      0.142\n    \n    \n      delta\n      0.075\n      0.090\n      0.007\n    \n  \n\n\n\n\nLastly, we can simulate trajectories for parameters from the posterior sample to visualize the posterior predictive distribution of populations and compare it with the observations."
  },
  {
    "objectID": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html",
    "href": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html",
    "title": "Masked autoregressive flows for stochastic differential equations",
    "section": "",
    "text": "A stochastic process is a sequence of random variables \\(X_t\\) indexed by a time parameter \\(t\\) that can be discrete or continuous. The dynamics of a continuous process is often described by a stochastic differential equation (SDE) of the form\n\\[\ndX_t = \\mu(X_t, t)dt + \\sigma(X_t, t)dW_t,\n\\]\nwhere \\(\\mu(X, t)\\) is called the drift and \\(\\sigma(X, t)\\) the volatility.\nIn statistical physics, the random variable \\(X_t\\) can describe the positions at time \\(t\\) of a population of molecules swimming around in water, bumping randomly into smaller molecules following a Wiener process \\(W_t\\). In finance, \\(X_t\\) can describe the uncertainty over an asset price in the future due to the random behavior of market participants.\nSolving this SDE means finding the probability distribution of \\(X_t\\) for \\(t>0\\), given the initial distribution of \\(X_{t=0}\\). This can be done by specifying the probability density function (PDF) \\(p(x; t)\\) for \\(t>0\\) given the initial PDF \\(p(x; t=0)\\) (here the semicolon indicates that \\(x\\) is the value realized by the random variable, and \\(t\\) the time parameter that indexes the distributions).\nFor certain special cases of \\(\\mu(X, t)\\) and \\(\\sigma(X, t)\\), the SDE can be solved explicitly (see these lecture notes for some examples). In general, one has to rely on numerical methods."
  },
  {
    "objectID": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#euler-maruyama",
    "href": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#euler-maruyama",
    "title": "Masked autoregressive flows for stochastic differential equations",
    "section": "Euler-Maruyama",
    "text": "Euler-Maruyama\nThe Euler–Maruyama method is probably the most flexible one. It generates approximated samples of the process \\(X_t\\) at discretized time steps \\(t_1, \\dots, t_N\\),\n\\[\nX_{i+1} = X_i + \\mu(X_i, t_i)\\Delta t_i + \\sigma(X_i, t_i)\\,\\sqrt{\\Delta t_i}\\, Z_i,\n\\]\nwhere every \\(Z_i\\) is drawn from a standard normal distribution.\nIn TensorFlow Probability, this method can be implemented as a tfd.MarkovChain distribution. The following code snippet illustrates it with a geometric Brownian motion.\n\ndrift = .1\nvolatility = .2\n\nn_steps = 501\ntime = tf.linspace(0., 10, n_steps)\ntime_step = np.mean(np.diff(time))\n\ngeom_brownian_motion = tfd.MarkovChain(\n            initial_state_prior=tfd.Deterministic(1.),\n            transition_fn=lambda _, x: tfd.Normal(\n                loc=x + x * drift * time_step,\n                scale=x * volatility * tf.sqrt(time_step)),\n            num_steps=n_steps,\n            name='geometric_brownian_motion')\n\n\n\n\n\n\nSometimes, one is interested only in expected values of the form\n\\[\n\\mathbb{E}\\left[f(X_t)\\right] = \\int f(x)\\,p(x; t)\\,\\mathrm{d}x,\n\\]\nfor instance in asset pricing problems. Samples generated by the Euler-Maruyama methods can be used in Monte Carlo estimates\n\\[\n\\mathbb{E}\\left[f(X_t)\\right] \\approx \\sum_k f(\\hat X_{t, k}),\n\\]\nwhere the sum is taken over several realizations of the stochastic numerical integration.\nOther times, one needs to know the probability density function at a given time \\(t\\), for instance in inference problems with models that have a stochastic process component. Markov chains could provide that in theory, but at the expense of marginalizing out all the intermediate steps."
  },
  {
    "objectID": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#fokker-planck",
    "href": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#fokker-planck",
    "title": "Masked autoregressive flows for stochastic differential equations",
    "section": "Fokker-Planck",
    "text": "Fokker-Planck\nPhysicists came up with an alternative solution by deriving a partial differential equation (PDE) for \\(p(x;t)\\) that is equivalent to the SDE, the Fokker-Planck equation\n\\[\n\\frac{\\partial p(x; t)}{\\partial t} = -\\frac{\\partial}{\\partial x}[\\mu(x, t)p(x; t)] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[\\sigma^2(x, t)p(x; t)].\n\\]\nIt describes how the initial distribution \\(p(x;0)\\) is shifting under the influence of the drift term and diffusing because of the volatility term. For the non-physicists who want to try this at home, \\(p(x;0)\\) could indicate the position of a tea bag in a cup of hot water, \\(\\mu(x, t)\\) could describe how the water has been stirred, and \\(\\sigma(x, t)\\) would depend on the thermal agitation of the water.\nThese PDEs are usually solved numerically. This can be difficult when \\(\\mu(X, t)\\) renders the equation stiff (this happens in models of noisy microcircuits) or in high-dimensional problems (for instance models of the joint distribution of stock prices in large markets).\nA limitation of this modelling approach is the difficulty to sample from a distribution given by an arbitrary probability density function. This requires specialized algorithms, so in practice it is probably better to rely on Euler-Maruyama methods for sampling, and Fokker-Planck equations for density evaluation. Furthermore, the log-densities should be compatible with automatic differentiation to be useful in probabilistic inference algorithms, which is not always straightforward with PDE solvers."
  },
  {
    "objectID": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#probabilistic-machine-learning",
    "href": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#probabilistic-machine-learning",
    "title": "Masked autoregressive flows for stochastic differential equations",
    "section": "Probabilistic machine learning",
    "text": "Probabilistic machine learning\nProbabilistic machine learning provides a third approach, where one gets a model that can both generate new samples and evaluate densities in an a framework supporting automatic differentiation. The idea is to first generate training data with the Euler-Maruyama method, and then use machine learning to fit a probability distribution parameterized by a neural network to this synthetic data. The training objective is the conditional density estimation of the synthetic training data. This distribution should both be convenient to sample from and have a density that can be evaluated easily.\nSuch distributions can be constructed with autoregressive normalizing flows parameterized by conditional masked autoregressive networks. In a nutshell, a normalizing flow deforms a normal distribution into a more generic one through an invertible mapping. This mapping can be implemented as a neural network, provided the network is invertible, and the autoregressive condition imposed by masking certain weights ensures just that. Moreover, these autoregressive networks can accept conditional variables, so that they can estimate the parametric density \\(p(x; t)\\) if \\(t\\) is passed to the model as a conditional variable.\n\n# Generate synthetic training data\nn = 200\ns = geom_brownian_motion.sample(n)\n\nt = tf.reshape(tf.stack(s.shape[0]*[time], axis=0)[:,1:], (-1, 1))\nX = tf.reshape(s[:, 1:], (-1, 1))\n\n# Conditional density estimation with MADE.\nmade = tfb.AutoregressiveNetwork(\n  params=2,\n  hidden_units=[32, 32],\n  event_shape=(1,),\n  conditional=True,\n  kernel_initializer=tfk.initializers.VarianceScaling(0.1, seed=42),\n  conditional_event_shape=(1,)\n)\n\ndistribution = tfd.TransformedDistribution(\n  distribution=tfd.Sample(tfd.Normal(loc=0., scale=1.),\n                          sample_shape=(1,)),\n  bijector= tfb.Chain([tfb.Exp(), tfb.MaskedAutoregressiveFlow(made, name='maf')])\n)\n\n# Construct and fit a model.\nX_ = tfkl.Input(shape=(X.shape[-1],), dtype=tf.float32)\nt_ = tfkl.Input(shape=(t.shape[-1],), dtype=tf.float32)\nlog_prob_ = distribution.log_prob(\n  X_, bijector_kwargs={'maf': {'conditional_input': t_}})\nmodel = tfk.Model([X_, t_], log_prob_)\n\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.003),\n              loss=lambda _, log_prob: -log_prob)\n\nbatch_size = 500\n\nhistory = model.fit(x=[X, t],\n                    y=np.zeros((X.shape[0], 0), dtype=np.float32),\n                    batch_size=batch_size,\n                    epochs=40,\n                    steps_per_epoch=X.shape[0] // batch_size,\n                    shuffle=True,\n                    verbose=False)\n\nHere the autoregressive flow was chained with an exponential map to constrain its output to the support of the target distribution, \\(\\mathbb{R}_{>0}\\). Note how the conditional variable \\(t\\) is passed to the log_prob method in a dictionary of keyword arguments to make sure it ends up in the right neural network.\n\n\n\n\n\nThese learned probability density functions can be compared with the true solution\n\\[\nX_t = X_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\nof the geometric Brownian motion SDE, which follows a log-normal distribution. This was the main reason for using this example.\n\n\n\n\n\nIn the same plot, one can see they more or less agree."
  },
  {
    "objectID": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#simulation-based-inference",
    "href": "posts/2023-11-12-Masked-autoregressive-flows-for-stochastic-differential-equations.html#simulation-based-inference",
    "title": "Masked autoregressive flows for stochastic differential equations",
    "section": "Simulation based inference",
    "text": "Simulation based inference\nFor the sake of example, the model presented here only accepted the time \\(t\\) as a conditional variable, and the initial value was deterministic. One can generate richer training data by drawing the drift and volatility parameters as well as the initial value from proposal distributions and pass them as further conditional variables to the model. This makes it possible to learn a parameterized conditional distribution \\(p(x;t\\vert \\mu, \\sigma, x_0)\\) (here \\(t\\) is the parameter indexing the distributions, and \\(\\mu\\), \\(\\sigma\\) and \\(x_0\\) are conditioning it). This is particularly useful if the ultimate goal is to perform Bayesian inference of the parameters \\(\\mu\\), \\(\\sigma\\) or \\(x_0\\) from experimental data. Or one can use it to construct a transition function \\(p(x; t\\vert t_0; x_0)\\) of a Markov chain if the problem involves discrete observations of an underlying stochastic process.\nThe general methodology applied here falls under the scope of simulation based inference. This growing field of computational statistics addresses problems where a model can accurately simulate data, but it has no tractable likelihood function that would enable statistical inference. The Euler-Maruyama method is a good example as deriving a likelihood function would require integrating over all intermediate time steps. As a workaround, some simulation based inference algorithms propose to derive a surrogate likelihood function through machine learning (that was discussed in a previous post), which is what was done here with the conditional masked autoregressive flow."
  },
  {
    "objectID": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html",
    "href": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html",
    "title": "Differentiable and probabilistic programming for risk management and investment decisions",
    "section": "",
    "text": "Differentiable programming is one of the technologies that have fueled the recent deep learning revolutions. It allows for the automatic computation of derivatives (gradients) of functions with respect to their inputs. This is particularly relevant for gradient-based optimization algorithms such as the ones commonly used to train machine learning models. In finance and business applications, differentiable programming can greatly simplify sensitivity analysis by removing the need to compute partial derivatives by hand. Concretely, if the value \\(V\\) of a certain contract depends for instance on a sales forecast \\(S\\) and some rate of return \\(r\\),\n\\[\nV = f(S, r),\n\\]\nthe impact of small changes in the underlying variables can be estimated through a first-order approximation,\n\\[\n\\Delta V = \\frac{\\partial f}{\\partial S} \\Delta S + \\frac{\\partial f}{\\partial r} \\Delta r,\n\\]\nwhich should be straightforward to compute in a differentiable programming library such as JAX through the application of the grad operator.\n\nfrom jax import grad\nimport jax.numpy as jnp\n\ndef contract_value(sales_forecast, discount_rate):\n    time = jnp.arange(54)/2\n    market_time = jnp.arange(1, 42)/2\n    discount_curve = (1 + discount_rate)**(-time)\n    market_penetration_curve = jnp.concatenate(\n        [jnp.zeros(13),\n         jnp.sin((market_time-.1)/8)\n           * jnp.minimum(1, jnp.exp(-(market_time-15)/3))\n        ])\n    return jnp.sum(sales_forecast * market_penetration_curve * discount_curve)\n\nestimated_value = contract_value(100., .1)\nsensitivities = grad(contract_value, argnums=[0, 1])(100., .1)\n\nprint(f'estimated value: {estimated_value}')\nprint(f'sales sensitivity: {sensitivities[0]}')\nprint(f'rate sensitivity: {sensitivities[1]}')\n\nestimated value: 554.726806640625\nsales sensitivity: 5.547268867492676\nrate sensitivity: -7496.2802734375\n\n\nSensitivity analysis is important to inform investment decisions and risk management, so there is no question here about the benefits of automatic differentiation. Furthermore, functions in differentiable programming are chainable, and derivatives automatically follow the chain rule. So if the sales forecast is given by another function, for instance sales_forecast_fn(temperature, store_location), one can compose them to find the sentivity to the temperature.\n\ndef sales_forecast_fn(temperature, store_location):\n    return jnp.exp(temperature) #this is an ice cream business\n\ndef contract_value_2(discount_rate, temperature, store_location):\n    return contract_value(sales_forecast_fn(temperature, store_location),\n                          discount_rate)\n\nsensitivity_temperature = grad(contract_value_2, argnums=1)(.1, 10., 'Basel')\n\nIn particular, this also works when sales_forecast_fn is a differentiable machine learning model such as a neural network, so one can combine financial models with machine learning forecast models of their input variables.\nMaybe more interestingly, the finance team can then focus on the value functions and their financial input, while the commercial team is responsible for the sales forecast, and the differentiable programming framework will ensure the compatibility of the models and the backpropagation of sensitivities.\n\n\nThe store_location variable was not used here because it is discrete and differentiation only works for continuous numerical variables. It was added as an example of a limitation of sensitivity analysis through automatic differentiation. In the case of discrete variables, the concept of “small change” does not make sense anyway, and analysing distributions of contract values conditioned on the store location is going to be informative enough for the purpose of risk management and investment decisions."
  },
  {
    "objectID": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#probabilistic-programming-and-monte-carlo-simulations",
    "href": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#probabilistic-programming-and-monte-carlo-simulations",
    "title": "Differentiable and probabilistic programming for risk management and investment decisions",
    "section": "Probabilistic programming and Monte Carlo simulations",
    "text": "Probabilistic programming and Monte Carlo simulations\nEven for continuous variables, sensitivity analysis through partial derivatives only works for small changes. The next stage of risk management is to build a probabilistic risk model of the joint distribution of input variables and estimate their effect on the output of the value function through Monte Carlo simulation.\nThis used to be a tedious process, especially with multiple correlated random variables. Surprisingly, differentiable programming has also indirectly simplified that process. The recent years have seen the development of several probabilistic programming libraries built on top of differentiable programming libraries. The reason is that automatic differentiation greatly facilitates the implementation of gradient-based inference algorithms such as Hamiltonian Monte Carlo and variational inference, so there has been a push to streamline the whole analysis process.\nIn probabilistic programming, probabilistic models are specified in computer code, and inference is performed more or less automatically. The probabilistic models prescribe ways to draw samples of the probability distributions they encode, and to evaluate the probability density of observations. In the context of risk management, it is enough to express the risk model as a probabilistic program, and the sampling methods will perform Monte Carlo simulations automatically, as illustrated in the following example.\nAgain, the composability of differentiable programs makes it easy to connect a risk model to other parts of an analytics pipeline."
  },
  {
    "objectID": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#drug-development",
    "href": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#drug-development",
    "title": "Differentiable and probabilistic programming for risk management and investment decisions",
    "section": "Drug development",
    "text": "Drug development\nEstimating the value of a drug development program is the archetypical example of a valuation problem where probability distributions play an important role due to the stochastic nature of the whole process. Clinical development traditionally follows different trial phases to derisk the program medically and financially. The success of each phase determines if there will be investment in the next one, and ultimately if the drug will be commercialized and bring in revenue. The industry standard methodology for this valuation is the risk-adjusted net present value (rNPV), where every discounted future cash flow is weighted by the probability that it actually occurs. In other words, it is the expectation value of the random variable representing the sum of discounted future cash flows.\nSources of randomness in the NPV of a drug in development include the success or failure of different phases, that can be modelled by Bernoulli random variables, and the uncertainty in the parameters of the valuation model.\n\n\n\n\n\n\n  \n    \n      \n      estimated cost\n      duration\n      success rate\n    \n    \n      stage\n      \n      \n      \n    \n  \n  \n    \n      phase 1\n      4.0\n      1.0\n      0.66\n    \n    \n      phase 2\n      15.0\n      2.0\n      0.39\n    \n    \n      phase 3\n      45.0\n      2.0\n      0.62\n    \n    \n      regulatory submission\n      2.0\n      1.5\n      0.75\n    \n  \n\n\n\n\nThe volume of sales is particularly hard to estimate as it varies over time and the forecast has to be made a few years in advance. A common way to approach this problem, which was implemented in the contract_value function defined above, is to model with a normalized sales curve the market penetration and then the decay following the end of life of the patent and the entrance of competitors in the market. This sales curve can be empirically derived from previous launches, or artistically constructed like in the example shown here.\n\n\n\n\n\nThe sales can then be modelled with a single random variable, the peak sales, that will be multiplied by the sales curve. Since the estimation of the peak sales can vary over time because of external market conditions, modelling it as a stochastic process is a reasonable assumption. In practice, a geometric Brownian motion is a practical and realistic choice.\n\n\n\n\n\nThe variance increases over time, reflecting the increasing uncertainty of longer and longer forecasts.\nBut when this peak sales stochastic process gets multiplied by the sales curve and the discount curve, the long term variance gets squeezed.\n\n\n\n\n\nThis somewhat controls the distribution of discounted total revenues.\n\n\n\n\n\nThe probabilistic financial model can be implemented as a TensorFlow Probability joint distribution parameterized by a distribution-making generator. This is well-suited for Monte Carlo simulations as mathematical operations can be directly expressed in JAX or TensorFlow code depending on the backend, while tensorflow_probability.distributions objects handle the random variables in the model.\nHere it is assumed that the success rates and durations of clinical trial phases are known, but that the costs come with a 5% uncertainty and follow a normal distribution. Some metrics are extracted as tfd.Deterministic distributions so that they are recorded at sampling, the last one being the NPV of each realization of the simulator. The peak sales are modelled as a geometric Brownian motion as discussed above. If \\(S_t\\) obeys the stochastic differential equation\n\\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t\n\\]\nof a geometric Brownian motion, then its solution is given by\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t},\n\\]\nwhich for discrete observations can be implemented as an exponentiated Markov chain with normal updates.\n\n\nCode\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\n\nkey = random.PRNGKey(0)\nN_SAMPLE = 1000000\n\nRoot = tfd.JointDistributionCoroutineAutoBatched.Root\n\ndef simulate_npv(dev_cost_estimate,\n                 launch_cost_estimate,\n                 success_rate,\n                 peak_sales_estimate,\n                 growth, volatility,\n                 discount_rate):\n    \"\"\"\n    Returns a Monte Carlo sample of NPV scenarios along with\n    certain relevant metrics.\n    Implemented with a TensorFlow Probability joint distribution.\n    \"\"\"\n    launch_cost_estimate = jnp.array(launch_cost_estimate).reshape((1,))\n    dev_cost_std = 0.05 * dev_cost_estimate\n    launch_cost_std = 0.05 * launch_cost_estimate\n    \n    discount_curve = (1 + discount_rate)**(-time)\n    \n    @tfd.JointDistributionCoroutineAutoBatched\n    def cash_flow_model():\n        dev_cost = yield Root(tfd.Independent(tfd.Normal(dev_cost_estimate,\n                                                         dev_cost_std),\n                                              reinterpreted_batch_ndims=1,\n                                              name='dev_cost'))\n        launch_cost = yield Root(tfd.Normal(launch_cost_estimate,\n                                            launch_cost_std,\n                                            name='launch_cost'))\n        success = yield Root(tfd.Independent(tfd.Bernoulli(probs=success_rate),\n                                             reinterpreted_batch_ndims=1,\n                                             name='success'))\n        cash_flow_mask = jnp.cumprod(success, axis=-1)\n        cash_flow_mask = jnp.concatenate([jnp.array([1]),\n                                          cash_flow_mask],\n                                         axis=-1)\n        yield tfd.Deterministic(cash_flow_mask,\n                                name='cash_flow_mask')\n\n        brownian_motion = yield Root(tfd.MarkovChain(\n            initial_state_prior=tfd.Deterministic(0.),\n            transition_fn=lambda _, x: tfd.Normal(\n                loc=x + (growth - volatility**2/2) * time_step,\n                scale=volatility*jnp.sqrt(time_step)),\n            num_steps=54,\n            name='brownian_motion'))\n\n        peak_sales = peak_sales_estimate * jnp.exp(brownian_motion)\n\n        sales = peak_sales * sales_curve * time_step\n        discounted_sales = sales * discount_curve\n        sales_npv = jnp.sum(discounted_sales, axis=-1)\n        yield tfd.Deterministic(sales_npv, name='sales_npv')\n\n        cost_discount = (1 + discount_rate)**(\n            -jnp.concatenate([jnp.array([0.]), jnp.cumsum(length)]))\n        discounted_cost = jnp.concatenate([dev_cost,\n                                           launch_cost],\n                                          axis=-1) * cost_discount\n        yield tfd.Deterministic(discounted_cost, name='discounted_cost')\n\n        NPV = (sales_npv * margin * cash_flow_mask[..., -1]\n               - jnp.sum(discounted_cost * cash_flow_mask, axis=-1))\n\n        yield tfd.Deterministic(NPV, name='NPV')\n    \n    s = cash_flow_model.sample(N_SAMPLE, seed=key)\n    return s\n\n\nA histogram of the simulated NPV reveals four clusters, three of them corresponding to simulated failed programs, and a more diffuse one corresponding to the simulations that made it to commercialization.\n\n\n\n\n\nFocusing only on the failed ones shows that the phase 3 and submission failures form a single cluster due to the relatively low submission cost."
  },
  {
    "objectID": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#risk-management-through-diversification",
    "href": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#risk-management-through-diversification",
    "title": "Differentiable and probabilistic programming for risk management and investment decisions",
    "section": "Risk management through diversification",
    "text": "Risk management through diversification\nWhile the rNPV is really only a crude summary statistics of the distribution of potential outcomes, Monte Carlo simulations reveal how risky these investments can be and provide insights on how to mitigate those risks. The most immediate action is diversification. It reduces the variance and eventually merges the clusters. A simple model of a portfolio of 10 independent copies of the same drug development program considered above illustrates this mechanism.\n\n\n\n\n\nThe variance is reduced, but the probability of a negative value is still fairly high. If squeezing the variance tighter through even more diversification would help, shifting the distribution to the right through active optimization of its mean might be easier (and much cheaper) to implement."
  },
  {
    "objectID": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#active-optimization-of-the-expected-return",
    "href": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#active-optimization-of-the-expected-return",
    "title": "Differentiable and probabilistic programming for risk management and investment decisions",
    "section": "Active optimization of the expected return",
    "text": "Active optimization of the expected return\nThis is where differentiable programming and probabilistic programming nicely come together. The Monte Carlo estimation of the rNPV, which is the mean of the net present values of the simulated scenarios, is itself expressed in a differentiable program, and one can automatically compute its sensitivities.\nA subtle technicality here is that the realizations of Bernoulli random variables are discrete and obstruct the propagation of gradients, but one can use the probabilities instead to build a differentiation-friendly estimator.\n\n\nCode\ndef compute_rNPV(dev_cost_estimate,\n                 launch_cost_estimate,\n                 success_rate,\n                 peak_sales_estimate,\n                 growth, volatility,\n                 discount_rate):\n    \n    s = simulate_npv(dev_cost_estimate,\n                     launch_cost_estimate,\n                     success_rate,\n                     peak_sales_estimate,\n                     growth, volatility,\n                     discount_rate)\n    \n    probs = jnp.concatenate([jnp.ones((1,)),\n                             jnp.cumprod(success_rate)])\n    \n    return jnp.mean(probs[-1] * s.sales_npv * margin\n                    - jnp.sum(probs * s.discounted_cost, axis=-1))\n\nrNPV_sensitivities = grad(compute_rNPV, argnums=[0, 1, 2, 3, 4, 5])(\n     dev_cost_estimate,\n     launch_cost_estimate,\n     success_rate,\n     peak_sales_estimate,\n     growth, volatility,\n     discount_rate)\n\n\nIn practice, one cannot really directly change these variables, so at first glance these sensitivities are not very actionable. However, operational decisions on the execution of the trials typically involve trade-offs that can be hard to optimize. This problem can be addressed with differentiable chainable models. For instance, increasing the sample size should improve the success rate, which drives the rNPV up, but at the same time it increases costs and potentially delays commercialization, which drives the rNPV down. The overall change can be estimated by chaining the compute_rNPV function with dev_cost(sample_size) and success_rate(sample_size) functions, and computing the gradient with respect to the sample size variable.\nArguably, the models presented here have been known for a while, yet they are not always being used to their full potential in industry settings due to the technical challenges to implement them. By providing user-friendly APIs for Monte Carlo simulations and sensitivity computations, differentiable programming libraries will hopefully change that."
  },
  {
    "objectID": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#further-readings",
    "href": "posts/2023-11-11-Differentiable-programming-for-risk-management-and-investment-decisions.html#further-readings",
    "title": "Differentiable and probabilistic programming for risk management and investment decisions",
    "section": "Further readings",
    "text": "Further readings\n\nThe Autodiff Cookbook covers differentiable programming in JAX.\nThe example was inspired by an exercise from Valuation in Life Sciences, which discusses various valuation methods."
  },
  {
    "objectID": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html",
    "href": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html",
    "title": "Hamiltonian mechanics with JAX",
    "section": "",
    "text": "I finally spent some time playing around with JAX. I was especially curious about its automatic differentiation system, which is obviously very useful for computing gradients in machine learning, but also opens up new possibilities for physical modeling. In particular, the Autodiff Cookbook advertises the correspondence between the grad API and differential geometry, which is the fundamental language of classical mechanics.\nConcretely, you can quickly define functions on the phase space with jax.numpy and then automatically compute their gradients. The phase space is the space of all possible states of a physical system, for instance the positions \\(\\vec x_i\\) of its constituent particles and their momenta \\(\\vec p_i\\), or the orientation of a rigid body and its angular momentum. In general, the coordinates of a phase space can always be locally chosen so that they are split into positions \\(\\mathbf{q} = (q_1, \\dots, q_n)\\) and conjugate momenta \\(\\mathbf{p} = (p_1, \\dots, p_n)\\), and their time evolution is given by the equations of motion\n\\[\n\\begin{aligned}\n\\dot{\\mathbf{q}} &=  \\frac{\\partial H}{\\partial \\mathbf{p}} = \\lbrace \\mathbf{q}, H \\rbrace \\\\\n\\dot{\\mathbf{p}} &=  -\\frac{\\partial H}{\\partial \\mathbf{q}} = \\lbrace \\mathbf{p}, H \\rbrace\n\\end{aligned} \\tag{1}\n\\]\nwhere \\(H(\\mathbf{q}, \\mathbf{p})\\) is the Hamiltonian function of the system, or simply the Hamiltonian, which is essentially its energy. With automatic differentiation, all you need to build this set of differential equations is to specify the Hamiltonian function, which can spare you a lot of pain.\nThe curly braces are the Poisson bracket, a fundamental structure of the phase space. In the canonical coordinates \\((\\mathbf{q}, \\mathbf{p})\\), also called the Darboux coordinates, it is defined as\n\\[\n\\lbrace f, g \\rbrace = \\sum_{i=1}^n\\left(\n  \\frac{\\partial f}{\\partial q_i}\\frac{\\partial g}{\\partial p_i}\n  - \\frac{\\partial f}{\\partial p_i}\\frac{\\partial g}{\\partial q_i}\n\\right) \\tag{2}\n\\]\nfor any two functions \\(f\\) and \\(g\\) on the phase space. Together with the Hamiltonian function, the Poisson bracket encodes the time evolution of any observable of the system (a quantity that depends on the state of the system and can be measured),\n\\[\n\\frac d{dt}f(\\mathbf{q}, \\mathbf{p}, t) = \\lbrace f, H \\rbrace + \\frac{\\partial f}{\\partial t}. \\tag{3}\n\\]\nThe mathematical interpretation is that the Poisson bracket transforms the Hamiltonian function \\(H\\) into a vector field \\(\\lbrace \\cdot, H \\rbrace\\), the so-called Hamiltonian vector field.\nThe Poisson bracket is also an essential piece in the connection between classical and quantum mechanics, but this is another story and we will get back to it when quantum computers are more broadly available.\nIn JAX, the Poisson bracket in Darboux coordinates can be defined as a function that takes as its arguments two functions of q and p and returns a third function of q and p, where q and p are arrays."
  },
  {
    "objectID": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#harmonic-oscillator",
    "href": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#harmonic-oscillator",
    "title": "Hamiltonian mechanics with JAX",
    "section": "Harmonic Oscillator",
    "text": "Harmonic Oscillator\nAs an example, we can consider the harmonic oscillator, the Drosophila of theoretical physics, with Hamiltonian\n\\[H(\\mathbf{q}, \\mathbf{p}) = \\frac{\\mathbf{p}^2}{2m} + \\frac{k\\,\\mathbf{q}^2}{2}, \\tag{4}\\]\nand we set \\(m=k=1\\) for convenience. Two observables are simply the position and the momentum.\n\ndef harm_osc_hamiltonian(q, p):\n    return 0.5*jnp.dot(q, q) + 0.5*jnp.dot(p, p)\n\ndef position(q, p):\n    return q\n\ndef momentum(q, p):\n    return p\n\nprint(poisson_bracket_darboux(harm_osc_hamiltonian, position)(5., 10.))\n\n-10.0\n\n\nNote that this code works only if \\(q\\) and \\(p\\) are one-dimensional since the Poisson bracket accepts only scalar-valued functions as its arguments. If we work with higher-dimensional spaces, we can also select a single coordinate, for instance with a lambda function.\n\nq_3d = jnp.array([1., 2., 3.])\np_3d = jnp.array([4., 5., 6.])\n\nprint(poisson_bracket_darboux(harm_osc_hamiltonian, lambda q, p: q[1])(q_3d, p_3d))\n\n-5.0\n\n\nFor easy integration, the right hand side of the Hamiltonian equations of motion can be fed to the odeint integrator from the jax.experimental.ode module. The initial conditions can be specified as an array or a pytree of arrays. In this case, a tuple y0 = (q0, p0) of the initial values of the position and momentum is convenient as it can be passed as a *args to the functions we defined on the phase space.\nThe first argument of odeint is a function to evaluate the time derivative of the solution y at time t. In the geometric interpretation, we talk about a vector field. Here we can use the poisson_bracket_darboux function applied on the functions we defined on the phase space.\n\nfrom jax.experimental.ode import odeint\n\nt = jnp.linspace(0., 10., 101)\ny0 = (1., 0.)\n\ndef harm_osc_hamiltonian_vector_field(y, t):\n    return (poisson_bracket_darboux(position, harm_osc_hamiltonian)(*y),\n            poisson_bracket_darboux(momentum, harm_osc_hamiltonian)(*y))\n\ny = odeint(harm_osc_hamiltonian_vector_field, y0, t)\n\nWe can plot the solution to check that we obtained the expected sinusoidal oscillations."
  },
  {
    "objectID": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#gravity",
    "href": "posts/2021-12-04-hamiltonian-mechanics-with-jax.html#gravity",
    "title": "Hamiltonian mechanics with JAX",
    "section": "Gravity",
    "text": "Gravity\nTo explore more features of Hamiltonian mechanics, we need a problem with more dimensions. In this age of space tourism, the physics of a small test mass moving around in the gravitational field of a much more massive object is particularly relevant. “Much more massive” in this context means that we can assume that the very heavy body sits at rest and only the very light body orbits around it. The phase space of this system corresponds to the position \\(\\vec{q}\\) and momentum \\(\\vec{p}\\) of the test mass, where we have set the origin at the position of the heavy object. The Hamiltonian is given by\n\\[H(\\vec{q}, \\vec{p}) = \\frac{\\Vert\\vec{p}\\Vert^2}{2m} - \\frac{GmM}{\\Vert\\vec{q}\\Vert} \\tag{5}\\]\nwhere \\(m\\) and \\(M\\) are the masses of the light and heavy objects respectively, and \\(G\\) is the gravitational constant. Since I am a mathematical physicist, I like to use a natural unit system where \\(G\\cdot M=e=c=\\hbar=k_B=1\\) and cows are spherical with radius 1. For convenience, we also set \\(m=1\\).\nIf we wanted to use the Poisson bracket to compute the Hamiltonian vector field like we did for the one-dimensional harmonic oscillator, we would have to do it component by component, so it is more practical to use the gradients of \\(H\\) directly in the equations of motion (1).\n\ndef gravitation_hamiltonian(q, p):\n    return 0.5*jnp.dot(p, p) - 1/jnp.sqrt(jnp.dot(q, q))\n\nr0 = 1.\np0 = 1.2\nt = jnp.linspace(0., 15., 101)\ny0 = (jnp.array([r0, 0., 0.]), jnp.array([0., p0, 0.]))\n\ndef grav_hamiltonian_vector_field(y, t):\n    return (grad(gravitation_hamiltonian, argnums=1)(*y),\n          -grad(gravitation_hamiltonian, argnums=0)(*y))\n  \ny = odeint(grav_hamiltonian_vector_field, y0, t)\n\nThe initial conditions were chosen so that the orbital plane is orthogonal to the \\(z\\)-axis, and we can easily plot the orbit.\n\n\n\n\n\nIt is encouraging to see that this numerical solution follows Kepler’s first law and draws an elliptic orbit where the center of the gravitational field is one of the focal points.\nSince the Hamiltonian is invariant under rotations, Noether’s theorem guarantees that the angular momentum is conserved. Using equation (3), we can quickly check that the time derivative of its \\(z\\)-component (the only non-zero component here) \\(\\dot L_z = \\lbrace L_z, H \\rbrace\\) is zero numerically. This is a nice exercise to showcase the practicality of the vectorizing map vmap.\n\ndef angular_momentum(q, p):\n    return jnp.cross(q, p)\n\ndef L_z(q, p):\n    return angular_momentum(q, p)[3]\n\nfrom jax import vmap\nprint(vmap(poisson_bracket_darboux(L_z, gravitation_hamiltonian))(*y))\n\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0.]\n\n\nThese two examples illustrate the power of JAX to express concepts from differential geometry in code and run simulations efficiently, which facilitates the development of physics-based models. To explore that subject further, I can recommend to look into JAX, MD, a library for molecular dynamics simulations, or JAX-CFD, one for computational fluid dynamics."
  },
  {
    "objectID": "posts/2021-01-13-anomaly-detection-in-protocol-deviations.html",
    "href": "posts/2021-01-13-anomaly-detection-in-protocol-deviations.html",
    "title": "Anomaly detection in protocol deviations",
    "section": "",
    "text": "The number of protocol deviations \\(n_{pdevs}\\) reported by an investigator site should depend linearly on the number of patients \\(n_{pats}\\) enrolled at that site, as more patients mean more chances of deviations, so these are the minimal attributes that should get collected prior to the analysis. The following table is a sample of the data we will use as an example.\n\n\n\n\n\n\n  \n    \n      \n      site_id\n      n_pats\n      n_pdevs\n    \n  \n  \n    \n      0\n      site_0\n      9\n      15\n    \n    \n      1\n      site_1\n      10\n      14\n    \n    \n      2\n      site_2\n      15\n      18\n    \n    \n      3\n      site_3\n      9\n      10\n    \n    \n      4\n      site_4\n      5\n      4\n    \n  \n\n\n\n\nA quick glance at the full dataset reveals there is indeed a relationship that looks linear, so a potential approach would be to build a regression model \\(n_{pdevs} \\sim \\theta \\cdot n_{pats}\\) and quantify how every observation of \\(n_{pdevs}\\) deviates from its estimation.\n\n\n\n\n\nFrom that scatterplot, it also appears that the residuals of a regression would not be iid. Rather, their variance would grow as the number of patients increases. This rules out least square regression, as it assumes iid normal residuals. Since we are dealing with count data, working with the Poisson distribution is a natural approach,\n\\[n_{pdevs} \\vert n_{pats} \\sim Poi(\\lambda(n_{pats})),\\]\nand we can set \\(\\lambda(n_{pats}) = \\theta \\cdot n_{pats}\\) to reflect our assumption of a linear relationship between \\(n_{pdevs}\\) and \\(n_{pats}\\). Note that a regular Poisson regression would fail to capture that linear relationship due to its exponential link function.\nIn this model, we immediately have \\(E\\left[n_{pdevs} \\vert n_{pats}\\right] = Var\\left[n_{pdevs} \\vert n_{pats}\\right] = \\theta \\cdot n_{pats}\\), which seems to reproduce the increasing spread of \\(n_{pdevs}\\).\nWe can infer the value of \\(\\theta\\) through maximum likelihood estimation and use the resulting conditional Poisson model at each site to compute the cumulative distribution function (CDF) of the observed numbers of protocol deviations.\n\n\nCode\ndef loss(par, n_pat, n_dev):\n    theta = tf.math.exp(par[0])\n    dist = tfd.Poisson(n_pat * theta)\n    return -tf.reduce_sum(dist.log_prob(n_dev))\n\ndef compute_cdf(par, n_pat, n_dev):\n    theta = tf.math.exp(par[0])\n    dist = tfd.Poisson(n_pat * theta)\n    return dist.cdf(n_dev)\n\n@tf.function\ndef loss_and_gradient(par, n_pat, n_dev):\n    return tfp.math.value_and_gradient(lambda par: loss(par, n_pat, n_dev), par)\n\n\ndef fit(n_pat, n_dev):\n    init = 2*tf.ones(1)\n    opt = tfp.optimizer.lbfgs_minimize(\n        lambda par: loss_and_gradient(par, n_pat, n_dev), init, max_iterations=1000\n    )\n    return opt\n\nn_pats = tf.constant(data['n_pats'], dtype=tf.float32)\nn_pdev = tf.constant(data['n_pdevs'], dtype=tf.float32)\n\nmle = fit(n_pats, n_pdev)\n\n#print(f\"converged: {mle.converged}\")\n#print(f\"iterations: {mle.num_iterations}\")\n\nx = np.linspace(0, 40)\npar = mle.position\ny = np.exp(par[0]) * x\n\ncdfs = compute_cdf(par, n_pats, n_pdev)\n\n\nThese CDF values are concentrated around 0 and 1, which makes this approach quite impractical and suggests that the variance of the model is lower than the variance of the data.\n\n\n\n\n\nThe low variance can be increased by treating \\(\\lambda(n_{pats})\\) as a random function, rather than a deterministic one. So we assume that \\(\\lambda(n_{pats})\\) is drawn from a gamma distribution, \\(\\lambda(n_{pats}) \\sim \\Gamma(\\alpha, \\beta)\\), where the rate parameter \\(\\beta\\) is inversely proportional to the expected number of protocol deviations from a given site, \\(\\beta = \\beta_{pat} / n_{pats}\\), in order to ensure linearity in \\(n_{pats}\\). In this context, maximum likelihood estimation would be a nightmare to implement (because of the rate parameters of the Poisson distribution) and probably not very stable, so it is best to turn to Bayesian inference via MCMC algorithms. We thus pick gamma priors for \\(\\alpha\\) and \\(\\beta_{pat}\\) with a shape parameters of 2 to prevent the corresponding Markov chains from drifting too close to zero, where pathological behaviors seem to occur with more permissive priors in this model.\n\n\nCode\nsites = tf.constant(data['site_id'])\nn_pats = tf.constant(data['n_pats'], dtype=tf.float32)\nn_pdev = tf.constant(data['n_pdevs'], dtype=tf.float32)\n\nmdl_pd = tfd.JointDistributionSequential([\n    #alpha\n    tfd.Gamma(2, 2, name='alpha'),\n    #beta_pt\n    tfd.Gamma(2, 2, name='beta_pt'),\n    #pdev rates for each patient\n    lambda beta_pt, alpha: tfd.Independent(\n        tfd.Gamma(alpha[...,tf.newaxis], beta_pt[...,tf.newaxis] / n_pats[tf.newaxis,...]),\n        reinterpreted_batch_ndims=1\n    ),\n    #observed pdevs\n    lambda rates: tfd.Independent(tfd.Poisson(rates), reinterpreted_batch_ndims=1)\n])\n\n\nWe can sample the posterior distribution of this model with a Hamiltonian Monte Carlo algorithm and assess the convergence of the Markov chains before computing the posterior probabilities of interest.\n\n\nCode\ndtype = tf.dtypes.float32\nnchain = 5\nburnin=3000\nnum_steps=10000\nalpha0, beta_pt0, rates0, _ = mdl_pd.sample(nchain)\ninit_state = [alpha0, beta_pt0, rates0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [0.01, 0.01, 0.01]]\ntarget_log_prob_fn = lambda *init_state: mdl_pd.log_prob(\n    list(init_state) + [tf.cast(n_pdev, dtype=dtype)])\n\n\nunconstraining_bijectors = 3*[tfb.Exp()]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=num_steps, burnin=burnin):\n    \n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.is_accepted\n               )\n\n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn,\n        num_leapfrog_steps=3,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin\n    )\n\n    # Sampling from the chain.\n    [alpha, beta_pt, rates], is_accepted = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return alpha, beta_pt, rates, is_accepted\n\nalpha, beta_pt, rates, is_accepted = run_chain(\n    init_state, step_size, target_log_prob_fn, unconstraining_bijectors)\n\nalpha_ = alpha[burnin:,:]\nalpha_ = tf.reshape(alpha_, [alpha_.shape[0]*alpha_.shape[1]])\nbeta_pt_ = beta_pt[burnin:,:]\nbeta_pt_ = tf.reshape(beta_pt_, [beta_pt_.shape[0]*beta_pt_.shape[1]])\nrates_ = rates[burnin:,:]\nrates_ = tf.reshape(rates_, [rates_.shape[0]*rates_.shape[1], rates_.shape[2]])\n\nrates_dist_ = tfd.Gamma(alpha_[:,tf.newaxis], beta_pt_[:, tf.newaxis] / n_pats[tf.newaxis,...])\nrates_cdf_ = rates_dist_.cdf(rates_)\n\nposterior = {}\nposterior['alpha'] = tf.transpose(alpha[burnin:, :]).numpy()\nposterior['beta_pt'] = tf.transpose(beta_pt[burnin:, :]).numpy()\nposterior['rate0'] = tf.transpose(rates[burnin:, :, 0])\nposterior['rate1'] = tf.transpose(rates[burnin:, :, 1])\nposterior['rate2'] = tf.transpose(rates[burnin:, :, 2])\n\naz_trace = az.from_dict(posterior=posterior)\n\nprint(f'MCMC acceptance rate: {is_accepted.numpy().mean()}')\n\naz.plot_trace(az_trace)\nplt.show()\n\n\nMCMC acceptance rate: 0.73832\n\n\n\n\n\nGiven a Markov chain sample \\((\\hat\\alpha, \\hat\\beta_{pat}, (\\hat\\lambda_i)_{i=1,\\dots,N})\\), where \\(i\\) indexes the investigator sites, we can evaluate the CDF of \\(\\Gamma(\\hat\\alpha, \\hat\\beta_{pat} / n_{pats, i})\\) at \\(\\hat\\lambda_i\\) and average these quantities along the whole Markov chain to obtain an indicator of over- and underreporting. This indicator corresponds to the rate tail area of the inferred Poisson rates under their posterior predictive distribution. Low values mean a risk of underreporting, and high values a risk of overreporting (see the last column of the following sample table).\n\n\n\n\n\n\n  \n    \n      \n      site\n      n_pats\n      n_pdev\n      mean_pdev_rate\n      std_pdev_rate\n      rate_tail_area\n    \n  \n  \n    \n      0\n      site_0\n      9\n      15\n      15.51\n      3.82\n      0.42\n    \n    \n      1\n      site_1\n      10\n      14\n      14.56\n      3.72\n      0.36\n    \n    \n      2\n      site_2\n      15\n      18\n      18.73\n      4.28\n      0.31\n    \n    \n      3\n      site_3\n      9\n      10\n      10.77\n      3.19\n      0.30\n    \n    \n      4\n      site_4\n      5\n      4\n      4.86\n      2.08\n      0.24\n    \n    \n      5\n      site_5\n      7\n      19\n      18.98\n      4.26\n      0.61\n    \n    \n      6\n      site_6\n      25\n      31\n      31.74\n      5.59\n      0.32\n    \n    \n      7\n      site_7\n      2\n      11\n      9.82\n      2.83\n      0.82\n    \n    \n      8\n      site_8\n      6\n      7\n      7.75\n      2.71\n      0.32\n    \n    \n      9\n      site_9\n      17\n      27\n      27.65\n      5.17\n      0.40\n    \n  \n\n\n\n\nThe distribution of the rate tail areas looks more convenient than in the first simple model. Not only did we add variance with a mixture model, but we also assess the inferred Poisson parameters rather than the observations, and the former are shrunk by their prior.\n\n\n\n\n\nThis metric also seems to agree with the intuition of what underreporting and overreporting should look like.\n\n\n\n\n\nWe can set thresholds for over- and underreporting alerts at .8 and .2 respectively to illustrate how an auditor could use this model to select investigator sites to focus on.\n\n\n\n\n\n\n\n\n\n\nThis method illustrates the potential of Bayesian modeling to supercharge a regression analysis toolbox with a variety of likelihood functions that can capture the intricacies of the data generating process and inference methods that are more flexible in quantifying uncertainties than the standard GLM methods. These properties are particularly helpful in practical tasks such as anomaly detection or risk management that combine expert insights with quantitative modeling."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "",
    "text": "If you plan to automate the execution of multible Bayesian inference jobs, typically to regularly update your posterior distribution as new data comes in, you might find that MCMC algorithms take too long to sample their chains. Variational inference can speed things up considerably (you can find a good introduction here), at the expense of converging only to an approximation of the true posterior, which is often good enough for practical applications.\nLately, I have been experimenting with TensorFlow-Probability that implement automatic differentiation variational inference, namely tfp.vi.fit_surrogate_posterior and tfp.layers, to see how to integrate them into some of my projects.\nI collected insights from various guides, tutorials, and code documentation, that I am summarizing here, mainly for future reference, but also for the benefit of people I will manage to convert to Bayesianism. The code is applied to a toy example of Bayesian logistic regression on simulated data, because it is helpful in this context to compare results to the true parameters of the data generating process."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#mcmc",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#mcmc",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "MCMC",
    "text": "MCMC\nIt is always good to start with a benchmark, so I collected an MCMC sample of the posterior distribution and computed the means and standard deviations of the model parameters.\n\n# Specify the model for Bayesian logistic regression.\nmdl_logreg = tfd.JointDistributionSequentialAutoBatched([\n    #betas\n    tfd.Sample(tfd.Normal(loc=0., scale=5.), X.shape[1]),\n    #alpha\n    tfd.Normal(loc=0., scale=20.),\n    #observations\n    lambda alpha, betas: tfd.Independent(\n        tfd.Bernoulli(logits=alpha + tf.tensordot(X, betas, axes=1)),\n        reinterpreted_batch_ndims=1)\n])\n\n# Specify the MCMC algorithm.\ndtype = tf.dtypes.float32\nnchain = 5\nb0, a0, _ = mdl_logreg.sample(nchain)\ninit_state = [b0, a0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [.1, .1]]\ntarget_log_prob_fn = lambda *init_state: mdl_logreg.log_prob(\n    list(init_state) + [y])\n\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Identity(),\n    tfb.Identity(),\n]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=8000, burnin=1000):\n\n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.target_log_prob,\n            pkr.inner_results.inner_results.leapfrogs_taken,\n            pkr.inner_results.inner_results.has_divergence,\n            pkr.inner_results.inner_results.energy,\n            pkr.inner_results.inner_results.log_accept_ratio\n               )\n  \n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin,\n      step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n          inner_results=pkr.inner_results._replace(step_size=new_step_size)),\n      step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n      log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio\n    )\n\n    chain_state, sampler_stat = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return chain_state, sampler_stat\n\n# Run the chain\nsamples, sampler_stat = run_chain(\n    init_state, step_size, target_log_prob_fn, unconstraining_bijectors)\n\n# using the pymc3 naming convention\nsample_stats_name = ['lp', 'tree_size', 'diverging', 'energy', 'mean_tree_accept']\nsample_stats = {k:v.numpy().T for k, v in zip(sample_stats_name, sampler_stat)}\nsample_stats['tree_size'] = np.diff(sample_stats['tree_size'], axis=1)\n\nvar_name = ['beta', 'alpha']\nposterior = {k:np.swapaxes(v.numpy(), 1, 0) \n             for k, v in zip(var_name, samples)}\n\naz_trace = az.from_dict(posterior=posterior, sample_stats=sample_stats)\n\naz.plot_trace(az_trace)\nplt.show()"
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.vi",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.vi",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "Variational inference with tfp.vi",
    "text": "Variational inference with tfp.vi\nThe dedicated tool for variational inference in TensorFlow-Probability, tfp.vi.fit_surrogate_posterior, requires a similar amount of preparatory work as tfp.mcmc algorithms. The specification of the target posterior is actually the same.\n\nmdl_logreg = tfd.JointDistributionSequentialAutoBatched([\n    #betas\n    tfd.Sample(tfd.Normal(loc=0., scale=5.), X.shape[1]),\n    #offset\n    tfd.Normal(loc=0., scale=20.),\n    #observations\n    lambda offset, betas: tfd.Independent(\n        tfd.Bernoulli(logits=offset + tf.tensordot(X, betas, axes=1)),\n        reinterpreted_batch_ndims=1)\n])\n\nunnormalized_log_prob = lambda *x: mdl_logreg.log_prob(x + (y,))\n\nThen, instead of specifying a Markov chain, we have to define a variational family of surrogate posterior candidates. This can require quite a bit of work, but if our model has been built with a joint distribution list and we are happy with a mean field approximation (this is usually the case if we care only about the marginal posterior distributions of the individual model parameters and not their correlation), the TensorFlow tutorial on modeling with joint distributions provides a helper function to do that. Note that if the support of the distributions is not a full \\(\\mathbb{R}^n\\), we have to implement unconstraining bijectors. The same tutorial shows how to do it.\n\n# Build meanfield ADVI for a jointdistribution\n# Inspect the input jointdistribution and replace the list of distribution with\n# a list of Normal distribution, each with the same shape.\ndef build_meanfield_advi(jd_list, observed_node=-1):\n    \"\"\"\n    The inputted jointdistribution needs to be a batch version\n    \"\"\"\n    # Sample to get a list of Tensors\n    list_of_values = jd_list.sample(1)  # <== sample([]) might not work\n\n    # Remove the observed node\n    list_of_values.pop(observed_node)\n\n    # Iterate the list of Tensor to a build a list of Normal distribution (i.e.,\n    # the Variational posterior)\n    distlist = []\n    for i, value in enumerate(list_of_values):\n        dtype = value.dtype\n        rv_shape = value[0].shape\n        loc = tf.Variable(\n            tf.random.normal(rv_shape, dtype=dtype),\n            name='meanfield_%s_mu' % i,\n            dtype=dtype)\n        scale = tfp.util.TransformedVariable(\n            tf.fill(rv_shape, value=tf.constant(0.02, dtype)),\n            tfb.Softplus(),\n            name='meanfield_%s_scale' % i,\n        )\n\n        approx_node = tfd.Normal(loc=loc, scale=scale)\n        if loc.shape == ():\n            distlist.append(approx_node)\n        else:\n            distlist.append(\n              # TODO: make the reinterpreted_batch_ndims more flexible (for \n              # minibatch etc)\n              tfd.Independent(approx_node, reinterpreted_batch_ndims=1)\n            )\n\n    # pass list to JointDistribution to initiate the meanfield advi\n    meanfield_advi = tfd.JointDistributionSequential(distlist)\n    return meanfield_advi\n\nIt remains to choose an optimizer and set a few hyperparameters such as the number of optimization steps, the sample size used to estimate the loss function, and the learning rate of the optimizer. To better tune those and then to assess convergence, it can be helpful to enrich the trace function with statistics of the variational distribution.\n\nmeanfield_advi = build_meanfield_advi(mdl_logreg, observed_node=-1)\n\n# Check the logp and logq\nadvi_samples = meanfield_advi.sample(4)\nprint([\n  meanfield_advi.log_prob(advi_samples),\n  unnormalized_log_prob(*advi_samples)\n  ])\n\n# Specify a trace function that collects statistics during inference and an optimizer\ntrace_fn = lambda x: (x.loss, meanfield_advi.mean(), meanfield_advi.stddev())\nopt = tf.optimizers.Adam(learning_rate=.08)\n\n#@tf.function(experimental_compile=True)\ndef run_approximation():\n    loss_ = tfp.vi.fit_surrogate_posterior(\n                unnormalized_log_prob,\n                surrogate_posterior=meanfield_advi,\n                optimizer=opt,\n                sample_size=50,\n                num_steps=200,\n                trace_fn=trace_fn\n    )\n    return loss_\n\nloss_, q_mean_, q_std_ = run_approximation()\n\n\n\n\n\n\nThe loss itself is obviously a good indicator of convergence, but the model parameters seem to need a few more iterations to reach a steady state of the optimizer.\n\n\n\n\n\nThe standard deviation estimates exhibit some more noise.\n\n\n\n\n\nTo reduce the noise, one can try to increase the sample size used to estimate the loss function, or decrease the learning rate. The price for both actions is a slower convergence, so the number of iterations would need to be adjusted accordingly. Implementing a learning rate schedule could offer a trade-off."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.layers",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#variational-inference-with-tfp.layers",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "Variational inference with tfp.layers",
    "text": "Variational inference with tfp.layers\nIf the model can be cast as a neural network with a prior distribution on the neuron parameters, chances are it can be expressed as a Keras model with dedicated tfp.layers, a TensorFlow tool for probabilistic machine learning.\nThe probabilistic layers tutorial covers a least squares regression example in details and was a good inspiration, especially the case 4: aleatoric & epistemic uncertainty. The biggest difference is in the specification of the prior, to which they assign learnable parameters.\nTo understand how to build a probabilistic machine learning model, we can start with the neural network expression of the logistic regression, namely a single neuron with a sigmoid activation, and “probabilize” it.\n\ntfk = tf.keras\n\nclassical_model = tf.keras.Sequential([\n    tfk.layers.Dense(1),\n    tfk.layers.Activation('sigmoid')\n])\n\nclassical_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n                        loss=tfk.losses.BinaryCrossentropy()\n                       )\n\nclassical_model.fit(X, y, epochs=50)\n\nThe output of this network is the probability parameter of a Bernoulli distribution that is fit to the observed data through minimization of the binary cross-entropy. The activation layer can be replaced with a tfp.layers.DistributionLambda layer that outputs a tfd.Bernoulli distribution directly, which can be fit through minimization of the negative log-likelihood (note that the activation can be skipped if we use the logit parameter).\nThe parameters of the logistic regression are encoded in the tfk.layers.Dense layer. Its probabilistic version, tfp.layers.DenseVariational, also specifies a prior distribution for these parameters as well as a variational family to estimate their posterior.\n\n# Define the negative log likelihood loss function for the `DistributionLambda`\n# head of the model.\nnegloglik = lambda y, rv_y: -rv_y.log_prob(y)\n\n# Define a function to constrain the scale parameters to the real positive\ndef constrain_scale(x):\n    c = np.log(np.expm1(1.))\n    return 1e-5 + tf.nn.softplus(c + x)\n\n# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\ndef posterior_mean_field(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    return tf.keras.Sequential([\n        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n        tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n            tfd.Normal(loc=t[..., :n],\n                       scale=constrain_scale(t[..., n:])\n                       #scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n                      ),\n            reinterpreted_batch_ndims=1)),\n    ])\n\n# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\ndef prior_ridge(kernel_size, bias_size, dtype=None):\n    return lambda _: tfd.Independent(\n        tfd.Normal(loc=tf.zeros(kernel_size + bias_size),\n                   scale=tf.concat([5*tf.ones(kernel_size),\n                                    20*tf.ones(bias_size)],\n                                   axis=0)),\n        reinterpreted_batch_ndims=1\n    )\n\n# Specify the model\nprobabilistic_model = tf.keras.Sequential([\n  tfp.layers.DenseVariational(units=1,\n                              make_posterior_fn=posterior_mean_field,\n                              make_prior_fn=prior_ridge,\n                              kl_weight=1/X.shape[0]\n                             ),\n  tfp.layers.DistributionLambda(lambda t: tfd.Bernoulli(logits=t)),\n])\n\nTo understand the kl_weight argument of tfp.layers.DenseVariational, we need to take a look at the mathematics behind variational inference that has been so far left out of this discussion. To estimate the posterior distribution \\(P(Z\\vert X)\\) of the parameters \\(Z\\) given the data \\(X\\), the variational inference algorithms implemented here look for the distribution \\(Q(Z)\\) in the variational family that minimizes the Kullback-Leibler divergence\n\\[\nD_{\\mathrm{KL}}(Q \\Vert P) = \\mathbb{E}_Q\\left[ \\log \\frac{Q(Z)}{P(Z\\vert X)} \\right]\n\\]\nof \\(P(Z\\vert X)\\) from \\(Q(Z)\\). This quantity still depends on the unknown posterior \\(P(Z\\vert X)\\), but minimizing it is equivalent to maximizing the evidence lower bound\n\\[\n\\mathrm{ELBO} = \\mathbb{E}_Q\\left[ \\log P(X \\vert Z) + \\log P(Z) - \\log Q(Z) \\right]\n\\]\nwhich depends only on the likelihood, the prior, and the variational distributions.\nWe prefer minimization problems, so we consider the negative ELBO loss function, and we make the dependency on individual data points explicit,\n\\[\n- \\mathrm{ELBO} =  \\mathbb{E}_Q\\left[ \\sum_i \\left( - \\log P(X_i \\vert Z) + \\frac{1}{N}( \\log Q(Z) - \\log P(Z))\\right) \\right]\n\\]\nwhere \\(N\\) is the number of data points. The first term in the sum corresponds to the negative log-likelihood passed as the loss argument to probabilistic_model.compile. The rest of the sum is a regularization implemented in tfp.layers.DenseVariational, where \\(Q(Z)\\) corresponds to the make_posterior_fn argument, \\(P(Z)\\) to make_prior_fn, and \\(N\\) to kl_weight=1/X.shape[0].\nAs a side note, the sample size argument in tfp.vi.fit_surrogate_posterior gives the number of points sampled from \\(Q\\) to compute a Monte Carlo estimate of the gradient of the ELBO.\nWith the Keras API, we can use callbacks to collect training statistics or implement early stopping policies, and we have access to the tf.data.Dataset API for batch training (kl_weight would need to be adapted accordingly). Here we use callbacks to record the parameters of the variational distribution during training.\n\ndef get_model_stats(probabilistic_model):\n    weights = probabilistic_model.layers[0].weights[0]\n    k = X.shape[1]\n    locs = weights[:k+1].numpy()\n    scales = constrain_scale(weights[k+1:]).numpy()\n    return locs, scales\n\nparams_history = []\n\nparams_callback = tfk.callbacks.LambdaCallback(\n    on_epoch_end=lambda epoch, logs: params_history.append(\n        np.array(get_model_stats(probabilistic_model))))\n\n\nprobabilistic_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n                            loss=negloglik\n                           )\n\nhistory = probabilistic_model.fit(X, y,\n                                  epochs=200,\n                                  callbacks=[params_callback])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence of the standard deviation parameters is a bit slow but could be improved with a more informed choice of starting values."
  },
  {
    "objectID": "posts/2021-02-01-variational-inference-with-tfp.html#conclusion",
    "href": "posts/2021-02-01-variational-inference-with-tfp.html#conclusion",
    "title": "Variational inference with TensorFlow-Probability",
    "section": "Conclusion",
    "text": "Conclusion\nThe means and standard deviations of the posterior distributions inferred with tfp.mcmc, tfp.vi and tfp.layers (bnn) are summarized in the following table.\n\n\n\n\n\n\n  \n    \n      \n      true_value\n      mcmc_mean\n      mcmc_std\n      vi_mean\n      vi_std\n      bnn_mean\n      bnn_std\n    \n    \n      parameter\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      beta_0\n      1.000\n      1.095\n      0.145\n      1.094\n      0.125\n      1.108\n      0.115\n    \n    \n      beta_1\n      0.000\n      0.079\n      0.122\n      0.084\n      0.115\n      0.055\n      0.120\n    \n    \n      beta_2\n      -2.000\n      -1.840\n      0.184\n      -1.838\n      0.163\n      -1.835\n      0.146\n    \n    \n      alpha\n      1.000\n      1.223\n      0.142\n      1.203\n      0.131\n      1.218\n      0.124\n    \n  \n\n\n\n\nThe results are very similar across the three methods. Convergence is quite faster with variational inference, but it requires a bit more work to specify sensible variational families.\nWhile tfp.vi is applicable to a wider class of problems, tfp.layers gives access to Keras functionalities such as callbacks, and, more interestingly, batch training with the tf.data.Dataset API.\nThere is no definitive rule for which method to apply to which problem, but it is important to be aware of the limitations and benefits of variational inference algorithms before using them. In this example we knew the true parameters in advance, but in real applications, one should have validation procedures in place to ensure the variational family is large enough to capture the phenomenon of interest, for instance by comparison with MCMC results, or for prediction tasks with a test set where the labels/outcomes are known."
  },
  {
    "objectID": "posts/2021-06-05-gaussian-random-walks-with-tfp.html",
    "href": "posts/2021-06-05-gaussian-random-walks-with-tfp.html",
    "title": "Gaussian random walks",
    "section": "",
    "text": "def make_gaussian_random_walk(length, step_size=1.):\n    random_walk = tfd.TransformedDistribution(\n        tfd.TransformedDistribution(\n            tfd.Sample(tfd.Normal(loc=0, scale=step_size), sample_shape=(length-1,)),\n            tfb.Pad(paddings=((1,0),))\n        ),\n        tfb.Cumsum()\n        )\n    return random_walk\n\nThe sample method of the resulting tfp distribution allows us to quickly simulate realizations of a random walk.\n\n\n\n\n\nTo make sure that this implementation actually works, we can reproduce the analysis of the stochastic volatility model treated in the PyMC3 documentation, and described as an example in the original No-U-Turn sampler paper by Hoffman and Gelman. Incidentally, it is how I found out that starting at zero was important. At first I was not doing it, and it led to an overestimation of the volatility at the onset of the time series.\nThe goal is to model the volatility of the S&P 500 index.\n\n\n\n\n\nThe model is described in the PyMC3 example and the NUTS paper, so we just provide the TensorFlow-Probability version:\n\nstoch_vol_mdl = tfd.JointDistributionSequential([\n    #step_size\n    tfd.Exponential(10, name='step_size'),\n    #volatility\n    lambda step_size: make_gaussian_random_walk(observed_returns.shape[0], step_size),\n    #nu (degrees of freedom)\n    tfd.Exponential(0.1, name='nu'),\n    #returns\n    lambda nu, volatility, step_size: tfd.Independent(\n        tfd.StudentT(df=nu[...,tf.newaxis],\n                     loc=0,\n                     scale=tf.math.exp(volatility),\n                     name='returns'),\n        reinterpreted_batch_ndims=1\n    )\n])\n\nTo infer the posterior distributions, we also use the No-U-Turn sampler, and we can compare the marginal posteriors of the nu and step_size parameters as well as the estimated volatility over time with the results of the PyMC3 case study to gain confidence in the validity of the method.\n\n\nCode\n# Specify the MCMC algorithm.\ndtype = tf.dtypes.float32\nnchain = 5\nssize0, vol0, nu0, _ = stoch_vol_mdl.sample(nchain)\ninit_state = [ssize0, vol0, nu0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [.1, .1, .1]]\ntarget_log_prob_fn = lambda *init_state: stoch_vol_mdl.log_prob(\n    list(init_state) + [observed_returns])\n\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Exp(),\n    tfb.Identity(),\n    tfb.Exp()\n]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=1000, burnin=1000):\n\n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.target_log_prob,\n            pkr.inner_results.inner_results.leapfrogs_taken,\n            pkr.inner_results.inner_results.has_divergence,\n            pkr.inner_results.inner_results.energy,\n            pkr.inner_results.inner_results.log_accept_ratio,\n            pkr.inner_results.inner_results.is_accepted\n               )\n  \n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin,\n      step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n          inner_results=pkr.inner_results._replace(step_size=new_step_size)),\n      step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n      log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio\n    )\n\n    chain_state, sampler_stat = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return chain_state, sampler_stat\n\n# Run the chain\nsamples, sampler_stat = run_chain(\n    init_state, step_size, target_log_prob_fn, unconstraining_bijectors)"
  },
  {
    "objectID": "posts/2021-01-08-probabilistic assessment-of-safety-underreporting.html",
    "href": "posts/2021-01-08-probabilistic assessment-of-safety-underreporting.html",
    "title": "Probabilistic assessment of safety underreporting",
    "section": "",
    "text": "A clinical trial is usually run across several investigator sites, which are required to report to the trial sponsor adverse events experienced by enrolled subjects. This is necessary to assess the safety of the intervention under investigation. Underreporting from certain sites has been a recurrent issue. Trial sponsors and health authorities rely on audits and inspections to ensure completeness of the collected safety data. This effort can and should be informed by statistical analysis of the adverse event reporting process with a focus on identifying sites that report at lower rates.\nThe main hurdle is that you are looking for missing data that you do not know is missing. We developed a solution around an observational model for adverse event reporting and a way to characterizing outlying investigator sites.\nThe relevant data is the count of adverse events reported by every patient enrolled in a study and their site assignment. To illustrate our methodology, we use data from the control arm of NCT00617669, an oncology study from Project Data Sphere.\nBayesian data analysis combines expert knowledge on the process of interest, expressed as a probabilistic model, with observed data to infer posterior distributions of the model parameters. These posterior distributions allow us to quantify uncertainties and risks, and to compute posterior expectation values of quantities of interest.\nHierarchical models provide a natural framework for subgroup analysis, where similar entities such as the sites of a single study can share information while maintaining a certain degree of independence. In our situation, at the bottom of the hierarchy, the count of adverse events reported by the \\(n_i\\) patients of site \\(i\\) can be modelled with a Poisson distribution, \\(Y_i \\sim \\mathrm{Poi}(\\lambda_i)\\). The \\(N_{\\mathrm{sites}}\\) Poisson rates \\(\\lambda_i\\) can in turn be modelled as realizations of a random variable unique to the whole study with Gamma distribution \\(\\Gamma(\\alpha, \\beta)\\). The parameters \\(\\alpha\\) and \\(\\beta\\) are unknown, so we assume a vague prior for both of them, \\(\\alpha \\sim \\mathrm{Exp}(1)\\) and \\(\\beta \\sim \\mathrm{Exp}(10)\\). The full joint distribution \\[\nP(\\alpha, \\beta, \\lambda_i, Y_{i,j}) = P(\\alpha)P(\\beta)\\prod_{i=1}^{N_{\\mathrm{sites}}}P(\\lambda_i\\vert\\alpha,\\beta)\\prod_{j=1}^{n_i} P(Y_{i,j}\\vert \\lambda_i)\n\\] is summarized in the following graphical representation.\n\nNumerical modelling of such joint distributions is made easy by probabilistic programming libraries such as TensorFlow-Probability (utilized here), Stan, PyMC3 or Pyro.\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfb = tfp.bijectors\ntfd = tfp.distributions\n\nsites = tf.constant(data['site_number'])\nobserved_ae = tf.constant(data['ae_count_cumulative'])\n\nunique_sites, sites_idx, sites_counts = tf.unique_with_counts(sites)\n\nmdl_ae = tfd.JointDistributionSequential([\n    #alpha\n    tfd.Gamma(1, 1, name='alpha'),\n    #beta\n    tfd.Gamma(1, 10, name='beta'),\n    #Poisson rates for all sites\n    lambda beta, alpha: tfd.Sample(tfd.Gamma(alpha, beta),\n                                   sample_shape=unique_sites.shape,\n                                   name='rates'),\n    #observed AEs\n    lambda rates: tfd.Independent(\n        tfd.Poisson(tf.gather(rates, sites_idx, axis=-1)),\n        reinterpreted_batch_ndims=1,\n        name='observations')\n])\n\nIn Bayesian inference, the analytical derivation of the posterior distribution is often impossible, but the same probabilistic programming libraries provide efficient implementations of MCMC algorithms that return samples of the posterior distribution.\n\n\nCode\ndtype = tf.dtypes.float32\nnchain = 10\nburnin=1000\nnum_steps=10000\nalpha0, beta0, rates0, _ = mdl_ae.sample(nchain)\ninit_state = [alpha0, beta0, rates0]\nstep_size = [tf.cast(i, dtype=dtype) for i in [.1, .1, .1]]\ntarget_log_prob_fn = lambda *init_state: mdl_ae.log_prob(\n    list(init_state) + [tf.cast(observed_ae, dtype=dtype)])\n\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Exp(),\n    tfb.Exp(),\n    tfb.Exp()\n]\n\n@tf.function(autograph=False, experimental_compile=True)\ndef run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n              num_steps=num_steps, burnin=burnin):\n    \n    def trace_fn(_, pkr):\n        return (\n            pkr.inner_results.inner_results.is_accepted\n               )\n\n    kernel = tfp.mcmc.TransformedTransitionKernel(\n      inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn,\n        num_leapfrog_steps=3,\n        step_size=step_size),\n      bijector=unconstraining_bijectors)\n\n    hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n      inner_kernel=kernel,\n      num_adaptation_steps=burnin\n    )\n\n    # Sampling from the chain.\n    [alpha, beta, rates], is_accepted = tfp.mcmc.sample_chain(\n        num_results=num_steps,\n        num_burnin_steps=burnin,\n        current_state=init_state,\n        kernel=hmc,\n        trace_fn=trace_fn)\n    return alpha, beta, rates, is_accepted\n\n\nTo assess convergence, we sample several chains that we can inspect visually (here with the ArviZ package) to make sure that they converge to the same distribution, mix well, and do not display pathological autocorrelations.\n\n\n\n\n\nIf we have to monitor several studies, we might want to automate that process. In that case, we can compute statistics of the sampled chains such as effective sample sizes or \\(\\hat{R}\\) and implement automatic checks, for instance that \\(\\hat{R}\\) is sufficiently close to 1.\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_mean\n      ess_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      3001\n      3.193\n      1.246\n      1.081\n      5.565\n      0.022\n      0.016\n      3146.0\n      3146.0\n      2849.0\n      3764.0\n      1.0\n    \n    \n      3002\n      3.050\n      0.616\n      1.964\n      4.250\n      0.006\n      0.004\n      11787.0\n      11787.0\n      11502.0\n      17056.0\n      1.0\n    \n    \n      3003\n      6.043\n      1.695\n      3.084\n      9.280\n      0.022\n      0.015\n      6031.0\n      6031.0\n      5772.0\n      9765.0\n      1.0\n    \n    \n      3004\n      12.764\n      2.030\n      9.037\n      16.628\n      0.014\n      0.010\n      19819.0\n      19819.0\n      19489.0\n      28664.0\n      1.0\n    \n    \n      alpha\n      1.774\n      0.223\n      1.364\n      2.192\n      0.003\n      0.002\n      6352.0\n      6352.0\n      6253.0\n      16128.0\n      1.0\n    \n    \n      beta\n      0.119\n      0.017\n      0.087\n      0.151\n      0.000\n      0.000\n      7576.0\n      7576.0\n      7441.0\n      18742.0\n      1.0\n    \n  \n\n\n\n\nFrom the samples \\((\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\lambda}_i)\\) of the Markov chain, we can estimate the posterior risk of underreporting. One way to do it is to compute the left tail area of each \\(\\lambda_i\\) (remember that the index \\(i\\) enumerates the sites) under the distribution \\(\\Gamma(\\hat{\\alpha}, \\hat{\\beta})\\) and average it along the trace of the Markov chain. This corresponds to the probability that a Poisson rate drawn randomly from the study level distribution falls below the inferred Poisson rate of site \\(i\\), or, more explicitly, that a reference site from the same study would report less adverse events.\n\n\n\n\n\n\n  \n    \n      \n      site\n      mean_ae_rate\n      std_ae_rate\n      rate_tail_area\n      observed_ae\n    \n  \n  \n    \n      0\n      3001\n      3.20\n      1.25\n      0.09\n      [4, 1]\n    \n    \n      1\n      3002\n      3.05\n      0.62\n      0.08\n      [2, 2, 1, 2, 5, 5, 5, 1]\n    \n    \n      2\n      3003\n      6.04\n      1.69\n      0.22\n      [7, 4]\n    \n    \n      3\n      3004\n      12.76\n      2.03\n      0.52\n      [3, 27, 8]\n    \n    \n      4\n      3005\n      9.35\n      2.10\n      0.37\n      [12, 6]\n    \n    \n      5\n      3006\n      3.69\n      1.32\n      0.11\n      [2, 4]\n    \n    \n      6\n      3007\n      6.03\n      2.31\n      0.22\n      [5]\n    \n    \n      7\n      3008\n      17.95\n      1.72\n      0.69\n      [11, 4, 16, 31, 23, 23]\n    \n    \n      8\n      3009\n      6.52\n      1.76\n      0.24\n      [6, 6]\n    \n    \n      9\n      3010\n      14.71\n      0.92\n      0.59\n      [21, 10, 6, 17, 10, 7, 26, 19, 18, 1, 18, 23, ...\n    \n  \n\n\n\n\nA lower value of this rate tail area thus indicates a higher risk of underreporting. This metric can be used by auditors and inspectors to prioritize their activities. Moreover, since it is the probability of a specified event, one can immediately compare the rate tail areas of sites from different studies. This is especially interesting for quality programs that oversee several trials.\nThis approach demonstrates the flexibility of Bayesian methods to build models that answer specific questions about a given process. In this example, the reporting rates of the different sites are the quantities of interest, but they are unobserved and have to be inferred from the available data with a mathematical model. The user-friendly API of modern probabilistic programming libraries combined with efficient inference algorithms have been making this type of workflow much easier than in the past and will certainly fuel a broader adoption in sectors that have not been traditionally driven by quantitative insights."
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html",
    "title": "Bayesian Poisson factorization",
    "section": "",
    "text": "A lot of recommender systems are built on matrix factorization models, where the partially observed matrix of user/item interactions is approximated by a product of matrices encoding latent characteristics of users and items. They can be corrected by user and item bias terms, and modified by activation functions that map to the data type of the observed interactions (e.g. binary kudos on Strava activities, counts of visits of a YouTube channel, ratings on Tripadvisor, or time spent watching a TikTok video before swiping up).\nIn machine learning, these matrix factorizations are often implemented as embeddings into latent spaces followed by scalar products of the user latent vectors by the item latent vectors, and the model is fit to historical data with a gradient descent algorithm. A Keras tutorial on collaborative filtering describes the methodology with the Movielens dataset.\nWhile the resulting model only provides point estimates of future interactions between users and items, a Bayesian treatment of the problem would add an approximation of the uncertainty of these estimates. Formally, we could express the same model with priors on the vector embeddings in a probabilistic programming library and sample the posteriors with an MCMC algorithm, but computing the likelihood of past interactions can be impractical for very large datasets. Moreover, due to the invariance of the model under permutations of the embedding dimensions, MCMC sampling of the multimodal posterior would be a nightmare. On the other hand, minimizing the Kullback-Leibler divergence \\(D_{KL}\\left(Q(Z) \\Vert P(Z\\vert X)\\right)\\) between a surrogate posterior and the true posterior distribution through variational inference produces a mode-seeking behavior (see for instance these lecture notes), a bit like gradient descent in “classical” machine learning finds a local minimum.\nIn this blog post, we will be exploring how to implement a model inspired by Gopalan, Hofman and Blei (Scalable Recommendation with Poisson Factorization) with TensorFLow Probability."
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html#bayesian-models-and-variational-inference",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html#bayesian-models-and-variational-inference",
    "title": "Bayesian Poisson factorization",
    "section": "Bayesian models and variational inference",
    "text": "Bayesian models and variational inference\nAs a reminder, minimizing the (intractable) Kullback-Leibler divergence \\(D_{KL}\\left(Q(Z) \\Vert P(Z\\vert X)\\right)\\) between the variational distribution \\(Q(Z)\\) and the true posterior \\(P(Z\\vert X)\\) is equivalent to maximizing the (computable) evidence lower bound\n\\[\n\\mathrm{ELBO} = \\mathbb{E}_Q\\left[ \\log P(X \\vert Z) \\right] - D_{KL}(Q(Z) \\Vert P(Z)),\n\\]\nwhere the first term is the expectation under the surrogate distribution of the likelihood function of the model and the second term the negative Kullback-Leibler divergence between the surrogate distribution and the prior distribution of the model parameters.\nIn the mean field approximation, where we assume that the variational distribution \\(Q\\) factorizes over the latent variables, we can implement variational inference with probabilistic layers from the tfp.layers module, which will automatically keep track of the variational parameters during training. These layers are conveniently combined in a Keras model, where the last layer is typically a distribution layer corresponding to the observed data \\(X\\), and from which which we can use the log_prob method to compute the log-likelihood of the model. In their forward mode, the probabilistic layers draw samples from the surrogate distribution \\(Q\\) they implement that we can use to compute Monte-Carlo estimates of \\(\\mathbb{E}_Q\\left[ \\log P(X \\vert Z) \\right]\\), the first part of the ELBO function.\nThe second part, \\(D_{KL}(Q(Z) \\Vert P(Z))\\), can be implemented either through an activity regularizer, activity_regularizer = tfpl.KLDivergenceRegularizer(prior_distribution), or through a custom loss term added in the call method of the layer. The former is suitable for global parameters of the model that are shared by all training examples, and the latter is required for latent variables associated to specific training examples.\nWe will wrap up these initial theoretical considerations with a couple of observations that are important for a good implementation of these methods. The first is that Keras models are trained through minimization of a loss function, so instead of maximizing the ELBO, we will be minimizing the negative ELBO. Concretely, the loss function passed to the model.compile method will be the negative log-likelihood, and \\(D_{KL}(Q(Z) \\Vert P(Z))\\) will be added to the loss rather than subtracted from it. The second is that Keras model training routines evaluate the loss function as a sum over the training examples. While this is straightforward for the likelihood part of the loss when observations are assumed to be conditionally independent,\n\\[\n- \\log P(X \\vert Z) = - \\sum_i  \\log P(X_i \\vert Z),\n\\]\nextra care needs to be taken for the Kullback-Leibler term\n\\[\nD_{KL}(Q(Z) \\Vert P(Z)) = \\mathbb{E}_Q\\left[ \\log Q(Z) - \\log P(Z))\\right],\n\\]\nwhich has to be expressed as a sum over the training examples like the log-likelihood. In practice, this can often be achieved through a weighted sum. For instance, if we have only parameters that are shared by all training examples, we can apply a \\(1/N\\) weight, where \\(N\\) is the number of data points \\(X_i\\),\n\\[\n- \\mathrm{ELBO} =  \\mathbb{E}_Q\\left[ \\sum_i \\left( - \\log P(X_i \\vert Z) + \\frac{1}{N}( \\log Q(Z) - \\log P(Z))\\right) \\right].\n\\]\nFor parameters linked to only subsets of the data points, or even individual data points (we often speak of latent variables), a bit of algebra might be needed to find the proper weights.\nConstant weights can be specified as optional parameters of methods like tfpl.KLDivergenceRegularizer, but non-uniform weights have to be coded in the add_loss method of the Keras model, which is the reason why this approach is required as mentioned earlier."
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-embeddings",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-embeddings",
    "title": "Bayesian Poisson factorization",
    "section": "Probabilistic embeddings",
    "text": "Probabilistic embeddings\nWhile a standard embedding maps discrete values to vectors in a latent space, a probabilistic embedding specifies a probability distribution over the latent space. It can be constructed as a sequence of a parameter layer, that contains the variational distribution parameters, and a TensorFlow Probability distribution layer that encapsulates the sample method and the prior distribution for the KL term. The parameter layer can be expressed as a standard embedding into a space of dimension \\(n_{params} \\times D\\), where \\(D\\) is the dimension of the original latent space, and \\(n_{params}\\) the number of variational parameters of the corresponding component of the variational distribution \\(Q\\).\nA custom Keras layer wraps the construction of a probabilistic embedding, with its __init__ method receiving the hyperparameters. In the following code example, we picked Gamma distributions for both the priors and the variational distributions. Note how the KL term is added to the loss with self.add_loss and weighted by latent_kl_weights passed as an argument to the call method of the custom Keras layer. It allows us to specify a unique weight for each training example from inputs.\n\nclass GammaEmbedding(tfkl.Layer):\n    def __init__(self, num_classes, embedding_size,\n                 embedding_concentration, embedding_rate,\n                 **kwargs):\n        super(GammaEmbedding, self).__init__(**kwargs)\n        \n        self.embedding_parameters = tfkl.Embedding(\n            num_classes,\n            2 * embedding_size,\n            embeddings_initializer=\"he_normal\"\n        )\n        \n        self.embedding_distribution = tfpl.DistributionLambda(\n            lambda x: tfd.Independent(\n                tfd.Gamma(tf.math.exp(x[:, :embedding_size]),\n                          rate=tf.math.exp(x[:, embedding_size:])),\n                reinterpreted_batch_ndims=1)\n        )\n        \n        self.embedding_prior = tfd.Independent(\n            tfd.Gamma(\n                embedding_concentration *\n                tf.ones(shape=(embedding_size,), dtype=tf.float32),\n                rate=embedding_rate),\n            reinterpreted_batch_ndims=1\n        )\n            \n    def __call__(self, inputs, latent_kl_weights):\n        embedding_param = self.embedding_parameters(inputs)\n        embedding_distribution = self.embedding_distribution(embedding_param)\n        self.add_loss(\n            tf.reduce_sum(latent_kl_weights *\n                embedding_distribution.kl_divergence(self.embedding_prior)\n                         )\n        )\n        return embedding_distribution\n\nThe user and item bias terms mentioned earlier could be implemented as one-dimensional probabilistic embeddings and added to the scalar products of user and item embeddings, but Gopalan et al. suggest to introduce these degrees of freedom as random rate parameters of the embedding Gamma distributions with a hierarchical model construction. Compared to the simple Gamma embedding where the embedding priors were fixed and specified in the __init__ method, we now need them to evolve with the variational parameters of their parent distribution during training, so they need to be dynamically computed in the __call__ method. To model the random rate parameter, we can make use of the Gamma embedding layer we already constructed, with an embedding dimension of 1.\n\nclass RateAdjustedGammaEmbedding(tfkl.Layer):\n    def __init__(self, num_classes, embedding_size,\n                 parent_concentration, parent_rate,\n                 embedding_concentration, **kwargs):\n        super(RateAdjustedGammaEmbedding, self).__init__(**kwargs)\n        \n        self.embedding_size = embedding_size\n        \n        self.rate_distribution = GammaEmbedding(num_classes=num_classes,\n                                                embedding_size=1,\n                                                embedding_concentration=\n                                                parent_concentration,\n                                                embedding_rate=parent_rate\n                                               )\n        \n        \n        self.embedding_concentration = embedding_concentration\n        \n        self.embedding_parameters = tfkl.Embedding(\n            num_classes,\n            2 * embedding_size,\n            embeddings_initializer=\"he_normal\"\n        )\n        \n        self.embedding_distribution = tfpl.DistributionLambda(\n            lambda x: tfd.Independent(\n                tfd.Gamma(tf.math.exp(x[:, :embedding_size]),\n                          rate=tf.math.exp(x[:, embedding_size:])),\n                reinterpreted_batch_ndims=1\n            )\n        )\n            \n    def __call__(self, inputs, latent_kl_weights):\n        embedding_param = self.embedding_parameters(inputs)\n        embedding_distribution = self.embedding_distribution(embedding_param)\n\n        embedding_rate = self.rate_distribution(inputs, latent_kl_weights)\n        embedding_prior = tfd.Independent(\n            tfd.Gamma(self.embedding_concentration,\n                      rate=embedding_rate * tf.ones((1, self.embedding_size))),\n            reinterpreted_batch_ndims=1)\n        self.add_loss(\n            tf.reduce_sum(latent_kl_weights *\n                          embedding_distribution.kl_divergence(embedding_prior))\n        )\n        \n        return embedding_distribution"
  },
  {
    "objectID": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-recommender",
    "href": "posts/2022-02-23-bayesian-poisson-factorization.html#probabilistic-recommender",
    "title": "Bayesian Poisson factorization",
    "section": "Probabilistic recommender",
    "text": "Probabilistic recommender\nWith the probabilistic embeddings defined as custom layers, the full model only needs a few lines of code. The user and movie embeddings are constructed as rate-adjusted Gamma embeddings, and their scalar product will be the rate of the Poisson distribution that generates the observations, implemented as a distribution lambda layer.\nThe KL weights need to be passed to the corresponding probabilistic layers, so we need to include them in the input of the model, for instance in two additional columns. For the user part, we observe that the KL divergence term is decomposed as a sum over user terms,\n\\[\n\\mathbb{E}_Q\\left[ \\log Q(Z_{users}) - \\log P(Z_{users})) \\right] = \\sum_u \\mathbb{E}_Q\\left[ \\log Q(Z_u) - \\log P(Z_u)) \\right],\n\\]\nbut we need to express it as a sum over all user/movie interactions of the training set. If we simply replace the sum, we are counting the same user once for every movie they have rated, so we can rescale these terms by this number,\n\\[\n\\mathbb{E}_Q\\left[ \\log Q(Z_{users}) - \\log P(Z_{users})) \\right] = \\sum_i \\mathbb{E}_Q\\left[ \\log Q(Z_{u[i]}) - \\log P(Z_{u[i]})) \\right]\\frac1{N_{u[i]}},\n\\]\nwhere \\(u[i]\\) denotes the user of interaction \\(i\\), and \\(N_{u[i]}\\) the number of movies rated by this user. The KL weights of the movie part can be derived in the same way.\n\nclass ProbabilisticRecommender(tfk.Model):\n    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n        super(ProbabilisticRecommender, self).__init__(**kwargs)\n        self.num_users = num_users\n        self.num_movies = num_movies\n        self.embedding_size = embedding_size\n        \n        self.user_embedding = RateAdjustedGammaEmbedding(\n            num_users,\n            embedding_size,\n            parent_concentration=1.,\n            parent_rate=.8,\n            embedding_concentration=1.\n        )\n        \n        self.movie_embedding = RateAdjustedGammaEmbedding(\n            num_movies,\n            embedding_size,\n            parent_concentration=1.,\n            parent_rate=.8,\n            embedding_concentration=1.\n        )\n\n        self.head = tfpl.DistributionLambda(lambda t: tfd.Poisson(t))\n        \n    def call(self, inputs):\n        user_vector = self.user_embedding(inputs[:, 0], inputs[:, 2])\n        movie_vector = self.movie_embedding(inputs[:, 1], inputs[:, 3])\n        dot_user_movie = tf.reduce_sum(user_vector * movie_vector, axis=-1)\n        return self.head(dot_user_movie)\n\nAs mentioned in the preliminary observations, this model requires a negative log-likelihood loss function, otherwise it is straightforward to train it like any Keras model.\n\nEMBEDDING_SIZE = 20\n\nBATCH_SIZE = 1024\n\nnegloglik = lambda y, rv_y: -rv_y.log_prob(y)\n\nprob_model = ProbabilisticRecommender(num_users,\n                                      num_movies,\n                                      embedding_size=EMBEDDING_SIZE\n                                     )\nprob_model.compile(\n    loss=negloglik, optimizer=tfk.optimizers.Adam(learning_rate=0.01)\n)\n\nOnce the model has been trained, we can call it on new user/item pairs to produce a Poisson distribution of posterior predicted observations. We can directly sample user ratings from this distribution with .sample(), or call its .rate_parameter() method to find the posterior predicted rate. The latter offers a higher resolution to rank items for a given user (it is a continuous variable rather than an integer) and is therefore more practical for recommender systems.\nWhen the model is called, each probabilistic layer returns a single sample from its learned variational distribution. To estimate the posterior predicted Poisson rate of a user/item interaction, one can call the model several times to obtain a sample. In real world applications, drawing a single Poisson rate or only a few of them rather than estimating the posterior mean to score an item might prove more useful as it offers a broader variety of suggestions to users whose tastes are less certain, namely with user embedding distributions of higher variance. It addresses the exploration-exploitation trade-off with a mechanism similar to Thompson sampling (see also this previous blog post).\nIn general, users who have provided less ratings or good ratings across a large spectrum of items will get less defined embeddings, and this approach will give them recommendations that explore the item landscape more broadly than for users with tighter embeddings. These users with better-known tastes will still get random suggestions relatively far from their usual preference, albeit less frequently than the users with less defined embeddings, but this is not something that happens with models based on classical embeddings, which always return the same results.\nThis mechanism is also interesting when acquiring new users who have not yet provided ratings. We can itialize their embeddings to match the priors, or learned distributions from similar users but with wider variance, and use this untrained model to draw recommendations that are compatible with our prior knowledge. When they start giving ratings, we can train the corresponding user embedding layers to incorporate the new knowledge, while freezing the item embedding layers for stability and increased speed.\nWith TensorFlow Probability layers, we can thus add a Bayesian flavor to more traditional recommender systems and address issues such as exploration-exploitation trade-offs or cold starts in a more principled way. From another angle, we can express probabilistic models such as matrix factorization models as Keras models and take advantage of the tf.data.Dataset API for batch training with potentially large datasets."
  },
  {
    "objectID": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html",
    "href": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html",
    "title": "Cleaning Strava data with Kalman smoothers",
    "section": "",
    "text": "Note: This is a second version of the article, edited on 2022-11-09, with a better method to reject outliers suggested by Isaac Skog.\nLike many cyclists and mountain bikers, I record and share my rides with Strava. When I use my electric mountain bike, I normally export the relevant data to Strava from the Bosch eBike Flow app that controls and monitors the performance of the motor and gets geolocation data from the mobile phone. I noticed that this data is sometimes affected by measurement errors that look like weird spikes on my routes. This is visible on this screenshot of a ride I did in Morgins last summer.\nTo the data scientist in me who likes to correlate how much he rides week on week with the amount of ice cream he eats, this is frustrating because these errors are reflected in the total distances and elevation gains.\nAccording to my satellite navigation guru friend Yannick, this can happen when the signal from a satellite bounces on cliffs before reaching the receiver, and I assume that the processing software puts strong priors on roads and trails that lead to funky corrections. For instance, the border crossing of Pas de Morgins seems to attract quite a lot of points from the nearby hillside.\nMeasurement errors can be somewhat corrected with signal processing techniques, but first we need to get the data. Strava allows you to export gpx files of your rides, which can be read with the gpxpy library. The result is a time series of latitude, longitude and elevation measurements collected every second (except when the motor automatically turns off when I spend too much time eating snacks or petting cows and alpacas), and as a preprocessing step we can normalize them.\nSome of the errors like the one at Pas de Morgins seem to be characterized by the track alternating between the real underlying trajectory and a single erroneous point. If we count the occurrences of the most frequent coordinates, that point at Pas de Morgins is precisely at the top of the list.\nTherefore the first obvious fix would be to exclude the most frequently repeated points and try to replace them through interpolation of the real trajectory, for instance with a Kalman filter, with the added benefit of reducing measurement noise on the rest of the trajectory.\nIn the version of the Kalman filter with no control input to the dynamical system, we assume that the data is generated by a latent Markov process (the state \\(z_{t+1}\\) at a given time step only depends on the previous step \\(z_t\\) and not the earlier ones) with normally distributed transitions,\n\\[\nz_{t+1} \\vert z_t \\sim \\mathcal{N} \\left( F\\, z_t + b, \\ \\Sigma_{tr} \\right),\n\\]\nwhere \\(F\\) is a (possibly time-dependent) transition matrix, \\(b\\) a bias vector and \\(\\Sigma_{tr}\\) the covariance matrix of the transition noise. We further assume that a similar Gaussian linear process generates the observations from a given latent state,\n\\[\nx_t \\vert z_t \\sim \\mathcal{N} \\left( H\\, z_t + c, \\ \\Sigma_{obs} \\right),\n\\]\nwhere \\(H\\) is an observation matrix, \\(c\\) a bias term and \\(\\Sigma_{obs}\\) the covariance of the observation process.\nWith these strong assumptions of normality and linear transformations, the posterior distribution of the latent states given observations can be derived through linear algebra operations. These operations are readily implemented in the tfd.LinearGaussianStateSpaceModel distribution in the TensorFlow Probability library. The forward_filter method runs a Kalman filter to compute the filtered marginal distribution \\(P(z_t \\vert x_{1..t})\\) conditioned only on past observations, and the posterior_marginal method runs a Kalman smoother to compute the filtered marginal distribution \\(P(z_t \\vert x_{1..T})\\) conditioned on the full history of observations, including the future ones. These algorithms are discussed in details in section 8.3 of Probabilistic Machine Learning: Advanced Topics. Since we have access to the whole GPS track, we will of course use the Kalman smoother. Conveniently, these methods also work if we condition only on a subset of the observations. Which observations should be ignored can be specified with an optional mask argument.\nThe first example in the official TensorFlow Probability documentation is precisely the tracking problem that interests us, where the latent space is the real position and the observation is the noisy measurement. Here we will increase the dimension of the latent space and add a velocity vector \\(\\mathbf{v}\\) and an acceleration vector \\(\\mathbf{a}\\) to the true position \\(\\mathbf{s} = (latitude, longitude, elevation)\\), so that \\(z = (\\mathbf{s}, \\mathbf{v}, \\mathbf{a})\\), and use transitions inspired by classical mechanics,\n\\[\nz_{t+1} \\vert z_t \\sim \\mathcal{N} \\left( (\\mathbf{s}_t + \\mathbf{v}_t, \\mathbf{v}_t + \\mathbf{a}_t, 0), \\Sigma_{tr} \\right),\n\\]\nwith a diagonal transition covariance matrix \\(\\Sigma_{tr}\\) that carries much more uncertainty in its acceleration components. This kind of model is useful to estimate the velocity and acceleration solely from the position measurements (simply taking finite differences would be very inaccurate given the measurement noise), and it also incorporates knowledge from the laws of physics on how past states are going to influence future states. In a nutshell, it assumes that the forces affecting the bike and its rider are subject to random changes (I can brake, accelerate or turn, or collide with external obstacles), and my velocity and position are going to be a solution to Newton’s second law of motion (\\(F = ma\\)). Such a model can extrapolate the future position from the current velocity, and correct the velocity and acceleration estimations from the measured positions.\nNote that to estimate the physical velocity and acceleration, it would be better to transform the spherical coordinates into cartesian coordinates, but for our purpose of error correction, it should be good enough to use the (locally normalized) spherical ones.\nIf we overlap the observed time series of normalized geographic coordinates with the filtered ones, it appears that the solid blocks on the observed values that correspond to the back and forth patterns do get corrected by the Kalman smoothing with masked repeated values.\nThe spikes caused by isolated errors are a bit attenuated by the filter, provided the observation noise scale parameter is large enough. If it is too small, extreme points are interpreted as legitimate measurements because they are unlikely to be observation errors under the model specifications. But too large a scale parameter blurs out the inferred trajectory as it cannot resolve the trail geometry.\nSo the trick is to run a first Kalman smoother with an observation noise scale parameter that is large enough to identify the individual outliers (while masking the repeated measurements that we already identified), followed by a second one with lower noise while masking the individual outliers in addition to the repeated measurements to infer the true trajectory. For the identification of individual outliers after the first smoothing, we can make use of the posterior marginal distributions \\(\\mathcal{N}(\\hat\\mu_t, \\hat\\Sigma_t)\\) returned by the smoother that should “explain” the observation \\(x_t\\) after multiplication by the observation matrix \\(H\\). Concretely, the quantity\n\\[Q = \\left(x_t - H\\, \\hat\\mu_t\\right)^T \\, \\left(H \\, \\hat\\Sigma_t \\, H^T \\right)^{-1} \\, \\left(x_t - H\\, \\hat\\mu_t\\right)\\]\nshould follow a \\(\\chi^2_3\\) distribution under the model assumptions, so we can use a chi-squared test to pick outliers. Note that the threshold of that test has to be set fairly low to catch all outliers, because of the larger noise scale paramter of the first smoother.\nThe posterior mean of the second smoother produces a clean estimation of the real trajectory, free of any weird spike. As a last step, we can rescale it and plot it on OpenStreetMap to admire the result and think of future bike rides!"
  },
  {
    "objectID": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html#acknowledgement",
    "href": "posts/2022-11-05-Cleaning-Strava-data-with-Kalman-smoothers.html#acknowledgement",
    "title": "Cleaning Strava data with Kalman smoothers",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nI would like to thank Maxime Baillifard for showing me his hometrails around Morgins, an area that seems to affect GPS signals like the Bermuda Triangle, Yannick Stebler for providing a rational explanation about signals bouncing on cliffs that debunked that myth, and Isaac Skog for suggesting to use a chi-squared test to reject outliers after reading a first version of this article that was using a less robust quadratic form."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Articles",
    "section": "",
    "text": "probabilistic programming\n\n\ntensorflow-probability\n\n\nmachine learning\n\n\n\n\nCommon methods for solving SDEs are discussed, along with their limitations when it comes to statistical inference. Conditional masked autoregressive flows are presented as a flexible alternative.\n\n\n\n\n\n\nNov 12, 2023\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndifferentiable programming\n\n\nprobabilistic programming\n\n\ntensorflow-probability\n\n\nquantitative finance\n\n\ndrug development\n\n\nrisk management\n\n\n\n\nSome applications of differentiable programming in data science beyond the usual machine learning algorithms, with an example in drug development valuation.\n\n\n\n\n\n\nNov 11, 2023\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimulation-based inference\n\n\nmachine learning\n\n\ntensorflow-probability\n\n\n\n\nA discussion of simulation-based inference fueled by machine learning algorithms, with an example of neural likelihood estimation in TensorFlow Probability.\n\n\n\n\n\n\nJul 14, 2023\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscientific computing\n\n\nsimulation-based inference\n\n\njax\n\n\ntensorflow-probability\n\n\n\n\nA discussion of differentiable programming and scientific computing in the JAX ecosystem, with an example of simulation-based inference.\n\n\n\n\n\n\nJun 24, 2023\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmountain biking\n\n\nsignal processing\n\n\ntensorflow-probability\n\n\nsatellite navigation\n\n\n\n\nA tale of mountain biking, paranormal activity and signal processing.\n\n\n\n\n\n\nNov 5, 2022\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nvariational inference\n\n\ntensorflow-probability\n\n\nmachine learning\n\n\n\n\nA probabilistic recommender system implemented with TensorFlow Probability layers.\n\n\n\n\n\n\nFeb 23, 2022\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJAX\n\n\nautograd\n\n\ndifferential geometry\n\n\nclassical mechanics\n\n\n\n\nAn application of automatic differentiation with JAX in classical mechanics.\n\n\n\n\n\n\nDec 4, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrandom walks\n\n\nbayesian modeling\n\n\ntime series\n\n\ntensorflow-probability\n\n\n\n\nA construction of a TensorFlow-Probability distribution implementing Gaussian random walks, with an application to stochastic volatility modeling.\n\n\n\n\n\n\nJun 5, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontextual bandits\n\n\nreinforcement learning\n\n\nbayesian modeling\n\n\nvariational inference\n\n\nprobabilistic machine learning\n\n\ntensorflow-probability\n\n\n\n\nAn introduction to Thompson sampling and how to implement it with probabilistic machine learning to tackle contextual bandits.\n\n\n\n\n\n\nFeb 9, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nvariational inference\n\n\ntensorflow-probability\n\n\n\n\nAn overview of the variational inference APIs available in TensorFlow-Probability.\n\n\n\n\n\n\nFeb 1, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nclinical quality\n\n\n\n\nWhen math helps you define too many and too few protocol deviations.\n\n\n\n\n\n\nJan 13, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbayesian modeling\n\n\nclinical quality\n\n\n\n\nAdverse event underreporting is a problem that sometimes affects clinical trials. Probabilistic programming is the modern way to address this issue.\n\n\n\n\n\n\nJan 8, 2021\n\n\nYves Barmaz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My background is in theoretical physics (quantum fields and strings) and I now work as a data scientist in the pharmaceutical industry. I focus mostly on quantitative methods in drug development, optimization of business processes, and recently probabilistic modeling to support the development of next generation nanopore sequencers.\nIn my spare time, I enjoy ski touring, mountain biking and cycling, especially in the Swiss Alps where I grew up.\nIn this blog, I am sharing learnings from my data science and applied mathematics journey."
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About",
    "section": "Get in touch",
    "text": "Get in touch\nYou can email me at yves.barmaz@gmail.com."
  },
  {
    "objectID": "about.html#selected-publications",
    "href": "about.html#selected-publications",
    "title": "About",
    "section": "Selected publications",
    "text": "Selected publications\n\nBayesian modeling for the detection of adverse events underreporting in clinical trials\nEnabling Data-Driven Clinical Quality Assurance: Predicting Adverse Event Reporting in Clinical Trials Using Machine Learning\nUsing Statistical Modeling for Enhanced and Flexible Pharmacovigilance Audit Risk Assessment and Planning\nChern-Simons Theory with Wilson Lines and Boundary in the BV-BFV Formalism"
  }
]